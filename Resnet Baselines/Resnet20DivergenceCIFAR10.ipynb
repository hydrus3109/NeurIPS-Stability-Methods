{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeH1MRhpDspz"
      },
      "outputs": [],
      "source": [
        "# %env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym2-5JKp6PMa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "\n",
        "random.seed(3242023)\n",
        "torch.manual_seed(3242023) # Seeded with a constant, so that behavior is deterministic.\n",
        "torch.cuda.manual_seed_all(3242023)\n",
        "\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "   device = \"cpu\"\n",
        "\n",
        "samples = 25000\n",
        "batch_size = 5000\n",
        "sample_mode = \"independent\" # Should be one of \"independent\" or an integer number of samples to exclude from each training\n",
        "epochs = 200\n",
        "num_trials = 3\n",
        "clip_grad = True\n",
        "grad_group_clipping_size = 100\n",
        "grad_group_clipping_value = 0.1\n",
        "\n",
        "print(f\"Pytorch running on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrV8iqiK6NlP"
      },
      "outputs": [],
      "source": [
        "def _weights_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3]).to(device)\n",
        "\n",
        "\"\"\"\n",
        "All code above here with the exception of setting the random seed was taken from\n",
        "https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py\n",
        "\"\"\"\n",
        "\n",
        "def get_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.view(-1))\n",
        "    params = torch.cat(params)\n",
        "    return params\n",
        "\n",
        "def get_unfrozen_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        if param.requires_grad:\n",
        "            params.append(param.view(-1))\n",
        "    params = torch.cat(params)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRmL3XIJ3U_W"
      },
      "outputs": [],
      "source": [
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def test_model(model, preprocessing = lambda x: x):\n",
        "    test_loss = 0.0\n",
        "    testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size = 20)\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    # iterate over test data\n",
        "    for data, target in testloader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # preprocess data with an arbitrary function\n",
        "        data = preprocessing(data)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update test loss\n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(20):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "    # average test loss\n",
        "    test_loss = test_loss/len(testloader.dataset)\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    for i in range(10):\n",
        "        if class_total[i] > 0:\n",
        "\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "              classes[i], 100 * class_correct[i] / class_total[i],\n",
        "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "    return 100. * np.sum(class_correct) / np.sum(class_total)\n",
        "\n",
        "def get_l2_norm(data):\n",
        "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
        "    deviation_data = data-mu\n",
        "    total_norm = 0.0\n",
        "    for row in deviation_data:\n",
        "        norm = torch.norm(row, p=2)\n",
        "        norm *= norm\n",
        "        total_norm += norm\n",
        "    return total_norm / data.size()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVrXAHve6XDw"
      },
      "outputs": [],
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD_DEV = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD_DEV),\n",
        "    transforms.RandomCrop(32, 4),\n",
        "    transforms.RandomHorizontalFlip()\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "net = resnet20()\n",
        "net.to(device)\n",
        "\n",
        "uploaded = files.upload()\n",
        "torch_object = torch.load(BytesIO(list(uploaded.values())[0]), map_location=device)\n",
        "net.load_state_dict(torch_object)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "\n",
        "params = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnqGxnYF6CcG"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss of pretrained model:\")\n",
        "test_model(net)\n",
        "# for param in net.parameters():\n",
        "#     param.requires_grad = False\n",
        "# for param in net.linear.parameters():\n",
        "#     param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfZFWaX69F3j"
      },
      "outputs": [],
      "source": [
        "net_base = net\n",
        "params_all = []\n",
        "print(\"Beginning training...\")\n",
        "for trial in range(num_trials):\n",
        "    params_trial = []\n",
        "    net = resnet20()\n",
        "    net.to(device)\n",
        "    net.load_state_dict(net_base.state_dict()) # Ensure we start from the same place with all models. This appears to be the crucial step.\n",
        "    # for param in net.parameters():\n",
        "    #     param.requires_grad = False\n",
        "    # for param in net.linear.parameters():\n",
        "    #     param.requires_grad = True\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "    if sample_mode == \"independent\":\n",
        "        train_subset = torch.utils.data.Subset(trainset, random.sample(range(50000), samples))\n",
        "    else:\n",
        "        train_subset = torch.utils.data.Subset(trainset, random.sample(range(samples), samples-sample_mode))\n",
        "    trainloader = torch.utils.data.DataLoader(train_subset, batch_size=grad_group_clipping_size if clip_grad else batch_size,\n",
        "                                          shuffle=False)\n",
        "    net.train()\n",
        "\n",
        "    print(f\"Beginning trial #{trial}\")\n",
        "    for epoch in range(epochs):\n",
        "        for data in iter(trainloader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            if clip_grad:\n",
        "                # Clip each parameter's per-sample gradient. Adapted from https://opacus.ai/tutorials/guide_to_grad_sampler\n",
        "                for p in net.parameters():\n",
        "                    per_sample_grad = p.grad.detach().clone()\n",
        "                    print(\"Pre norm: \" + torch.norm(per_sample_grad, p=2))\n",
        "                    torch.nn.utils.clip_grad_norm(per_sample_grad, max_norm=grad_group_clipping_value)\n",
        "                    print(\"Post norm: \" + torch.norm(per_sample_grad, p=2))\n",
        "                    p.grad = per_sample_grad\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} of trial {trial} successfully completed with {loss} train loss\")\n",
        "        params_trial.append(get_unfrozen_params(net))\n",
        "    params_all.append(params_trial)\n",
        "    print(f\"Test loss at the conclusion of trial {trial}:\")\n",
        "    test_model(net)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZD1VOjg9nbo"
      },
      "outputs": [],
      "source": [
        "params_all = [torch.stack(trial) for trial in params_all]\n",
        "params = torch.stack(params_all) # Dimensions (in order) correspond to: parameters, time steps, and separarte trials\n",
        "torch.save(params, f\"{epochs}EpochsDivergence{num_trials}Trials.pt\")\n",
        "files.download(f\"{epochs}EpochsDivergence{num_trials}Trials.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4WmEiv4ycQi"
      },
      "outputs": [],
      "source": [
        "print(params.size())\n",
        "difference_norms = []\n",
        "model_norms = []\n",
        "for step in range(params.size()[1]):\n",
        "    params_step = params[:, step, :]\n",
        "    difference_norms.append(get_l2_norm(params_step))\n",
        "    for model in range(params.size()[0]):\n",
        "        model_norm = params[model, step, :]\n",
        "        model_norms.append(torch.norm(model_norm, p=2))\n",
        "avg_norm = sum(model_norms)/len(model_norms)\n",
        "difference_norms_relative = [(norm/avg_norm).item() for norm in difference_norms]\n",
        "plt.plot(difference_norms_relative)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}