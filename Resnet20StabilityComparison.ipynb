{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJM1Mc+EGllYDTxUK2EW0a"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ],
      "metadata": {
        "id": "IQRwBPWj-tN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXic2-A-E7Gp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "torch.manual_seed(3242023) # Seeded with a constant, so that behavior is deterministic.\n",
        "torch.cuda.manual_seed_all(3242023)\n",
        "\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "   device = \"cpu\"\n",
        "\n",
        "print(f\"Pytorch running on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _weights_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3]).to(device)\n",
        "\n",
        "\"\"\"\n",
        "All code above here with the exception of setting the random seed was taken from\n",
        "https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py\n",
        "\"\"\"\n",
        "\n",
        "def pretrained_resnet20(pretrained_path):\n",
        "    ans = ResNet(BasicBlock, [3, 3, 3]).to(device)\n",
        "    ans.load_state_dict(torch.load(pretrained_path))\n",
        "    return ans"
      ],
      "metadata": {
        "id": "k6YzPgmRFF79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.view(-1))\n",
        "    params = torch.cat(params)\n",
        "    return params\n",
        "\n",
        "def get_l2_distance(model1, model2):\n",
        "    params1 = get_params(model1)\n",
        "    params2 = get_params(model2)\n",
        "    diff = params1-params2\n",
        "    norm = torch.norm(diff, p=2)\n",
        "    return norm\n",
        "\n",
        "def get_l2_distance_as_fraction(model1, model2):\n",
        "    params1 = get_params(model1)\n",
        "    params2 = get_params(model2)\n",
        "    diff = params1-params2\n",
        "    norm = torch.norm(diff, p=2)\n",
        "    size = (torch.norm(params1, p=2) + torch.norm(params2, p=2)) / 2\n",
        "    return norm/size"
      ],
      "metadata": {
        "id": "5FXsdGmMOu3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD_DEV = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD_DEV),\n",
        "])\n",
        "\n",
        "samples = 5000\n",
        "samples_to_remove = 0\n",
        "epochs = 100\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset_full = torch.utils.data.Subset(trainset, range(samples))\n",
        "trainset_partial =  torch.utils.data.Subset(trainset, range(samples-samples_to_remove))\n",
        "\n",
        "trainloader_full = torch.utils.data.DataLoader(trainset_full, batch_size=samples,\n",
        "                                          shuffle=False)\n",
        "trainloader_partial = torch.utils.data.DataLoader(trainset_partial, batch_size=(samples-samples_to_remove),\n",
        "                                          shuffle=False)\n",
        "\n",
        "net_full = resnet20()\n",
        "net_partial = resnet20()\n",
        "net_partial.load_state_dict(net_full.state_dict()) # Ensure we start from the same place with both models. This appears to be the crucial step.\n",
        "\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_full = torch.optim.SGD(net_full.parameters(), lr=0.1, momentum=0.9)\n",
        "optimizer_partial = torch.optim.SGD(net_partial.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "\n",
        "loss_by_epoch_full = []\n",
        "loss_by_epoch_partial = []\n",
        "epoch_wide_running_loss_full = 0\n",
        "epoch_wide_running_loss_partial = 0"
      ],
      "metadata": {
        "id": "XsMfznJNFKV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "02print(\"Beginning training...\")\n",
        "\n",
        "net_full.train()\n",
        "net_partial.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    dist = get_l2_distance(net_full, net_partial)\n",
        "    dist_as_fraction = get_l2_distance_as_fraction(net_full, net_partial)\n",
        "    print(f\"Before epoch {epoch}: L2 norm of difference after removing {samples_to_remove} samples out of {samples} sample dataset: {dist}. This is {dist_as_fraction*100}% of the l2 norm of the models themselves.\")\n",
        "\n",
        "    torch.manual_seed(3242023)\n",
        "    torch.cuda.manual_seed_all(3242023)\n",
        "\n",
        "    for i, data in enumerate(trainloader_full, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_full.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net_full(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_full.step()\n",
        "\n",
        "        # log statistics\n",
        "        epoch_wide_running_loss_full += loss.item()\n",
        "    loss_by_epoch_full.append(epoch_wide_running_loss_full/samples)\n",
        "\n",
        "    torch.manual_seed(3242023)\n",
        "    torch.cuda.manual_seed_all(3242023)\n",
        "\n",
        "    for i, data in enumerate(trainloader_partial, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_partial.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net_partial(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_partial.step()\n",
        "\n",
        "        # log statistics\n",
        "        epoch_wide_running_loss_partial += loss.item()\n",
        "    loss_by_epoch_partial.append(epoch_wide_running_loss_partial/(samples-samples_to_remove))\n",
        "\n",
        "    print(f\"Epoch {epoch}: full dataset loss: {epoch_wide_running_loss_full/samples}, dataset sans {samples_to_remove} samples loss: {epoch_wide_running_loss_partial/(samples-samples_to_remove)}\")\n",
        "    epoch_wide_running_loss_full = 0\n",
        "    epoch_wide_running_loss_partial = 0\n",
        "\n",
        "print(loss_by_epoch_full, loss_by_epoch_partial)"
      ],
      "metadata": {
        "id": "H8R88CKKFNwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist = get_l2_distance(net_full, net_partial)\n",
        "dist_as_fraction = get_l2_distance_as_fraction(net_full, net_partial)\n",
        "print(f\"L2 norm of difference after removing {samples_to_remove} samples out of {samples} sample dataset: {dist}. This is {dist_as_fraction*100}% of the l2 norm of the models themselves.\")"
      ],
      "metadata": {
        "id": "gR3X0tmDPwNO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}