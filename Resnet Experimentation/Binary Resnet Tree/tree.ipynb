{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04d3c44-2a48-4e9f-88c8-8287f47bc1f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T15:10:01.665688Z",
     "iopub.status.busy": "2023-06-18T15:10:01.665044Z",
     "iopub.status.idle": "2023-06-18T15:10:02.909038Z",
     "shell.execute_reply": "2023-06-18T15:10:02.907910Z",
     "shell.execute_reply.started": "2023-06-18T15:10:01.665661Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms   \n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from numpy.linalg import norm\n",
    "from torch.cuda.random import device_count\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision \n",
    "import math\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ca7bc5-e7bd-4cb8-859f-45ddb198ff15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T15:10:05.677601Z",
     "iopub.status.busy": "2023-06-18T15:10:05.676756Z",
     "iopub.status.idle": "2023-06-18T15:10:11.162198Z",
     "shell.execute_reply": "2023-06-18T15:10:11.161637Z",
     "shell.execute_reply.started": "2023-06-18T15:10:05.677571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.random import device_count\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "transform1 = transforms.Compose(\n",
    "    [    \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4, 0.4822, 0.4465), (0.247, 0.243, 0.261)),   \n",
    "    ])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./', train=True,download=True,transform = transform1)\n",
    "indices = torch.randperm(len(trainset))[:5000]\n",
    "trainset1 = torch.utils.data.Subset(trainset, indices)\n",
    "modelloader = torch.utils.data.DataLoader(trainset, shuffle=True, num_workers=2, batch_size = 100)\n",
    "validset = torchvision.datasets.CIFAR10(root='./', train=True,download=True, transform = transform) \n",
    "valid = list(range(40000, 50000,1))\n",
    "validset1 = torch.utils.data.Subset(validset, valid)\n",
    "validloader = torch.utils.data.DataLoader(validset1, shuffle=True, num_workers=4,batch_size = 100)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train = False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size = 1)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c8a208f-7b7c-43a0-bf9c-a83c88a143a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T15:10:11.163468Z",
     "iopub.status.busy": "2023-06-18T15:10:11.163273Z",
     "iopub.status.idle": "2023-06-18T15:10:11.174773Z",
     "shell.execute_reply": "2023-06-18T15:10:11.174319Z",
     "shell.execute_reply.started": "2023-06-18T15:10:11.163447Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                             F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20(classes):\n",
    "    return ResNet(BasicBlock, [3, 3, 3], classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee28788f-7e34-4568-a8a7-38e9e49d8c23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T15:10:11.175493Z",
     "iopub.status.busy": "2023-06-18T15:10:11.175324Z",
     "iopub.status.idle": "2023-06-18T15:10:11.181762Z",
     "shell.execute_reply": "2023-06-18T15:10:11.181326Z",
     "shell.execute_reply.started": "2023-06-18T15:10:11.175476Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 =nn.Conv2d(64, 128, 3)\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 24)\n",
    "        self.fc2 = nn.Linear(24,2)\n",
    "       # self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "      #  print(x.size())\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "       # x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 =nn.Conv2d(64, 128, 3)\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 24)\n",
    "        self.fc2 = nn.Linear(24,3)\n",
    "       # self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "      #  print(x.size())\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "       # x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b247a606-d88d-4900-8490-d433b0e7ff1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T22:46:44.651970Z",
     "iopub.status.busy": "2023-06-17T22:46:44.651786Z",
     "iopub.status.idle": "2023-06-17T22:46:44.671454Z",
     "shell.execute_reply": "2023-06-17T22:46:44.669758Z",
     "shell.execute_reply.started": "2023-06-17T22:46:44.651970Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer1transform(array,size):\n",
    "    newarr = array.detach().clone()\n",
    "    for i in range(size):\n",
    "        if array[i] <= 2:\n",
    "            newarr[i] = 0\n",
    "        elif array[i] < 6:\n",
    "            newarr[i] = 1\n",
    "        else: \n",
    "            newarr[i] = 2\n",
    "    return newarr\n",
    "def layer2transform1(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        ttensor = torch.Tensor([2])\n",
    "        ttensor = ttensor.to(device)\n",
    "        if array[i] <= 2:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] == 0:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            elif array[i] == 1:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "            elif array[i] == 2: \n",
    "                newarr = torch.cat((newarr,ttensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def layer2transform2(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        ttensor = torch.Tensor([2])\n",
    "        ttensor = ttensor.to(device)\n",
    "        if array[i] >2 and array[i] <= 5:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] == 3:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            elif array[i] == 4:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "            elif array[i] == 5: \n",
    "                newarr = torch.cat((newarr,ttensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def layer2transform3(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        ttensor = torch.Tensor([2])\n",
    "        ttensor = ttensor.to(device)\n",
    "        thtensor = torch.Tensor([3])\n",
    "        thtensor = ttensor.to(device)\n",
    "        if array[i] > 5:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] == 6:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            elif array[i] == 7:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "            elif array[i] == 8: \n",
    "                newarr = torch.cat((newarr,ttensor),dim=0)\n",
    "            elif array[i] == 9:\n",
    "                newarr = torch.cat((newarr,thtensor),dim=0)\n",
    "    return newdata, newarr\n",
    "\n",
    "def test():\n",
    "    for data, target in modelloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        layer2transform2(data,target)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615de72a-49c8-4c4e-b56b-0c926556484c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T15:10:15.322737Z",
     "iopub.status.busy": "2023-06-18T15:10:15.322097Z",
     "iopub.status.idle": "2023-06-18T15:10:15.338818Z",
     "shell.execute_reply": "2023-06-18T15:10:15.338193Z",
     "shell.execute_reply.started": "2023-06-18T15:10:15.322714Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer1transform(array,size):\n",
    "    newarr = array.detach().clone()\n",
    "    for i in range(size):\n",
    "        if array[i] <5:\n",
    "            newarr[i] = 0\n",
    "        else: \n",
    "            newarr[i] = 1\n",
    "    return newarr\n",
    "def layer2transform1(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        if array[i] < 5:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] < 2:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            else:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def layer2transform2(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        if array[i] >= 5:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] <7:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            else:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def layer3transform1(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        if array[i] <2:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] ==0:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            else:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def layer3transform2(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        ttensor = torch.Tensor([2])\n",
    "        ttensor = ttensor.to(device)\n",
    "        if array[i] >=2 and array[i] <5:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] == 2:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            elif array[i] == 3:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "            elif array[i] == 4: \n",
    "                newarr = torch.cat((newarr,ttensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def layer3transform3(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        if array[i] >=5 and array[i] <7:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] ==5:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            else:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def layer3transform4(data, array):\n",
    "    newdata = torch.Tensor()\n",
    "    newdata = newdata.to(device)\n",
    "    newarr = torch.Tensor()\n",
    "    newarr = newarr.to(device)\n",
    "    for i in range(100):\n",
    "        ztensor = torch.zeros(1)\n",
    "        ztensor = ztensor.to(device)\n",
    "        otensor = torch.Tensor([1])\n",
    "        otensor = otensor.to(device)\n",
    "        ttensor = torch.Tensor([2])\n",
    "        ttensor = ttensor.to(device)\n",
    "        if array[i] >= 7:\n",
    "            newdata = torch.cat((newdata,data[i].unsqueeze(0)),dim=0)       \n",
    "            if array[i] == 7:\n",
    "                newarr = torch.cat((newarr,ztensor),dim=0)\n",
    "            elif array[i] == 8:\n",
    "                newarr = torch.cat((newarr,otensor),dim=0)\n",
    "            elif array[i] == 9: \n",
    "                newarr = torch.cat((newarr,ttensor),dim=0)\n",
    "    return newdata, newarr\n",
    "def test():\n",
    "    for data, target in modelloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        layer2transform2(data,target)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06cc39-ad28-4672-8f8f-8472a528851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = resnet20(3)\n",
    "model2 = resnet20(3)\n",
    "model3 = resnet20(3)\n",
    "model4 = resnet20(4)\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "model4.to(device)\n",
    "criterion1 = nn.CrossEntropyLoss() \n",
    "criterion2 = nn.CrossEntropyLoss() \n",
    "criterion3 = nn.CrossEntropyLoss() \n",
    "criterion4 = nn.CrossEntropyLoss() \n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01, weight_decay = 0.001, momentum = 0.9)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01, weight_decay = 0.001, momentum = 0.9)\n",
    "optimizer3 = optim.SGD(model3.parameters(), lr=0.01, weight_decay = 0.001, momentum = 0.9)\n",
    "optimizer4 = optim.SGD(model4.parameters(), lr=0.01, weight_decay = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda805a0-7b8a-48d6-9340-a43a9ccacdf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T15:10:20.000895Z",
     "iopub.status.busy": "2023-06-18T15:10:20.000591Z",
     "iopub.status.idle": "2023-06-18T15:10:21.325250Z",
     "shell.execute_reply": "2023-06-18T15:10:21.324413Z",
     "shell.execute_reply.started": "2023-06-18T15:10:20.000873Z"
    }
   },
   "outputs": [],
   "source": [
    "model1 = Net()\n",
    "model2 = Net()\n",
    "model3 = Net()\n",
    "model4 = Net()\n",
    "model5 = Net2()\n",
    "model6 = Net()\n",
    "model7 = Net2()\n",
    "model1.load_state_dict(torch.load(\"model1\"))\n",
    "model2.load_state_dict(torch.load(\"model2\"))\n",
    "model3.load_state_dict(torch.load(\"model3\"))\n",
    "model4.load_state_dict(torch.load(\"model4\"))\n",
    "model5.load_state_dict(torch.load(\"model5\"))\n",
    "model6.load_state_dict(torch.load(\"model6\"))\n",
    "model7.load_state_dict(torch.load(\"model7\"))\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "model4.to(device)\n",
    "model5.to(device)\n",
    "model6.to(device)\n",
    "model7.to(device)\n",
    "criterion1 = nn.CrossEntropyLoss() \n",
    "criterion2 = nn.CrossEntropyLoss() \n",
    "criterion3 = nn.CrossEntropyLoss() \n",
    "criterion4 = nn.CrossEntropyLoss() \n",
    "criterion5 = nn.CrossEntropyLoss() \n",
    "criterion6= nn.CrossEntropyLoss() \n",
    "criterion7= nn.CrossEntropyLoss() \n",
    "optimizer1 = optim.Adam(model1.parameters(), lr = 0.001)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr = 0.001)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr = 0.001)\n",
    "optimizer4 = optim.Adam(model4.parameters(), lr = 0.001)\n",
    "optimizer5 = optim.Adam(model5.parameters(), lr = 0.001)\n",
    "optimizer6 = optim.Adam(model6.parameters(), lr = 0.001)\n",
    "optimizer7 = optim.Adam(model7.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bbcc19-d3c2-44aa-9e5c-778bdc193972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T22:49:06.305219Z",
     "iopub.status.busy": "2023-06-17T22:49:06.304822Z",
     "iopub.status.idle": "2023-06-17T22:49:06.316799Z",
     "shell.execute_reply": "2023-06-17T22:49:06.315546Z",
     "shell.execute_reply.started": "2023-06-17T22:49:06.305191Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainmodels(epoch): \n",
    "    train_loss1 = 0.0\n",
    "    train_loss2 = 0.0\n",
    "    train_loss3 = 0.0\n",
    "    train_loss4 = 0.0\n",
    "    for data, target in modelloader:\n",
    "        data, target = data.to(device), target.to(device) \n",
    "        initialtarget = layer1transform(target,100)\n",
    "        data1,target1 =layer2transform1(data,target)\n",
    "        target1 = target1.type(torch.LongTensor) \n",
    "        target1 = target1.to(device)\n",
    "        data2,target2 =layer2transform2(data,target)\n",
    "        target2 = target2.type(torch.LongTensor) \n",
    "        target2 = target2.to(device)\n",
    "        data3,target3 =layer2transform3(data,target)\n",
    "        target3 = target3.type(torch.LongTensor) \n",
    "        target3 = target3.to(device)\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        optimizer4.zero_grad()\n",
    "        output1 = model1(data)\n",
    "        output2 = model2(data1)\n",
    "        output3 = model3(data2)\n",
    "        output4 = model4(data3)\n",
    "        loss1 = criterion1(output1, initialtarget)\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        train_loss1 += loss1.item()*data.size(0)\n",
    "        loss2 = criterion2(output2,target1)\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        train_loss2 += loss2.item()*data.size(0)\n",
    "        loss3 = criterion3(output3,target2)\n",
    "        loss3.backward()\n",
    "        optimizer3.step()\n",
    "        train_loss3 += loss3.item()*data.size(0)\n",
    "        loss4 = criterion4(output4,target3)\n",
    "        loss4.backward()\n",
    "        optimizer4.step()\n",
    "        train_loss4 += loss4.item()*data.size(0)\n",
    "    train_loss1 = train_loss1/len(modelloader.dataset)\n",
    "    train_loss2 = train_loss2/len(modelloader.dataset)\n",
    "    train_loss3 = train_loss3/len(modelloader.dataset)\n",
    "    train_loss4 = train_loss4/len(modelloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss1: {:.6f} \\tTraining Loss2: {:.6f} \\tTraining Loss3: {:.6f} \\tTraining Loss4: {:.6f}'.format(epoch, train_loss1, train_loss2, train_loss3, train_loss4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71722698-27f6-4c91-8804-41c630ddddf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T15:10:28.665929Z",
     "iopub.status.busy": "2023-06-18T15:10:28.665430Z",
     "iopub.status.idle": "2023-06-18T15:10:28.675232Z",
     "shell.execute_reply": "2023-06-18T15:10:28.674571Z",
     "shell.execute_reply.started": "2023-06-18T15:10:28.665905Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainmodels2(epoch): \n",
    "    train_loss1 = 0.0\n",
    "    train_loss2 = 0.0\n",
    "    train_loss3 = 0.0\n",
    "    train_loss4 = 0.0\n",
    "    train_loss5 = 0.0\n",
    "    train_loss6 = 0.0\n",
    "    train_loss7 = 0.0\n",
    "    for data, target in modelloader:\n",
    "        data, target = data.to(device), target.to(device) \n",
    "        initialtarget = layer1transform(target,100)\n",
    "        data1,target1 =layer2transform1(data,target)\n",
    "        target1 = target1.type(torch.LongTensor) \n",
    "        target1 = target1.to(device)\n",
    "        data2,target2 =layer2transform2(data,target)\n",
    "        target2 = target2.type(torch.LongTensor) \n",
    "        target2 = target2.to(device)\n",
    "        data3,target3 =layer3transform1(data,target)\n",
    "        target3 = target3.type(torch.LongTensor) \n",
    "        target3 = target3.to(device)\n",
    "        data4,target4 =layer3transform2(data,target)\n",
    "        target4 = target4.type(torch.LongTensor) \n",
    "        target4 = target4.to(device)\n",
    "        data5,target5 =layer3transform3(data,target)\n",
    "        target5 = target5.type(torch.LongTensor) \n",
    "        target5 = target5.to(device)\n",
    "        data6,target6 =layer3transform4(data,target)\n",
    "        target6 = target6.type(torch.LongTensor) \n",
    "        target6 = target6.to(device)\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        optimizer4.zero_grad()\n",
    "        optimizer5.zero_grad()\n",
    "        optimizer6.zero_grad()\n",
    "        optimizer7.zero_grad()\n",
    "        output1 = model1(data)\n",
    "        output2 = model2(data1)\n",
    "        output3 = model3(data2)\n",
    "        output4 = model4(data3)\n",
    "        output5 = model5(data4)\n",
    "        output6 = model6(data5)\n",
    "        output7 = model7(data6)\n",
    "        loss1 = criterion1(output1, initialtarget)\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        train_loss1 += loss1.item()*data.size(0)\n",
    "        loss2 = criterion2(output2,target1)\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        train_loss2 += loss2.item()*data.size(0)\n",
    "        loss3 = criterion3(output3,target2)\n",
    "        loss3.backward()\n",
    "        optimizer3.step()\n",
    "        train_loss3 += loss3.item()*data.size(0)\n",
    "        loss4 = criterion4(output4,target3)\n",
    "        loss4.backward()\n",
    "        optimizer4.step()\n",
    "        train_loss4 += loss4.item()*data.size(0)\n",
    "        loss5 = criterion5(output5,target4)\n",
    "        loss5.backward()\n",
    "        optimizer5.step()\n",
    "        train_loss5 += loss5.item()*data.size(0)\n",
    "        loss6 = criterion6(output6,target5)\n",
    "        loss6.backward()\n",
    "        optimizer6.step()\n",
    "        train_loss6 += loss6.item()*data.size(0)\n",
    "        loss7 = criterion7(output7,target6)\n",
    "        loss7.backward()\n",
    "        optimizer7.step()\n",
    "        train_loss7 += loss7.item()*data.size(0)\n",
    "    train_loss1 = train_loss1/len(modelloader.dataset)\n",
    "    train_loss2 = train_loss2/len(modelloader.dataset)\n",
    "    train_loss3 = train_loss3/len(modelloader.dataset)\n",
    "    train_loss4 = train_loss4/len(modelloader.dataset)\n",
    "    train_loss5 = train_loss5/len(modelloader.dataset)\n",
    "    train_loss6 = train_loss6/len(modelloader.dataset)\n",
    "    train_loss7 = train_loss7/len(modelloader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss1: {:.6f} \\tTraining Loss2: {:.6f} \\tTraining Loss3: {:.6f} \\tTraining Loss4: {:.6f}'.format(epoch, train_loss1, train_loss2, train_loss3, train_loss4))\n",
    "    print('Epoch: {} \\tTraining Loss5: {:.6f} \\tTraining Loss6: {:.6f} \\tTraining Loss7: {:.6f}'.format(epoch, train_loss5, train_loss6, train_loss7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ad0ea29-9ca4-4da1-a573-e424c262ff03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T18:54:30.557444Z",
     "iopub.status.busy": "2023-06-18T18:54:30.557189Z",
     "iopub.status.idle": "2023-06-18T19:43:57.317659Z",
     "shell.execute_reply": "2023-06-18T19:43:57.316513Z",
     "shell.execute_reply.started": "2023-06-18T18:54:30.557424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss1: 0.133103 \tTraining Loss2: 0.012182 \tTraining Loss3: 0.018250 \tTraining Loss4: 0.008977\n",
      "Epoch: 0 \tTraining Loss5: 0.049511 \tTraining Loss6: 0.008273 \tTraining Loss7: 0.020197\n",
      "Epoch: 1 \tTraining Loss1: 0.132874 \tTraining Loss2: 0.018645 \tTraining Loss3: 0.015750 \tTraining Loss4: 0.012056\n",
      "Epoch: 1 \tTraining Loss5: 0.070219 \tTraining Loss6: 0.009168 \tTraining Loss7: 0.013293\n",
      "Epoch: 2 \tTraining Loss1: 0.134812 \tTraining Loss2: 0.013670 \tTraining Loss3: 0.018952 \tTraining Loss4: 0.035210\n",
      "Epoch: 2 \tTraining Loss5: 0.061772 \tTraining Loss6: 0.012157 \tTraining Loss7: 0.017916\n",
      "Epoch: 3 \tTraining Loss1: 0.132104 \tTraining Loss2: 0.013591 \tTraining Loss3: 0.017623 \tTraining Loss4: 0.023480\n",
      "Epoch: 3 \tTraining Loss5: 0.055341 \tTraining Loss6: 0.016188 \tTraining Loss7: 0.015348\n",
      "Epoch: 4 \tTraining Loss1: 0.130764 \tTraining Loss2: 0.015566 \tTraining Loss3: 0.015309 \tTraining Loss4: 0.014456\n",
      "Epoch: 4 \tTraining Loss5: 0.059824 \tTraining Loss6: 0.018144 \tTraining Loss7: 0.016932\n",
      "Epoch: 5 \tTraining Loss1: 0.134975 \tTraining Loss2: 0.014658 \tTraining Loss3: 0.014935 \tTraining Loss4: 0.006319\n",
      "Epoch: 5 \tTraining Loss5: 0.055446 \tTraining Loss6: 0.019580 \tTraining Loss7: 0.018241\n",
      "Epoch: 6 \tTraining Loss1: 0.134735 \tTraining Loss2: 0.013891 \tTraining Loss3: 0.014311 \tTraining Loss4: 0.015579\n",
      "Epoch: 6 \tTraining Loss5: 0.054873 \tTraining Loss6: 0.011209 \tTraining Loss7: 0.020787\n",
      "Epoch: 7 \tTraining Loss1: 0.132388 \tTraining Loss2: 0.015860 \tTraining Loss3: 0.018816 \tTraining Loss4: 0.011531\n",
      "Epoch: 7 \tTraining Loss5: 0.055268 \tTraining Loss6: 0.017714 \tTraining Loss7: 0.021046\n",
      "Epoch: 8 \tTraining Loss1: 0.133772 \tTraining Loss2: 0.014205 \tTraining Loss3: 0.019217 \tTraining Loss4: 0.006539\n",
      "Epoch: 8 \tTraining Loss5: 0.060447 \tTraining Loss6: 0.011820 \tTraining Loss7: 0.014603\n",
      "Epoch: 9 \tTraining Loss1: 0.129306 \tTraining Loss2: 0.012243 \tTraining Loss3: 0.015339 \tTraining Loss4: 0.013709\n",
      "Epoch: 9 \tTraining Loss5: 0.059903 \tTraining Loss6: 0.020244 \tTraining Loss7: 0.018543\n",
      "Epoch: 10 \tTraining Loss1: 0.134522 \tTraining Loss2: 0.013104 \tTraining Loss3: 0.017208 \tTraining Loss4: 0.019904\n",
      "Epoch: 10 \tTraining Loss5: 0.059715 \tTraining Loss6: 0.017499 \tTraining Loss7: 0.013517\n",
      "Epoch: 11 \tTraining Loss1: 0.134944 \tTraining Loss2: 0.018018 \tTraining Loss3: 0.015879 \tTraining Loss4: 0.010600\n",
      "Epoch: 11 \tTraining Loss5: 0.062715 \tTraining Loss6: 0.017556 \tTraining Loss7: 0.017962\n",
      "Epoch: 12 \tTraining Loss1: 0.132358 \tTraining Loss2: 0.015996 \tTraining Loss3: 0.018234 \tTraining Loss4: 0.007123\n",
      "Epoch: 12 \tTraining Loss5: 0.055620 \tTraining Loss6: 0.009193 \tTraining Loss7: 0.019739\n",
      "Epoch: 13 \tTraining Loss1: 0.132968 \tTraining Loss2: 0.014626 \tTraining Loss3: 0.017091 \tTraining Loss4: 0.005917\n",
      "Epoch: 13 \tTraining Loss5: 0.058336 \tTraining Loss6: 0.008217 \tTraining Loss7: 0.016610\n",
      "Epoch: 14 \tTraining Loss1: 0.130482 \tTraining Loss2: 0.014127 \tTraining Loss3: 0.017154 \tTraining Loss4: 0.018791\n",
      "Epoch: 14 \tTraining Loss5: 0.061412 \tTraining Loss6: 0.008471 \tTraining Loss7: 0.017596\n",
      "Epoch: 15 \tTraining Loss1: 0.135710 \tTraining Loss2: 0.014337 \tTraining Loss3: 0.019105 \tTraining Loss4: 0.012300\n",
      "Epoch: 15 \tTraining Loss5: 0.053771 \tTraining Loss6: 0.012196 \tTraining Loss7: 0.013134\n",
      "Epoch: 16 \tTraining Loss1: 0.131144 \tTraining Loss2: 0.014303 \tTraining Loss3: 0.014980 \tTraining Loss4: 0.018320\n",
      "Epoch: 16 \tTraining Loss5: 0.064370 \tTraining Loss6: 0.012758 \tTraining Loss7: 0.022466\n",
      "Epoch: 17 \tTraining Loss1: 0.130152 \tTraining Loss2: 0.013337 \tTraining Loss3: 0.017726 \tTraining Loss4: 0.013844\n",
      "Epoch: 17 \tTraining Loss5: 0.062202 \tTraining Loss6: 0.019532 \tTraining Loss7: 0.018602\n",
      "Epoch: 18 \tTraining Loss1: 0.130291 \tTraining Loss2: 0.017774 \tTraining Loss3: 0.019415 \tTraining Loss4: 0.011080\n",
      "Epoch: 18 \tTraining Loss5: 0.058214 \tTraining Loss6: 0.020369 \tTraining Loss7: 0.015076\n",
      "Epoch: 19 \tTraining Loss1: 0.132395 \tTraining Loss2: 0.012879 \tTraining Loss3: 0.018210 \tTraining Loss4: 0.011199\n",
      "Epoch: 19 \tTraining Loss5: 0.063656 \tTraining Loss6: 0.010439 \tTraining Loss7: 0.022354\n",
      "Epoch: 20 \tTraining Loss1: 0.134166 \tTraining Loss2: 0.014940 \tTraining Loss3: 0.014234 \tTraining Loss4: 0.038591\n",
      "Epoch: 20 \tTraining Loss5: 0.063512 \tTraining Loss6: 0.009981 \tTraining Loss7: 0.016690\n",
      "Epoch: 21 \tTraining Loss1: 0.131237 \tTraining Loss2: 0.015249 \tTraining Loss3: 0.016453 \tTraining Loss4: 0.016133\n",
      "Epoch: 21 \tTraining Loss5: 0.054497 \tTraining Loss6: 0.013594 \tTraining Loss7: 0.024500\n",
      "Epoch: 22 \tTraining Loss1: 0.129766 \tTraining Loss2: 0.015191 \tTraining Loss3: 0.015821 \tTraining Loss4: 0.012446\n",
      "Epoch: 22 \tTraining Loss5: 0.056833 \tTraining Loss6: 0.013711 \tTraining Loss7: 0.018300\n",
      "Epoch: 23 \tTraining Loss1: 0.131135 \tTraining Loss2: 0.013297 \tTraining Loss3: 0.015055 \tTraining Loss4: 0.004350\n",
      "Epoch: 23 \tTraining Loss5: 0.056357 \tTraining Loss6: 0.012711 \tTraining Loss7: 0.018170\n",
      "Epoch: 24 \tTraining Loss1: 0.130122 \tTraining Loss2: 0.013275 \tTraining Loss3: 0.021401 \tTraining Loss4: 0.011869\n",
      "Epoch: 24 \tTraining Loss5: 0.053376 \tTraining Loss6: 0.024089 \tTraining Loss7: 0.016675\n",
      "Epoch: 25 \tTraining Loss1: 0.134403 \tTraining Loss2: 0.017691 \tTraining Loss3: 0.017362 \tTraining Loss4: 0.020298\n",
      "Epoch: 25 \tTraining Loss5: 0.058171 \tTraining Loss6: 0.020767 \tTraining Loss7: 0.022420\n",
      "Epoch: 26 \tTraining Loss1: 0.129386 \tTraining Loss2: 0.018153 \tTraining Loss3: 0.015025 \tTraining Loss4: 0.014599\n",
      "Epoch: 26 \tTraining Loss5: 0.058228 \tTraining Loss6: 0.011081 \tTraining Loss7: 0.021523\n",
      "Epoch: 27 \tTraining Loss1: 0.131028 \tTraining Loss2: 0.013306 \tTraining Loss3: 0.017144 \tTraining Loss4: 0.014682\n",
      "Epoch: 27 \tTraining Loss5: 0.064120 \tTraining Loss6: 0.014830 \tTraining Loss7: 0.014622\n",
      "Epoch: 28 \tTraining Loss1: 0.133426 \tTraining Loss2: 0.016826 \tTraining Loss3: 0.013937 \tTraining Loss4: 0.012558\n",
      "Epoch: 28 \tTraining Loss5: 0.057963 \tTraining Loss6: 0.015118 \tTraining Loss7: 0.017355\n",
      "Epoch: 29 \tTraining Loss1: 0.134522 \tTraining Loss2: 0.017921 \tTraining Loss3: 0.014457 \tTraining Loss4: 0.010189\n",
      "Epoch: 29 \tTraining Loss5: 0.061466 \tTraining Loss6: 0.023379 \tTraining Loss7: 0.018355\n",
      "Epoch: 30 \tTraining Loss1: 0.132550 \tTraining Loss2: 0.015084 \tTraining Loss3: 0.015951 \tTraining Loss4: 0.007144\n",
      "Epoch: 30 \tTraining Loss5: 0.058268 \tTraining Loss6: 0.010870 \tTraining Loss7: 0.009961\n",
      "Epoch: 31 \tTraining Loss1: 0.130814 \tTraining Loss2: 0.019655 \tTraining Loss3: 0.018495 \tTraining Loss4: 0.014718\n",
      "Epoch: 31 \tTraining Loss5: 0.058011 \tTraining Loss6: 0.021237 \tTraining Loss7: 0.024975\n",
      "Epoch: 32 \tTraining Loss1: 0.132927 \tTraining Loss2: 0.016177 \tTraining Loss3: 0.016260 \tTraining Loss4: 0.018910\n",
      "Epoch: 32 \tTraining Loss5: 0.061076 \tTraining Loss6: 0.015839 \tTraining Loss7: 0.019638\n",
      "Epoch: 33 \tTraining Loss1: 0.131337 \tTraining Loss2: 0.017298 \tTraining Loss3: 0.016072 \tTraining Loss4: 0.009337\n",
      "Epoch: 33 \tTraining Loss5: 0.056065 \tTraining Loss6: 0.021590 \tTraining Loss7: 0.016093\n",
      "Epoch: 34 \tTraining Loss1: 0.131356 \tTraining Loss2: 0.016394 \tTraining Loss3: 0.017764 \tTraining Loss4: 0.014634\n",
      "Epoch: 34 \tTraining Loss5: 0.059235 \tTraining Loss6: 0.006629 \tTraining Loss7: 0.019603\n",
      "Epoch: 35 \tTraining Loss1: 0.132093 \tTraining Loss2: 0.013237 \tTraining Loss3: 0.016731 \tTraining Loss4: 0.012335\n",
      "Epoch: 35 \tTraining Loss5: 0.057658 \tTraining Loss6: 0.007719 \tTraining Loss7: 0.010962\n",
      "Epoch: 36 \tTraining Loss1: 0.132620 \tTraining Loss2: 0.012412 \tTraining Loss3: 0.019858 \tTraining Loss4: 0.014485\n",
      "Epoch: 36 \tTraining Loss5: 0.061245 \tTraining Loss6: 0.012367 \tTraining Loss7: 0.020310\n",
      "Epoch: 37 \tTraining Loss1: 0.133396 \tTraining Loss2: 0.014947 \tTraining Loss3: 0.018709 \tTraining Loss4: 0.011888\n",
      "Epoch: 37 \tTraining Loss5: 0.060080 \tTraining Loss6: 0.018120 \tTraining Loss7: 0.016864\n",
      "Epoch: 38 \tTraining Loss1: 0.130280 \tTraining Loss2: 0.013440 \tTraining Loss3: 0.014089 \tTraining Loss4: 0.021989\n",
      "Epoch: 38 \tTraining Loss5: 0.052712 \tTraining Loss6: 0.010537 \tTraining Loss7: 0.011385\n",
      "Epoch: 39 \tTraining Loss1: 0.127142 \tTraining Loss2: 0.016872 \tTraining Loss3: 0.015157 \tTraining Loss4: 0.018519\n",
      "Epoch: 39 \tTraining Loss5: 0.062021 \tTraining Loss6: 0.013742 \tTraining Loss7: 0.014319\n",
      "Epoch: 40 \tTraining Loss1: 0.133259 \tTraining Loss2: 0.015705 \tTraining Loss3: 0.016418 \tTraining Loss4: 0.009250\n",
      "Epoch: 40 \tTraining Loss5: 0.063701 \tTraining Loss6: 0.029096 \tTraining Loss7: 0.019493\n",
      "Epoch: 41 \tTraining Loss1: 0.131444 \tTraining Loss2: 0.016022 \tTraining Loss3: 0.016240 \tTraining Loss4: 0.013778\n",
      "Epoch: 41 \tTraining Loss5: 0.054204 \tTraining Loss6: 0.015179 \tTraining Loss7: 0.021937\n",
      "Epoch: 42 \tTraining Loss1: 0.134385 \tTraining Loss2: 0.016461 \tTraining Loss3: 0.013384 \tTraining Loss4: 0.026261\n",
      "Epoch: 42 \tTraining Loss5: 0.052277 \tTraining Loss6: 0.009451 \tTraining Loss7: 0.019078\n",
      "Epoch: 43 \tTraining Loss1: 0.131125 \tTraining Loss2: 0.013525 \tTraining Loss3: 0.016339 \tTraining Loss4: 0.018731\n",
      "Epoch: 43 \tTraining Loss5: 0.061715 \tTraining Loss6: 0.018998 \tTraining Loss7: 0.016897\n",
      "Epoch: 44 \tTraining Loss1: 0.131920 \tTraining Loss2: 0.015473 \tTraining Loss3: 0.017253 \tTraining Loss4: 0.016370\n",
      "Epoch: 44 \tTraining Loss5: 0.055868 \tTraining Loss6: 0.017615 \tTraining Loss7: 0.012102\n",
      "Epoch: 45 \tTraining Loss1: 0.132521 \tTraining Loss2: 0.015207 \tTraining Loss3: 0.014948 \tTraining Loss4: 0.007641\n",
      "Epoch: 45 \tTraining Loss5: 0.061584 \tTraining Loss6: 0.014321 \tTraining Loss7: 0.017225\n",
      "Epoch: 46 \tTraining Loss1: 0.130821 \tTraining Loss2: 0.017931 \tTraining Loss3: 0.016302 \tTraining Loss4: 0.014515\n",
      "Epoch: 46 \tTraining Loss5: 0.060624 \tTraining Loss6: 0.015229 \tTraining Loss7: 0.021890\n",
      "Epoch: 47 \tTraining Loss1: 0.128961 \tTraining Loss2: 0.015343 \tTraining Loss3: 0.015451 \tTraining Loss4: 0.012572\n",
      "Epoch: 47 \tTraining Loss5: 0.062228 \tTraining Loss6: 0.010606 \tTraining Loss7: 0.017882\n",
      "Epoch: 48 \tTraining Loss1: 0.130601 \tTraining Loss2: 0.012535 \tTraining Loss3: 0.018595 \tTraining Loss4: 0.011571\n",
      "Epoch: 48 \tTraining Loss5: 0.066547 \tTraining Loss6: 0.023122 \tTraining Loss7: 0.014605\n",
      "Epoch: 49 \tTraining Loss1: 0.132381 \tTraining Loss2: 0.016956 \tTraining Loss3: 0.014252 \tTraining Loss4: 0.013187\n",
      "Epoch: 49 \tTraining Loss5: 0.058414 \tTraining Loss6: 0.014601 \tTraining Loss7: 0.014245\n",
      "Epoch: 50 \tTraining Loss1: 0.130967 \tTraining Loss2: 0.014325 \tTraining Loss3: 0.018560 \tTraining Loss4: 0.009596\n",
      "Epoch: 50 \tTraining Loss5: 0.061098 \tTraining Loss6: 0.013220 \tTraining Loss7: 0.018763\n",
      "Epoch: 51 \tTraining Loss1: 0.130408 \tTraining Loss2: 0.014331 \tTraining Loss3: 0.016503 \tTraining Loss4: 0.025389\n",
      "Epoch: 51 \tTraining Loss5: 0.060853 \tTraining Loss6: 0.010095 \tTraining Loss7: 0.019645\n",
      "Epoch: 52 \tTraining Loss1: 0.129635 \tTraining Loss2: 0.012494 \tTraining Loss3: 0.017771 \tTraining Loss4: 0.012190\n",
      "Epoch: 52 \tTraining Loss5: 0.055693 \tTraining Loss6: 0.015808 \tTraining Loss7: 0.025073\n",
      "Epoch: 53 \tTraining Loss1: 0.128706 \tTraining Loss2: 0.013149 \tTraining Loss3: 0.016051 \tTraining Loss4: 0.015381\n",
      "Epoch: 53 \tTraining Loss5: 0.066549 \tTraining Loss6: 0.012277 \tTraining Loss7: 0.016922\n",
      "Epoch: 54 \tTraining Loss1: 0.132666 \tTraining Loss2: 0.014720 \tTraining Loss3: 0.019833 \tTraining Loss4: 0.010857\n",
      "Epoch: 54 \tTraining Loss5: 0.055674 \tTraining Loss6: 0.018712 \tTraining Loss7: 0.017283\n",
      "Epoch: 55 \tTraining Loss1: 0.130254 \tTraining Loss2: 0.015366 \tTraining Loss3: 0.017062 \tTraining Loss4: 0.013196\n",
      "Epoch: 55 \tTraining Loss5: 0.060512 \tTraining Loss6: 0.011991 \tTraining Loss7: 0.016937\n",
      "Epoch: 56 \tTraining Loss1: 0.128784 \tTraining Loss2: 0.015328 \tTraining Loss3: 0.014380 \tTraining Loss4: 0.018395\n",
      "Epoch: 56 \tTraining Loss5: 0.059239 \tTraining Loss6: 0.015336 \tTraining Loss7: 0.020416\n",
      "Epoch: 57 \tTraining Loss1: 0.130942 \tTraining Loss2: 0.016598 \tTraining Loss3: 0.014923 \tTraining Loss4: 0.009001\n",
      "Epoch: 57 \tTraining Loss5: 0.060026 \tTraining Loss6: 0.016128 \tTraining Loss7: 0.018901\n",
      "Epoch: 58 \tTraining Loss1: 0.130657 \tTraining Loss2: 0.014377 \tTraining Loss3: 0.016613 \tTraining Loss4: 0.011425\n",
      "Epoch: 58 \tTraining Loss5: 0.054315 \tTraining Loss6: 0.008930 \tTraining Loss7: 0.012349\n",
      "Epoch: 59 \tTraining Loss1: 0.132420 \tTraining Loss2: 0.019191 \tTraining Loss3: 0.018553 \tTraining Loss4: 0.015993\n",
      "Epoch: 59 \tTraining Loss5: 0.059839 \tTraining Loss6: 0.007407 \tTraining Loss7: 0.020375\n",
      "Epoch: 60 \tTraining Loss1: 0.129426 \tTraining Loss2: 0.015366 \tTraining Loss3: 0.019242 \tTraining Loss4: 0.017406\n",
      "Epoch: 60 \tTraining Loss5: 0.059502 \tTraining Loss6: 0.013099 \tTraining Loss7: 0.013035\n",
      "Epoch: 61 \tTraining Loss1: 0.131191 \tTraining Loss2: 0.010581 \tTraining Loss3: 0.016763 \tTraining Loss4: 0.012650\n",
      "Epoch: 61 \tTraining Loss5: 0.054655 \tTraining Loss6: 0.011613 \tTraining Loss7: 0.021525\n",
      "Epoch: 62 \tTraining Loss1: 0.129139 \tTraining Loss2: 0.013916 \tTraining Loss3: 0.018933 \tTraining Loss4: 0.015571\n",
      "Epoch: 62 \tTraining Loss5: 0.061159 \tTraining Loss6: 0.011847 \tTraining Loss7: 0.024078\n",
      "Epoch: 63 \tTraining Loss1: 0.129894 \tTraining Loss2: 0.013618 \tTraining Loss3: 0.015284 \tTraining Loss4: 0.014747\n",
      "Epoch: 63 \tTraining Loss5: 0.059438 \tTraining Loss6: 0.025765 \tTraining Loss7: 0.012928\n",
      "Epoch: 64 \tTraining Loss1: 0.129053 \tTraining Loss2: 0.015747 \tTraining Loss3: 0.016411 \tTraining Loss4: 0.008615\n",
      "Epoch: 64 \tTraining Loss5: 0.065821 \tTraining Loss6: 0.016827 \tTraining Loss7: 0.012979\n",
      "Epoch: 65 \tTraining Loss1: 0.126251 \tTraining Loss2: 0.015965 \tTraining Loss3: 0.018611 \tTraining Loss4: 0.016992\n",
      "Epoch: 65 \tTraining Loss5: 0.056934 \tTraining Loss6: 0.012979 \tTraining Loss7: 0.019422\n",
      "Epoch: 66 \tTraining Loss1: 0.130063 \tTraining Loss2: 0.011023 \tTraining Loss3: 0.013689 \tTraining Loss4: 0.025741\n",
      "Epoch: 66 \tTraining Loss5: 0.054113 \tTraining Loss6: 0.013843 \tTraining Loss7: 0.016756\n",
      "Epoch: 67 \tTraining Loss1: 0.132483 \tTraining Loss2: 0.017680 \tTraining Loss3: 0.018856 \tTraining Loss4: 0.010916\n",
      "Epoch: 67 \tTraining Loss5: 0.058075 \tTraining Loss6: 0.024624 \tTraining Loss7: 0.021743\n",
      "Epoch: 68 \tTraining Loss1: 0.132469 \tTraining Loss2: 0.016166 \tTraining Loss3: 0.018065 \tTraining Loss4: 0.015530\n",
      "Epoch: 68 \tTraining Loss5: 0.057516 \tTraining Loss6: 0.010768 \tTraining Loss7: 0.020412\n",
      "Epoch: 69 \tTraining Loss1: 0.130355 \tTraining Loss2: 0.015854 \tTraining Loss3: 0.016243 \tTraining Loss4: 0.011278\n",
      "Epoch: 69 \tTraining Loss5: 0.053983 \tTraining Loss6: 0.013147 \tTraining Loss7: 0.016360\n",
      "Epoch: 70 \tTraining Loss1: 0.127055 \tTraining Loss2: 0.012664 \tTraining Loss3: 0.016174 \tTraining Loss4: 0.012903\n",
      "Epoch: 70 \tTraining Loss5: 0.054631 \tTraining Loss6: 0.008517 \tTraining Loss7: 0.020929\n",
      "Epoch: 71 \tTraining Loss1: 0.129079 \tTraining Loss2: 0.016335 \tTraining Loss3: 0.015753 \tTraining Loss4: 0.012858\n",
      "Epoch: 71 \tTraining Loss5: 0.054237 \tTraining Loss6: 0.006146 \tTraining Loss7: 0.011825\n",
      "Epoch: 72 \tTraining Loss1: 0.129847 \tTraining Loss2: 0.014331 \tTraining Loss3: 0.014232 \tTraining Loss4: 0.024147\n",
      "Epoch: 72 \tTraining Loss5: 0.057111 \tTraining Loss6: 0.020305 \tTraining Loss7: 0.011601\n",
      "Epoch: 73 \tTraining Loss1: 0.126491 \tTraining Loss2: 0.013713 \tTraining Loss3: 0.017009 \tTraining Loss4: 0.011835\n",
      "Epoch: 73 \tTraining Loss5: 0.053181 \tTraining Loss6: 0.010934 \tTraining Loss7: 0.013397\n",
      "Epoch: 74 \tTraining Loss1: 0.125276 \tTraining Loss2: 0.011595 \tTraining Loss3: 0.015863 \tTraining Loss4: 0.014557\n",
      "Epoch: 74 \tTraining Loss5: 0.059123 \tTraining Loss6: 0.020965 \tTraining Loss7: 0.021507\n",
      "Epoch: 75 \tTraining Loss1: 0.131370 \tTraining Loss2: 0.020214 \tTraining Loss3: 0.015152 \tTraining Loss4: 0.010823\n",
      "Epoch: 75 \tTraining Loss5: 0.062097 \tTraining Loss6: 0.008558 \tTraining Loss7: 0.015086\n",
      "Epoch: 76 \tTraining Loss1: 0.126582 \tTraining Loss2: 0.015860 \tTraining Loss3: 0.018355 \tTraining Loss4: 0.010374\n",
      "Epoch: 76 \tTraining Loss5: 0.053765 \tTraining Loss6: 0.020551 \tTraining Loss7: 0.015415\n",
      "Epoch: 77 \tTraining Loss1: 0.129912 \tTraining Loss2: 0.011810 \tTraining Loss3: 0.016519 \tTraining Loss4: 0.030255\n",
      "Epoch: 77 \tTraining Loss5: 0.059944 \tTraining Loss6: 0.013297 \tTraining Loss7: 0.018405\n",
      "Epoch: 78 \tTraining Loss1: 0.132555 \tTraining Loss2: 0.016213 \tTraining Loss3: 0.016764 \tTraining Loss4: 0.012349\n",
      "Epoch: 78 \tTraining Loss5: 0.060229 \tTraining Loss6: 0.013935 \tTraining Loss7: 0.020643\n",
      "Epoch: 79 \tTraining Loss1: 0.131961 \tTraining Loss2: 0.013587 \tTraining Loss3: 0.016767 \tTraining Loss4: 0.012718\n",
      "Epoch: 79 \tTraining Loss5: 0.056588 \tTraining Loss6: 0.019448 \tTraining Loss7: 0.019452\n",
      "Epoch: 80 \tTraining Loss1: 0.133083 \tTraining Loss2: 0.013931 \tTraining Loss3: 0.014278 \tTraining Loss4: 0.013663\n",
      "Epoch: 80 \tTraining Loss5: 0.051165 \tTraining Loss6: 0.031735 \tTraining Loss7: 0.019806\n",
      "Epoch: 81 \tTraining Loss1: 0.131435 \tTraining Loss2: 0.016066 \tTraining Loss3: 0.017637 \tTraining Loss4: 0.018091\n",
      "Epoch: 81 \tTraining Loss5: 0.062071 \tTraining Loss6: 0.015727 \tTraining Loss7: 0.018444\n",
      "Epoch: 82 \tTraining Loss1: 0.131602 \tTraining Loss2: 0.017071 \tTraining Loss3: 0.017204 \tTraining Loss4: 0.011691\n",
      "Epoch: 82 \tTraining Loss5: 0.054867 \tTraining Loss6: 0.009485 \tTraining Loss7: 0.015456\n",
      "Epoch: 83 \tTraining Loss1: 0.132782 \tTraining Loss2: 0.013756 \tTraining Loss3: 0.014811 \tTraining Loss4: 0.081057\n",
      "Epoch: 83 \tTraining Loss5: 0.058441 \tTraining Loss6: 0.007743 \tTraining Loss7: 0.020405\n",
      "Epoch: 84 \tTraining Loss1: 0.128039 \tTraining Loss2: 0.014372 \tTraining Loss3: 0.014954 \tTraining Loss4: 0.025426\n",
      "Epoch: 84 \tTraining Loss5: 0.052150 \tTraining Loss6: 0.014643 \tTraining Loss7: 0.018325\n",
      "Epoch: 85 \tTraining Loss1: 0.124729 \tTraining Loss2: 0.014962 \tTraining Loss3: 0.017389 \tTraining Loss4: 0.016044\n",
      "Epoch: 85 \tTraining Loss5: 0.056906 \tTraining Loss6: 0.018435 \tTraining Loss7: 0.016500\n",
      "Epoch: 86 \tTraining Loss1: 0.129943 \tTraining Loss2: 0.013868 \tTraining Loss3: 0.015006 \tTraining Loss4: 0.010802\n",
      "Epoch: 86 \tTraining Loss5: 0.061440 \tTraining Loss6: 0.018682 \tTraining Loss7: 0.020659\n",
      "Epoch: 87 \tTraining Loss1: 0.127995 \tTraining Loss2: 0.016866 \tTraining Loss3: 0.018679 \tTraining Loss4: 0.015561\n",
      "Epoch: 87 \tTraining Loss5: 0.056971 \tTraining Loss6: 0.016756 \tTraining Loss7: 0.021131\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m  model4\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# trainmodels(epoch)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m  \u001b[43mtrainmodels2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m  \u001b[38;5;66;03m#test()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [7], line 53\u001b[0m, in \u001b[0;36mtrainmodels2\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m train_loss2 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss2\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     52\u001b[0m loss3 \u001b[38;5;241m=\u001b[39m criterion3(output3,target2)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mloss3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m optimizer3\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     55\u001b[0m train_loss3 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss3\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = [*range(250)] # you may increase this number to train a final model\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "for epoch in n_epochs:\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    model3.train()\n",
    "    model4.train()\n",
    "   # trainmodels(epoch)\n",
    "   \n",
    "    trainmodels2(epoch)\n",
    "    #test()\n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c25c3c85-dbbe-43d6-b010-6e7f5b323b16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T19:43:58.623153Z",
     "iopub.status.busy": "2023-06-18T19:43:58.622870Z",
     "iopub.status.idle": "2023-06-18T19:43:58.639779Z",
     "shell.execute_reply": "2023-06-18T19:43:58.639045Z",
     "shell.execute_reply.started": "2023-06-18T19:43:58.623123Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(), \"model1\")\n",
    "torch.save(model2.state_dict(), \"model2\")\n",
    "torch.save(model3.state_dict(), \"model3\")\n",
    "torch.save(model4.state_dict(), \"model4\")\n",
    "torch.save(model5.state_dict(), \"model5\")\n",
    "torch.save(model6.state_dict(), \"model6\")\n",
    "torch.save(model7.state_dict(), \"model7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3eb7310-74ba-4a0d-bace-093521fe0d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T18:37:59.476365Z",
     "iopub.status.busy": "2023-06-18T18:37:59.475806Z",
     "iopub.status.idle": "2023-06-18T18:38:28.232116Z",
     "shell.execute_reply": "2023-06-18T18:38:28.231303Z",
     "shell.execute_reply.started": "2023-06-18T18:37:59.476343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of airplane: 84% (849/1000)\n",
      "Test Accuracy of automobile: 88% (883/1000)\n",
      "Test Accuracy of  bird: 73% (738/1000)\n",
      "Test Accuracy of   cat: 64% (642/1000)\n",
      "Test Accuracy of  deer: 82% (820/1000)\n",
      "Test Accuracy of   dog: 73% (733/1000)\n",
      "Test Accuracy of  frog: 84% (847/1000)\n",
      "Test Accuracy of horse: 80% (803/1000)\n",
      "Test Accuracy of  ship: 86% (864/1000)\n",
      "Test Accuracy of truck: 86% (864/1000)\n",
      "\n",
      "Test Accuracy (Overall): 80% (8043/10000)\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        initialtarget = layer1transform(target,1)\n",
    "        output1 = model1(data)\n",
    "        _, pred = torch.max(output1, 1) \n",
    "        if pred == 0:\n",
    "            output2 = model2(data)\n",
    "            _, pred2 = torch.max(output2, 1)\n",
    "            if pred2 == 0:\n",
    "                output3 = model4(data)\n",
    "                _, predfinal = torch.max(output3, 1) \n",
    "            if pred2 == 1:\n",
    "                output3 = model5(data)\n",
    "                _, predfinal = torch.max(output3, 1)\n",
    "                predfinal = predfinal+2\n",
    "        if pred == 1:\n",
    "            output2 = model3(data)\n",
    "            _, pred2 = torch.max(output2, 1)\n",
    "            if pred2 == 0:\n",
    "                output3 = model6(data)\n",
    "                _, predfinal = torch.max(output3, 1) \n",
    "                predfinal = predfinal+5\n",
    "            if pred2 == 1:\n",
    "                output3 = model7(data)\n",
    "                _, predfinal = torch.max(output3, 1) \n",
    "                predfinal = predfinal+7\n",
    "        correct_tensor = predfinal.eq(target.data.view_as(predfinal))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        label = target.data[0]\n",
    "        class_correct[label] += correct.item()\n",
    "        class_total[label] += 1\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e40074-6b89-42b2-ad11-5d3feae62adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        initialtarget = layer1transform(target,1)\n",
    "        output1 = model1(data)\n",
    "        _, pred = torch.max(output1, 1) \n",
    "        if pred == 0:\n",
    "            output2 = model2(data)\n",
    "            _, pred2 = torch.max(output2, 1)\n",
    "            predfinal = pred2\n",
    "        if pred == 1:\n",
    "            output2 = model3(data)\n",
    "            _, pred2 = torch.max(output2, 1)\n",
    "            predfinal = pred2 +3\n",
    "        correct_tensor = predfinal.eq(target.data.view_as(predfinal))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        label = target.data[0]\n",
    "        class_correct[label] += correct.item()\n",
    "        class_total[label] += 1\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
