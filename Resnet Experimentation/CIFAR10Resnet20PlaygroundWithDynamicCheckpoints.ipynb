{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym2-5JKp6PMa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import scipy.io\n",
        "import copy\n",
        "from itertools import islice\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "\n",
        "random.seed(3242023)\n",
        "torch.manual_seed(3242023) # Seeded with a constant, so that behavior is deterministic.\n",
        "torch.cuda.manual_seed_all(3242023)\n",
        "\n",
        "# TODO not currently available torch.use_deterministic_algorithms(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    def print_memory_usage():\n",
        "        print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
        "        print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
        "        print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    def print_memory_usage(): print(\"Using cpu\")\n",
        "torch.set_default_device(device)\n",
        "\n",
        "\n",
        "print(f\"Pytorch running on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# n = lambda x: nn.GroupNorm(1, x)\n",
        "n = nn.BatchNorm2d\n",
        "\n",
        "def _weights_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = n(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = n(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     n(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = n(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3]).to(device)\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "oPHrtcXjBL6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRmL3XIJ3U_W"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def cov(mat):\n",
        "    \"\"\"\n",
        "    mat should be a matrix whose rows are separate samples.\n",
        "    \"\"\"\n",
        "    xbar = torch.zeros(mat.size()[1], device=device)\n",
        "    for row in mat:\n",
        "        xbar += row\n",
        "    xbar /= mat.size()[0]\n",
        "    matdelta = [row-xbar for row in mat]\n",
        "    print(f\"Squares of L2 norms of X-Xbar: {[torch.nansum(torch.square(row)) for row in matdelta]}\")\n",
        "    ans = []\n",
        "    for row in matdelta:\n",
        "        unsqueezed_row = row.unsqueeze(0)\n",
        "        ans.append(torch.matmul(unsqueezed_row.t(), unsqueezed_row))\n",
        "    return sum(ans)/(len(ans)-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVrXAHve6XDw"
      },
      "outputs": [],
      "source": [
        "def convert_to_one_hot(data): # Taken from https://stackoverflow.com/questions/36960320/convert-a-2d-matrix-to-a-3d-one-hot-matrix-numpy\n",
        "    \"\"\"\n",
        "    WARNING: mutates input, incrementing each value in it by 1\n",
        "    \"\"\"\n",
        "    data += 1\n",
        "    return np.squeeze((np.arange(data.max()) == data[...,None]-1).astype(int))\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # This intentionally doesn't include any data augmentation. That comes later (TODO it hasn't been implemented yet)\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "train, ans = zip(*trainset)\n",
        "test, test_ans = zip(*testset)\n",
        "train_mat = torch.stack(train).to(device)\n",
        "test_mat = torch.stack(test).to(device)\n",
        "ans_mat = convert_to_one_hot(np.array(ans))\n",
        "ans_mat_test = convert_to_one_hot(np.array(test_ans))\n",
        "print(train_mat.shape, test_mat.shape, ans_mat.shape, ans_mat_test.shape)\n",
        "\n",
        "def augment_train_data(train_data_in, labels):\n",
        "    temp = torch.cat([train_data_in, transforms.functional.hflip(train_data_in)], dim=0)\n",
        "    temp_labels = torch.cat([labels, labels], dim=0)\n",
        "    ans = torch.cat([temp, transforms.functional.vflip(temp)], dim=0)\n",
        "    ans_labels = torch.cat([temp_labels, temp_labels], dim=0)\n",
        "    return ans, ans_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq3EXBv-8xD5"
      },
      "outputs": [],
      "source": [
        "def get_params(layer):\n",
        "    return torch.cat((layer.weight.data, layer.bias.data.unsqueeze(1)), 1)\n",
        "\n",
        "def test_model(model, test_data, test_labels, microbatch_size=1000):\n",
        "    assert test_data.shape[0] % microbatch_size == 0 # TODO this isn't the most elegant\n",
        "    accuracies = []\n",
        "    for microbatch_idx in range(0, test_data.shape[0], microbatch_size):\n",
        "        test_out = model(test_data[microbatch_idx:microbatch_idx+microbatch_size])\n",
        "        accuracies.append(get_accuracy(test_out.detach(), test_labels[microbatch_idx:microbatch_idx+microbatch_size]))\n",
        "    return sum(accuracies)/len(accuracies)\n",
        "\n",
        "def clip_group_sample_grad(grad_list, max_norm=1.0):\n",
        "    reshaped_grad_list = []\n",
        "    for part_grad in grad_list:\n",
        "        reshaped_grad_list.append(torch.reshape(part_grad, (-1,)))\n",
        "    grad_tensor = torch.cat(reshaped_grad_list, 0)\n",
        "\n",
        "    l2_norm = torch.norm(grad_tensor, p=2)\n",
        "    print(f\"Current norm: {l2_norm}\")\n",
        "\n",
        "    if l2_norm > max_norm:\n",
        "        clip_strength = max_norm/l2_norm\n",
        "        for grad in grad_list:\n",
        "            grad *= clip_strength\n",
        "\n",
        "    return grad_list\n",
        "\n",
        "def measure_model_stability(models):\n",
        "    coeffs_list = []\n",
        "\n",
        "    for model in models:\n",
        "        coeffs_list.append(np.array(torch.cat([p.flatten() for p in model.parameters()]).detach().cpu()))\n",
        "\n",
        "    coeffs_list = [i.flatten() for i in coeffs_list]\n",
        "    coeffs_norms = [math.sqrt(np.dot(i, i)) for i in coeffs_list]\n",
        "\n",
        "    num_trials = len(models)\n",
        "\n",
        "    avg_coeff = sum(coeffs_list)/num_trials\n",
        "    coeff_diffs_list = [i-avg_coeff for i in coeffs_list]\n",
        "    coeffs_diffs_norms = [math.sqrt(np.dot(i, i)) for i in coeff_diffs_list]\n",
        "\n",
        "    avg_l2 = sum(coeffs_norms)/num_trials\n",
        "    avg_l2_deviation = sum(coeffs_diffs_norms)/num_trials\n",
        "\n",
        "    return avg_l2, avg_l2_deviation\n",
        "\n",
        "def train_models_divergent_one_step(models_in, optimizers, data_sets, label_sets, measure_stability=True, criterion=nn.CrossEntropyLoss(), test_data=None, test_labels=None, microbatch_size=100, clipping_func = lambda x: x, train_augment=True):\n",
        "    assert len(data_sets) == len(label_sets) and len(data_sets) == len(models_in)\n",
        "    accuracies = []\n",
        "    for i in range(len(data_sets)):\n",
        "        all_group_sample_gradients = []\n",
        "        if train_augment:\n",
        "            augmented_data, augmented_ans = augment_train_data(data_sets[i], label_sets[i])\n",
        "        else:\n",
        "            augmented_data, augmented_ans = data_sets[i], label_sets[i]\n",
        "        for j in range(0, len(data_sets[i]), microbatch_size):\n",
        "            microbatch = augmented_data[j:j+microbatch_size]\n",
        "            microbatch_ans = augmented_ans[j:j+microbatch_size]\n",
        "\n",
        "            out = models_in[i](microbatch)\n",
        "            loss = criterion(out, microbatch_ans)\n",
        "            loss.backward()\n",
        "\n",
        "            group_sample_gradients = clipping_func([p.grad.detach().clone() for p in models_in[i].parameters()])\n",
        "\n",
        "            all_group_sample_gradients.append(group_sample_gradients)\n",
        "            optimizers[i].zero_grad()\n",
        "\n",
        "        num_microbatches = len(all_group_sample_gradients) # MICROBATCHING IS HERE\n",
        "        processed_grads = [sum([idx[k] for idx in all_group_sample_gradients])/num_microbatches for k in range(len(all_group_sample_gradients[0]))]\n",
        "\n",
        "        for idx, p in enumerate(models_in[i].parameters()):\n",
        "            p.grad = processed_grads[idx]\n",
        "\n",
        "        optimizers[i].step()\n",
        "        if test_data is not None:\n",
        "            accuracy = test_model(models_in[i], test_data, test_labels)\n",
        "            print(f\"Model {i} has an accuracy of {accuracy}\")\n",
        "            accuracies.append(accuracy)\n",
        "    if measure_stability:\n",
        "        if len(accuracies) > 0:\n",
        "            return (measure_model_stability(models_in), sum(accuracies)/len(accuracies))\n",
        "        else:\n",
        "            return measure_model_stability(models_in)\n",
        "\n",
        "def train_models_divergent(train_set, train_ans, num_datapoints=10000, num_trials=8, steps=16, microbatch_size=100, test_data=None, test_labels=None, graph_results=True, clipping_threshold=1.0, base_model=None, train_augment=True):\n",
        "    \"\"\"\n",
        "    returns a list of tuples. Each tuple represents one epoch and has two\n",
        "    elements. The first element is the average l2 norm of the models after that\n",
        "    epoch. The second element is the average deviation in that same epoch.\n",
        "    \"\"\"\n",
        "    if microbatch_size <= 0:\n",
        "        microbatch_size = num_datapoints\n",
        "    data_sets, label_sets = random_sample(train_set, train_ans, num_datapoints, num_trials)\n",
        "    # data_sets, label_sets = random_sample_some_removed(train_set, train_ans, num_datapoints, 1, num_trials)\n",
        "    data_sets = torch.Tensor(data_sets).to(device)\n",
        "    label_sets = torch.Tensor(label_sets).to(device)\n",
        "    if test_data is not None:\n",
        "        test_data = torch.Tensor(test_data).to(device)\n",
        "        test_labels = torch.Tensor(test_labels).to(device)\n",
        "\n",
        "    if base_model is None:\n",
        "        base_model = resnet20()\n",
        "    models = [copy.deepcopy(base_model) for i in range(num_trials)]\n",
        "    optimizers = [torch.optim.SGD(m.parameters(), lr=0.1) for m in models] # TODO lower\n",
        "\n",
        "    stability_data = []\n",
        "    accuracy_data = []\n",
        "\n",
        "    if clipping_threshold <= 0:\n",
        "        temp_clip_func = lambda x: x\n",
        "    else:\n",
        "        temp_clip_func = lambda x: clip_group_sample_grad(x, clipping_threshold)\n",
        "\n",
        "    for step in range(steps):\n",
        "        if test_data is not None:\n",
        "            print(f\"Now beginning epoch {step}\")\n",
        "        results = train_models_divergent_one_step(models, optimizers, data_sets, label_sets, True, test_data=test_data, test_labels=test_labels, microbatch_size=microbatch_size, clipping_func=temp_clip_func, train_augment=train_augment)\n",
        "        stability_data.append(results[0])\n",
        "        if test_data is not None:\n",
        "            accuracy_data.append(results[1])\n",
        "\n",
        "    if graph_results:\n",
        "        l2_fractional_deviation = [data[1]/data[0] for data in stability_data]\n",
        "        plt.plot([0] + l2_fractional_deviation)\n",
        "    if test_data is not None:\n",
        "        return (stability_data, models, accuracy_data)\n",
        "    return (stability_data, models)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise_for_one_epoch(model, norm):\n",
        "    param_vector = nn.utils.parameters_to_vector(model.parameters())\n",
        "    num_params = len(param_vector)\n",
        "    noise = torch.randn(num_params)*norm\n",
        "    param_vector.add_(noise)\n",
        "    nn.utils.vector_to_parameters(param_vector, model.parameters())\n",
        "    return model\n",
        "\n",
        "def train_models_checkpointed(train_set, train_ans, test_set=None, test_ans=None, num_datapoints=10000, num_models=8, epochs_per_group=10, total_groups=10, microbatch_size=0, clipping_threshold=0, graph_results=False, noise_level=0.0, train_augment=True):\n",
        "    current_model = None\n",
        "    data = []\n",
        "    accuracies = []\n",
        "    for i in range(total_groups):\n",
        "        print(f\"\\nBeginning training group {i} of {total_groups}\\n\\n\")\n",
        "        out = train_models_divergent(train_set, train_ans, num_datapoints, num_models, epochs_per_group, microbatch_size, test_set, test_ans, False, clipping_threshold, current_model, train_augment)\n",
        "        current_model = out[1][0]\n",
        "        data.append(out[0])\n",
        "        if test_set is not None: accuracies.append(out[2])\n",
        "        if noise_level > 0.0:\n",
        "            add_noise_for_one_epoch(current_model, out[0][-1][1]*noise_level)\n",
        "\n",
        "    if graph_results: # TODO this needs to be completely revamped. As of right now, it's minimally useful.\n",
        "        for trial in data:\n",
        "            plt.plot([epoch[1]/epoch[0] for epoch in trial])\n",
        "        plt.show()\n",
        "        for trial in data:\n",
        "            plt.plot([epoch[1] for epoch in trial])\n",
        "        plt.show() # TODO this can and should be prettier\n",
        "    if test_set is not None: return (data, accuracies)\n",
        "    return data"
      ],
      "metadata": {
        "id": "R8vpmT8Dh5tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq9BmKV0LpwS"
      },
      "outputs": [],
      "source": [
        "def random_sample(data, labels, qty=1000, num_samples=1):\n",
        "    ans_data = []\n",
        "    ans_labels = []\n",
        "    dataset_size = data.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        listed = np.random.choice(range(dataset_size), qty, replace=False)\n",
        "        ans_data.append(np.take(data.cpu(), listed, axis=0))\n",
        "        ans_labels.append(np.take(labels, listed, axis=0))\n",
        "    return np.stack(ans_data), np.stack(ans_labels)\n",
        "\n",
        "def random_sample_some_removed(data, labels, qty=1000, samples_removed=1, num_samples=1):\n",
        "    ans_data = []\n",
        "    ans_labels = []\n",
        "    dataset_size = data.shape[0]\n",
        "    sample_pool = np.random.choice(range(dataset_size), qty, replace=False)\n",
        "    for i in range(num_samples):\n",
        "        listed = np.random.choice(sample_pool, qty-samples_removed, replace=False)\n",
        "        ans_data.append(np.take(data, listed, axis=0))\n",
        "        ans_labels.append(np.take(labels, listed, axis=0))\n",
        "    return np.stack(ans_data), np.stack(ans_labels)\n",
        "\n",
        "def grade_prediction_one_hot(prediction, truth):\n",
        "    return truth[np.argmax(prediction.cpu())] == 1\n",
        "\n",
        "def get_accuracy(output, ground_truth, grader = grade_prediction_one_hot):\n",
        "    samples = output.shape[0]\n",
        "    successes = 0\n",
        "    for sample in range(samples):\n",
        "        if grader(output[sample], ground_truth[sample]):\n",
        "            successes += 1\n",
        "    return successes/samples\n",
        "\n",
        "def linear_regression(train_data, train_ans):\n",
        "    if len(train_ans.shape) <= 2: # If data is not 1-hot\n",
        "        return np.linalg.lstsq(train_data, train_ans)[0]\n",
        "    ans = []\n",
        "    for batch in range(train_ans.shape[0]):\n",
        "        ans.append(np.linalg.lstsq(train_data[batch], train_ans[batch])[0])\n",
        "    return np.stack(ans)\n",
        "\n",
        "def test_stability(train_in, train_ans, num_removed = None, printout = True, train_printout = True, test_data = None, test_ans = None):\n",
        "    \"\"\"\n",
        "    Returns a tuple whose first element is the average model l2 norm and whose\n",
        "    second element is the standard deviation of l2 norms.\n",
        "    \"\"\"\n",
        "    coeffs_list = []\n",
        "    for i in range(train_in.shape[0]):\n",
        "        coeffs = np.linalg.lstsq(train_in[i], train_ans[i])[0]\n",
        "        if train_printout:\n",
        "            train_results = np.matmul(train_in[i], coeffs)\n",
        "            print(\"Train accuracy: \", get_accuracy(train_results, train_ans[i]))\n",
        "        if test_data is not None and test_ans is not None:\n",
        "            test_results = np.matmul(test_data, coeffs)\n",
        "            print(\"Test accuracy: \", get_accuracy(test_results, test_ans))\n",
        "        coeffs_list.append(coeffs)\n",
        "\n",
        "    coeffs = np.stack(coeffs_list).squeeze()\n",
        "    coeffs_list = [i.flatten() for i in coeffs_list]\n",
        "    coeffs_norms = [math.sqrt(np.dot(i, i)) for i in coeffs_list]\n",
        "\n",
        "    num_trials = len(coeffs_norms)\n",
        "\n",
        "    avg_coeff = sum(coeffs_list)/num_trials\n",
        "    coeff_diffs_list = [i-avg_coeff for i in coeffs_list]\n",
        "    coeffs_diffs_norms = [math.sqrt(np.dot(i, i)) for i in coeff_diffs_list]\n",
        "\n",
        "    avg_l2 = sum(coeffs_norms)/num_trials\n",
        "    avg_l2_deviation = sum(coeffs_diffs_norms)/num_trials\n",
        "\n",
        "    if printout:\n",
        "        print(\"\")\n",
        "        if num_removed == None:\n",
        "            print(f\"{num_trials} were conducted. Each trial contained {train_in.shape[1]} data points.\")\n",
        "            print(f\"Average l2 norm was {avg_l2:.2f}. We expect the average deviation to be equal to the average multiplied by the fraction of samples removed (e.g. for removing 1 sample from a set of 1000, divide by 1000)\")\n",
        "        else:\n",
        "            print(f\"{num_trials} trials were conducted. A bank of {train_in.shape[1]+num_removed} data points was used with each trial omitting {num_removed} data points\")\n",
        "            print(f\"Average l2 norm was {avg_l2:.2f}. We expect the average deviation to be equal to 1/{(train_in.shape[1]+num_removed)/num_removed}\")\n",
        "        print(f\"Average l2 norm deviation was {avg_l2_deviation:.2f}. This is equal to {100*avg_l2_deviation/avg_l2:.4f}% of the average l2 norm, or 1/{avg_l2/avg_l2_deviation:.1f}\")\n",
        "\n",
        "    return avg_l2, avg_l2_deviation\n",
        "\n",
        "# TODO when using small amounts of (raw, doing this with preprocessed data would be v. bad) data, use data augmentation (e.g. flip, crop, etc.) to act as if we have large amounts of data.\n",
        "# TODO this is the important one. From here on out, train a Resnet with 1,000 samples per group, 25 groups TODO add noise after each group proportional to l2 norm of that group. fine-tune # steps per group for maxmial privacy-utility tradeoff. TODO also try this with group-sample clipping.\n",
        "# TODO noise for the above is gaussian noise with norm equal to absolute l2 deviation.\n",
        "# TODO test BatchNorm vs GroupNorm in the Resnet20.\n",
        "\n",
        "stability_data = []\n",
        "for run_data in ((0, 0.0),): # ((100, 2.5),):\n",
        "    stability_data.append(train_models_checkpointed(train_mat, ans_mat, test_mat, ans_mat_test, 1000, 16, 16, 8, run_data[0], run_data[1], True, 0.0, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaQEUo97omrH"
      },
      "outputs": [],
      "source": [
        "# for hyperparams in stability_data:\n",
        "#     for series in hyperparams:\n",
        "#         plt.plot([0] + [i[1] for i in series])\n",
        "#     plt.show()\n",
        "print(len(hyperparams[0][0][0]))\n",
        "for hyperparams in stability_data:\n",
        "    list_of_points = [0]\n",
        "    for series in hyperparams[0]:\n",
        "        previous_stopping_point = list_of_points[-1]\n",
        "        for point in series:\n",
        "            print(point)\n",
        "            list_of_points.append(point[1]+previous_stopping_point)\n",
        "    plt.plot(list_of_points)\n",
        "    plt.show()\n",
        "    list_of_points = []\n",
        "    for series in hyperparams[1]:\n",
        "        for point in series:\n",
        "            list_of_points.append(point)\n",
        "    plt.plot(list_of_points)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRBcYR9bPr8Y"
      },
      "outputs": [],
      "source": [
        "for series in stability_data:\n",
        "    for i in series:\n",
        "        print(i[-1], end=\", \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}