{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61d34ee-8f05-4641-a5ee-9cd95297cb8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T02:43:21.704485Z",
     "iopub.status.busy": "2023-10-12T02:43:21.703490Z",
     "iopub.status.idle": "2023-10-12T02:43:23.308212Z",
     "shell.execute_reply": "2023-10-12T02:43:23.307105Z",
     "shell.execute_reply.started": "2023-10-12T02:43:21.704461Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms   \n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from numpy.linalg import norm\n",
    "from torch.cuda.random import device_count\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision \n",
    "import math\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243ee1f9-2b7e-473e-9286-70661bf6a0c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T04:41:25.271951Z",
     "iopub.status.busy": "2023-10-10T04:41:25.271606Z",
     "iopub.status.idle": "2023-10-10T04:41:28.518903Z",
     "shell.execute_reply": "2023-10-10T04:41:28.518300Z",
     "shell.execute_reply.started": "2023-10-10T04:41:25.271929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.random import device_count\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "transform1 = transforms.Compose(\n",
    "    [    \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4, 0.4822, 0.4465), (0.247, 0.243, 0.261)),   \n",
    "    ])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./', train=True,download=True,transform = transform1)\n",
    "indices = torch.randperm(len(trainset))[:5000]\n",
    "trainset1 = torch.utils.data.Subset(trainset, indices)\n",
    "modelloader = torch.utils.data.DataLoader(trainset1, shuffle=True, num_workers=2, batch_size = 100)\n",
    "validset = torchvision.datasets.CIFAR10(root='./', train=True,download=True, transform = transform) \n",
    "valid = list(range(40000, 50000,1))\n",
    "validset1 = torch.utils.data.Subset(validset, valid)\n",
    "validloader = torch.utils.data.DataLoader(validset1, shuffle=True, num_workers=4,batch_size = 100)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train = False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size = 20)\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e6b2dd-d332-4ac2-aa28-5836506cc9e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T02:43:27.463973Z",
     "iopub.status.busy": "2023-10-12T02:43:27.463192Z",
     "iopub.status.idle": "2023-10-12T02:43:28.249513Z",
     "shell.execute_reply": "2023-10-12T02:43:28.248357Z",
     "shell.execute_reply.started": "2023-10-12T02:43:27.463933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet20\n",
      "Total number of params 269722\n",
      "Total layers 20\n",
      "\n",
      "resnet32\n",
      "Total number of params 464154\n",
      "Total layers 32\n",
      "\n",
      "resnet44\n",
      "Total number of params 658586\n",
      "Total layers 44\n",
      "\n",
      "resnet56\n",
      "Total number of params 853018\n",
      "Total layers 56\n",
      "\n",
      "resnet110\n",
      "Total number of params 1727962\n",
      "Total layers 110\n",
      "\n",
      "resnet1202\n",
      "Total number of params 19421274\n",
      "Total layers 1202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
    "\n",
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    #print(classname)\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "def resnet32():\n",
    "    return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "\n",
    "def resnet44():\n",
    "    return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "\n",
    "def resnet56():\n",
    "    return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "\n",
    "def resnet110():\n",
    "    return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "\n",
    "def resnet1202():\n",
    "    return ResNet(BasicBlock, [200, 200, 200])\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    import numpy as np\n",
    "    total_params = 0\n",
    "\n",
    "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "        total_params += np.prod(x.data.numpy().shape)\n",
    "    print(\"Total number of params\", total_params)\n",
    "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for net_name in __all__:\n",
    "        if net_name.startswith('resnet'):\n",
    "            print(net_name)\n",
    "            test(globals()[net_name]())\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a746a4ea-9c7c-4ab0-8bbf-68d6e87a58c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T02:43:30.318170Z",
     "iopub.status.busy": "2023-10-12T02:43:30.317794Z",
     "iopub.status.idle": "2023-10-12T02:43:30.327691Z",
     "shell.execute_reply": "2023-10-12T02:43:30.326777Z",
     "shell.execute_reply.started": "2023-10-12T02:43:30.318141Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    test_loss = 0.\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    model.eval()\n",
    "    # iterate over test data\n",
    "    for data, target in testloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)    \n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(20):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(modelloader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "              classes[i], 100 * class_correct[i] / class_total[i],\n",
    "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48cf4ca-fc19-4709-81ec-73c5a9ce9fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T01:39:19.974163Z",
     "iopub.status.busy": "2023-06-10T01:39:19.973079Z",
     "iopub.status.idle": "2023-06-10T01:39:38.298642Z",
     "shell.execute_reply": "2023-06-10T01:39:38.297690Z",
     "shell.execute_reply.started": "2023-06-10T01:39:19.974130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.140683\n",
      "\n",
      "Test Accuracy of airplane: 53% (534/1000)\n",
      "Test Accuracy of automobile: 80% (809/1000)\n",
      "Test Accuracy of  bird: 58% (584/1000)\n",
      "Test Accuracy of   cat: 46% (469/1000)\n",
      "Test Accuracy of  deer: 55% (559/1000)\n",
      "Test Accuracy of   dog: 76% (769/1000)\n",
      "Test Accuracy of  frog: 79% (790/1000)\n",
      "Test Accuracy of horse: 72% (728/1000)\n",
      "Test Accuracy of  ship: 83% (839/1000)\n",
      "Test Accuracy of truck: 89% (890/1000)\n",
      "\n",
      "Test Accuracy (Overall): 69% (6971/10000)\n",
      "Test Loss: 9.147370\n",
      "\n",
      "Test Accuracy of airplane: 23% (230/1000)\n",
      "Test Accuracy of automobile: 37% (370/1000)\n",
      "Test Accuracy of  bird: 23% (238/1000)\n",
      "Test Accuracy of   cat: 47% (476/1000)\n",
      "Test Accuracy of  deer: 25% (256/1000)\n",
      "Test Accuracy of   dog: 85% (856/1000)\n",
      "Test Accuracy of  frog: 68% (687/1000)\n",
      "Test Accuracy of horse: 24% (246/1000)\n",
      "Test Accuracy of  ship: 86% (865/1000)\n",
      "Test Accuracy of truck: 89% (895/1000)\n",
      "\n",
      "Test Accuracy (Overall): 51% (5119/10000)\n",
      "conv1.weight Parameter containing:\n",
      "tensor([[[[ 0.7517,  0.6212, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.2255],\n",
      "          [ 0.0000, -0.0000, -0.2188]],\n",
      "\n",
      "         [[ 0.4143,  0.0000,  0.0000],\n",
      "          [-0.4730, -0.1900,  0.2227],\n",
      "          [-0.7591, -0.3777,  0.0000]],\n",
      "\n",
      "         [[-0.2926,  0.2390,  0.0000],\n",
      "          [-0.5327, -0.4229, -0.0000],\n",
      "          [-0.6251,  0.0000,  0.3133]]],\n",
      "\n",
      "\n",
      "        [[[-0.5803, -0.2359, -0.2609],\n",
      "          [ 0.0000, -0.3474,  0.6073],\n",
      "          [ 0.5785,  0.0000,  0.4693]],\n",
      "\n",
      "         [[ 0.4146, -0.0000,  0.3448],\n",
      "          [ 0.1939, -0.5181,  0.4007],\n",
      "          [-0.0000, -1.3339, -0.0000]],\n",
      "\n",
      "         [[ 0.5834,  0.4452, -0.5893],\n",
      "          [ 0.3560, -0.0000,  0.7594],\n",
      "          [ 0.3800, -0.8392, -0.5571]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.3060],\n",
      "          [ 0.5668, -0.0000, -0.2351],\n",
      "          [-0.0000, -0.0000, -0.5013]],\n",
      "\n",
      "         [[ 0.0000, -0.4334, -0.0000],\n",
      "          [ 0.1902, -0.1962, -0.7745],\n",
      "          [ 0.5358,  0.2175, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.2311, -0.3741],\n",
      "          [ 0.1921,  0.0000, -0.4017],\n",
      "          [ 0.5172,  0.0000, -0.4162]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.4046,  0.2181],\n",
      "          [ 0.2213, -0.7539,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.2825,  0.0000,  0.3196],\n",
      "          [ 0.2294, -0.0000, -0.0000],\n",
      "          [ 0.4793,  0.3000, -0.0000]],\n",
      "\n",
      "         [[ 0.2026,  0.0000, -0.4642],\n",
      "          [-0.0000,  0.2228, -0.0000],\n",
      "          [ 0.0000,  0.3075,  0.1958]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.1926,  0.0000],\n",
      "          [-1.0563, -0.6317, -0.0000]],\n",
      "\n",
      "         [[ 0.2144, -0.0000,  0.0000],\n",
      "          [ 0.2617,  0.2334,  0.0000],\n",
      "          [-0.2197,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.3605,  0.3022],\n",
      "          [ 0.5251,  1.0885, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2802, -0.5017,  0.0000],\n",
      "          [ 0.2953,  0.0000, -0.5377],\n",
      "          [ 0.4551,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.4145],\n",
      "          [-0.0000, -0.2006,  0.2994],\n",
      "          [ 0.3152,  0.5537,  0.3083]],\n",
      "\n",
      "         [[ 0.3696,  0.0000, -0.0000],\n",
      "          [ 0.2402,  0.0000, -0.6443],\n",
      "          [ 0.2046, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.2875, -0.2561],\n",
      "          [ 0.2448,  0.0000, -0.2934],\n",
      "          [ 0.2762,  0.0000, -0.3967]],\n",
      "\n",
      "         [[-0.1965, -0.2779, -0.0000],\n",
      "          [ 0.4812,  0.6286, -0.3415],\n",
      "          [-0.0000,  0.2977, -0.0000]],\n",
      "\n",
      "         [[-0.2568, -0.5646,  0.0000],\n",
      "          [ 0.2896, -0.0000, -0.0000],\n",
      "          [-0.2264, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7250,  0.4350,  0.5006],\n",
      "          [ 0.3172, -0.8959, -0.0000],\n",
      "          [-0.9611, -0.5617, -0.2917]],\n",
      "\n",
      "         [[-0.5407, -0.0000,  0.0000],\n",
      "          [ 0.4932,  0.4827,  0.0000],\n",
      "          [-0.2847, -0.0000,  0.3325]],\n",
      "\n",
      "         [[ 0.4266, -0.0000,  0.0000],\n",
      "          [ 0.6913, -0.2058, -0.4539],\n",
      "          [-0.5042, -0.3905,  0.2743]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.4981, -0.0000],\n",
      "          [ 0.2585,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.2834, -0.2000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.2106],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.4204]],\n",
      "\n",
      "         [[ 0.3755,  0.0000,  0.6179],\n",
      "          [ 0.0000, -0.2236,  0.5409],\n",
      "          [-0.2899,  0.3199, -0.3120]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6113, -0.0000, -0.0000],\n",
      "          [-0.2072, -0.3012,  0.2442],\n",
      "          [ 0.3824,  0.0000,  0.5541]],\n",
      "\n",
      "         [[-0.3136, -0.2145, -0.2133],\n",
      "          [-1.5243, -0.3357,  0.2875],\n",
      "          [-0.3986,  0.2758, -0.5503]],\n",
      "\n",
      "         [[-0.0000,  0.4944,  0.0000],\n",
      "          [-0.2118, -0.0000,  0.3850],\n",
      "          [-0.0000,  1.1584,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3414, -0.4229, -0.0000],\n",
      "          [ 0.0000,  0.2925, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.6977]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.2320,  0.2218,  0.4636],\n",
      "          [-0.0000, -0.0000,  0.4020]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.2830],\n",
      "          [ 0.2334,  0.0000,  0.0000],\n",
      "          [-0.2749, -0.3193,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.5161],\n",
      "          [-0.4849, -0.3969, -0.0000],\n",
      "          [-0.2125, -0.4940,  0.4921]],\n",
      "\n",
      "         [[ 0.3662, -0.2454, -0.0000],\n",
      "          [-0.0000, -0.3198,  0.4852],\n",
      "          [ 0.4829, -0.2558,  0.3766]],\n",
      "\n",
      "         [[-0.4960, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.2504, -0.0000,  0.5067]]],\n",
      "\n",
      "\n",
      "        [[[-0.2497,  0.5125,  0.5228],\n",
      "          [ 0.0000,  0.7768,  0.2304],\n",
      "          [ 0.0000, -0.3721, -1.1397]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.2199],\n",
      "          [ 0.0000,  0.4319, -0.2275],\n",
      "          [ 0.3704, -0.0000, -0.5084]],\n",
      "\n",
      "         [[-0.6020, -0.5938, -0.5775],\n",
      "          [-0.3176,  0.2577,  0.3511],\n",
      "          [ 0.6536,  0.3846,  0.4487]]],\n",
      "\n",
      "\n",
      "        [[[-0.2009, -0.4860, -0.8303],\n",
      "          [ 0.2971, -0.2379, -0.2032],\n",
      "          [ 0.2632, -0.0000, -0.3156]],\n",
      "\n",
      "         [[ 0.5523,  0.6713, -0.3126],\n",
      "          [ 0.0000,  0.2872,  0.3598],\n",
      "          [-0.2736,  0.3178, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.3389],\n",
      "          [ 0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.4441,  0.0000,  0.0000],\n",
      "          [-0.2598,  0.5042, -0.3825],\n",
      "          [-0.5921, -0.0000, -0.5838]],\n",
      "\n",
      "         [[ 0.0000,  0.5046,  0.4449],\n",
      "          [ 0.2400,  0.5467,  0.4278],\n",
      "          [ 0.0000,  0.0000,  0.5387]],\n",
      "\n",
      "         [[-0.4006, -0.0000, -0.5526],\n",
      "          [-0.2712, -0.0000, -0.3528],\n",
      "          [-0.0000,  0.5455, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.6268,  0.4761,  0.0000],\n",
      "          [-0.0000,  0.3829,  0.2847],\n",
      "          [ 0.3452,  0.0000, -0.5702]],\n",
      "\n",
      "         [[-0.5618,  0.6121, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.6374,  0.0000,  0.1987]],\n",
      "\n",
      "         [[-0.2202,  0.7176,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.2458],\n",
      "          [-0.4203, -0.3316, -0.5879]]]], device='cuda:0', requires_grad=True)\n",
      "bn1.weight Parameter containing:\n",
      "tensor([0.7648, 1.0797, 0.8115, 0.4953, 0.7835, 0.7044, 0.9440, 0.9355, 0.5558,\n",
      "        1.0272, 0.6338, 0.6418, 0.9505, 0.8073, 0.6546, 0.7535],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "bn1.bias Parameter containing:\n",
      "tensor([-0.0182,  0.6169,  0.0562, -0.0020,  0.0876, -0.0015, -0.0885,  0.2793,\n",
      "        -0.0787,  0.4174,  0.0550,  0.0810,  0.2335, -0.1317,  0.1283,  0.1715],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.0.conv1.weight Parameter containing:\n",
      "tensor([[[[-0.1275,  0.2809,  0.1176],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.2023]],\n",
      "\n",
      "         [[ 0.0000, -0.3366,  0.1760],\n",
      "          [ 0.3416,  0.1558,  0.2604],\n",
      "          [-0.1113, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1457, -0.0000, -0.0965],\n",
      "          [-0.1040, -0.1275, -0.0000],\n",
      "          [-0.1503, -0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1933,  0.4609, -0.0934],\n",
      "          [ 0.1817,  0.2887,  0.2161],\n",
      "          [-0.0000,  0.1234, -0.0931]],\n",
      "\n",
      "         [[ 0.3450, -0.0916,  0.2025],\n",
      "          [ 0.3123,  0.2309,  0.0960],\n",
      "          [-0.0000, -0.3061, -0.1341]],\n",
      "\n",
      "         [[-0.0000, -0.1309, -0.2121],\n",
      "          [-0.1350, -0.2544,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.2372]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0780],\n",
      "          [ 0.1992, -0.0000,  0.0000],\n",
      "          [-0.1487,  0.2166,  0.0000]],\n",
      "\n",
      "         [[-0.1968,  0.0000,  0.1023],\n",
      "          [ 0.2156, -0.1287, -0.1999],\n",
      "          [ 0.1205,  0.0898,  0.1488]],\n",
      "\n",
      "         [[ 0.0000,  0.0939,  0.2765],\n",
      "          [-0.0000,  0.0000, -0.1153],\n",
      "          [-0.1224, -0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1443,  0.0000,  0.0000],\n",
      "          [ 0.1225, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.1603, -0.1287]],\n",
      "\n",
      "         [[ 0.2411,  0.2278, -0.0000],\n",
      "          [ 0.1193,  0.1966,  0.2481],\n",
      "          [-0.1684, -0.0795, -0.1809]],\n",
      "\n",
      "         [[ 0.0000,  0.1434,  0.2928],\n",
      "          [ 0.2062, -0.0819, -0.1174],\n",
      "          [-0.0000,  0.0000, -0.2872]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0898],\n",
      "          [ 0.1520,  0.1832,  0.1064],\n",
      "          [-0.0000, -0.0000,  0.3556]],\n",
      "\n",
      "         [[ 0.2456,  0.1418,  0.0789],\n",
      "          [ 0.1475,  0.4514, -0.0799],\n",
      "          [-0.1563,  0.0000, -0.3894]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2638,  0.0000, -0.0000],\n",
      "          [-0.2004, -0.1436,  0.1200],\n",
      "          [-0.0000, -0.0000, -0.1775]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.1847,  0.0000, -0.0978],\n",
      "          [ 0.1690,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0995, -0.0964],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.1088,  0.2245,  0.1093]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2851, -0.2374, -0.0000],\n",
      "          [-0.1332, -0.2169,  0.0000],\n",
      "          [-0.0000,  0.1273, -0.1012]],\n",
      "\n",
      "         [[-0.0000,  0.2410,  0.2369],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.3154,  0.1876, -0.2253]],\n",
      "\n",
      "         [[-0.2772, -0.0875,  0.2044],\n",
      "          [-0.1053, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.2076,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0850, -0.1741, -0.1467],\n",
      "          [ 0.0000, -0.0000, -0.2622],\n",
      "          [ 0.1615,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.1760, -0.1184],\n",
      "          [-0.1583,  0.0000, -0.1290],\n",
      "          [ 0.2756,  0.2135,  0.1991]],\n",
      "\n",
      "         [[ 0.0848,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.2784, -0.1503],\n",
      "          [ 0.2957, -0.0000, -0.1767]]],\n",
      "\n",
      "\n",
      "        [[[-0.0781, -0.1114, -0.1165],\n",
      "          [ 0.2183,  0.0000,  0.2539],\n",
      "          [ 0.0804, -0.1037,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.3385,  0.0000],\n",
      "          [-0.2876,  0.1746,  0.0000],\n",
      "          [-0.0000, -0.1424, -0.2907]],\n",
      "\n",
      "         [[ 0.2196,  0.0000,  0.1409],\n",
      "          [ 0.0947, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0862,  0.0000, -0.0000],\n",
      "          [ 0.1158,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0842]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.2351, -0.0000,  0.0000],\n",
      "          [-0.1997,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.2577,  0.0000,  0.2729],\n",
      "          [ 0.0000,  0.0000,  0.1530],\n",
      "          [-0.0922,  0.0000,  0.1003]]],\n",
      "\n",
      "\n",
      "        [[[-0.1283, -0.1621,  0.0000],\n",
      "          [ 0.0000,  0.1293, -0.0000],\n",
      "          [ 0.2038,  0.0000,  0.1210]],\n",
      "\n",
      "         [[ 0.3285, -0.1231,  0.3006],\n",
      "          [ 0.0957, -0.1821, -0.1330],\n",
      "          [-0.1748, -0.2897, -0.1075]],\n",
      "\n",
      "         [[-0.0949, -0.1211, -0.2296],\n",
      "          [-0.1451, -0.0000,  0.1477],\n",
      "          [ 0.1960,  0.0000,  0.1470]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.1179,  0.0000],\n",
      "          [-0.1688, -0.2273, -0.1472],\n",
      "          [-0.2165, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1640, -0.1535,  0.0000],\n",
      "          [ 0.1954, -0.0808, -0.0798],\n",
      "          [ 0.1612, -0.2090,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.2329],\n",
      "          [-0.0000, -0.0908, -0.2079],\n",
      "          [-0.2110, -0.3584, -0.1765]]]], device='cuda:0', requires_grad=True)\n",
      "layer1.0.bn1.weight Parameter containing:\n",
      "tensor([0.8865, 0.9128, 0.8684, 0.6739, 1.0327, 1.0234, 0.8605, 0.9167, 1.0255,\n",
      "        1.1207, 0.8363, 0.7878, 0.9108, 0.8333, 1.0604, 0.9045],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.0.bn1.bias Parameter containing:\n",
      "tensor([-0.0224, -0.2529, -0.0716, -0.1089, -0.0807, -0.0407, -0.0578,  0.1081,\n",
      "        -0.1888,  0.2570,  0.0336, -0.1077, -0.0786,  0.2045, -0.1789, -0.0063],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.0.conv2.weight Parameter containing:\n",
      "tensor([[[[ 0.2724,  0.1741,  0.1922],\n",
      "          [-0.0000, -0.0000, -0.3387],\n",
      "          [-0.2411, -0.2455, -0.4391]],\n",
      "\n",
      "         [[-0.1386,  0.1110, -0.0000],\n",
      "          [ 0.2230, -0.0824, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.1844]],\n",
      "\n",
      "         [[ 0.1132,  0.1853, -0.1177],\n",
      "          [-0.1908, -0.1660, -0.0000],\n",
      "          [-0.1292, -0.0993, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1740, -0.1243, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.2327],\n",
      "          [-0.0000, -0.1188,  0.0000]],\n",
      "\n",
      "         [[ 0.1638,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.1267, -0.0000],\n",
      "          [-0.0000, -0.2708, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.1019,  0.1729],\n",
      "          [-0.1482, -0.2446, -0.0000],\n",
      "          [ 0.1876,  0.2663,  0.1661]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0975],\n",
      "          [ 0.1836, -0.0918,  0.2819],\n",
      "          [ 0.0000, -0.0900, -0.1009]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.1612],\n",
      "          [ 0.0000, -0.2251,  0.0000],\n",
      "          [-0.0000, -0.1000, -0.1171]],\n",
      "\n",
      "         [[-0.1306, -0.3479,  0.1078],\n",
      "          [ 0.2176, -0.2178,  0.1495],\n",
      "          [ 0.1079, -0.2450, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1389,  0.1006, -0.0000],\n",
      "          [-0.1252, -0.1408, -0.2222],\n",
      "          [ 0.0000, -0.0000, -0.3903]],\n",
      "\n",
      "         [[ 0.1524,  0.2146,  0.1612],\n",
      "          [-0.1292,  0.1451, -0.0000],\n",
      "          [-0.1294, -0.0869,  0.1791]],\n",
      "\n",
      "         [[-0.0000, -0.1141, -0.0000],\n",
      "          [ 0.2568, -0.0000,  0.1952],\n",
      "          [ 0.3139, -0.1127, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.1055, -0.1420],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0959,  0.1180,  0.0000]],\n",
      "\n",
      "         [[-0.1456, -0.1804, -0.0950],\n",
      "          [-0.1468, -0.0000, -0.1242],\n",
      "          [-0.2194, -0.0000,  0.0980]],\n",
      "\n",
      "         [[-0.1351, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0877]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1255,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.2919,  0.0000,  0.2205]],\n",
      "\n",
      "         [[-0.0914, -0.2715, -0.2985],\n",
      "          [ 0.1020, -0.0000, -0.1180],\n",
      "          [-0.1006, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.1307],\n",
      "          [ 0.0000,  0.1178,  0.0000],\n",
      "          [-0.0000,  0.1078, -0.0891]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1212, -0.0000,  0.0000],\n",
      "          [ 0.1666, -0.0851,  0.0975],\n",
      "          [-0.1120, -0.2462, -0.1433]],\n",
      "\n",
      "         [[-0.1399, -0.2498, -0.2974],\n",
      "          [-0.3283, -0.0792, -0.3430],\n",
      "          [-0.0000, -0.1490, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1854,  0.2258],\n",
      "          [ 0.2855,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.1709]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0954,  0.0000,  0.2001],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.2555, -0.0000],\n",
      "          [ 0.2144,  0.0000,  0.1614],\n",
      "          [ 0.0000,  0.1113, -0.1226]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.1195],\n",
      "          [-0.0000,  0.0886, -0.1669],\n",
      "          [-0.1887,  0.2275, -0.1312]]],\n",
      "\n",
      "\n",
      "        [[[-0.1503, -0.0000, -0.0000],\n",
      "          [-0.1761, -0.0992, -0.1215],\n",
      "          [-0.1056, -0.0000, -0.0998]],\n",
      "\n",
      "         [[-0.1291, -0.1697, -0.0000],\n",
      "          [ 0.0000, -0.1619, -0.3181],\n",
      "          [ 0.2950, -0.0000, -0.1212]],\n",
      "\n",
      "         [[ 0.1546,  0.2567,  0.1685],\n",
      "          [ 0.0000,  0.2253,  0.3189],\n",
      "          [-0.0000, -0.1118,  0.2744]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2074,  0.3453,  0.0000],\n",
      "          [ 0.1548, -0.0000, -0.1042],\n",
      "          [-0.0000, -0.4136, -0.2529]],\n",
      "\n",
      "         [[ 0.1677,  0.0000,  0.1272],\n",
      "          [-0.0000, -0.0000,  0.1297],\n",
      "          [-0.0000, -0.0000, -0.1150]],\n",
      "\n",
      "         [[ 0.1323, -0.0000,  0.2514],\n",
      "          [-0.2403,  0.2325, -0.1418],\n",
      "          [ 0.2552,  0.1655, -0.2959]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.1256],\n",
      "          [ 0.0000,  0.1782,  0.0000],\n",
      "          [ 0.1048,  0.3180,  0.1883]],\n",
      "\n",
      "         [[-0.0946, -0.0000, -0.1804],\n",
      "          [-0.0000, -0.1983, -0.3606],\n",
      "          [ 0.0842, -0.2610, -0.2941]],\n",
      "\n",
      "         [[ 0.1747,  0.1425,  0.1335],\n",
      "          [ 0.1929, -0.0000, -0.0000],\n",
      "          [ 0.1631, -0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1372,  0.0000, -0.1550],\n",
      "          [-0.1619, -0.3217, -0.1495],\n",
      "          [-0.1110, -0.0858,  0.0000]],\n",
      "\n",
      "         [[-0.1269, -0.0000, -0.2376],\n",
      "          [-0.0000, -0.1318, -0.3111],\n",
      "          [ 0.0000, -0.1514, -0.2061]],\n",
      "\n",
      "         [[ 0.0000, -0.1199, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.3429],\n",
      "          [ 0.0000,  0.1870,  0.0000]]]], device='cuda:0', requires_grad=True)\n",
      "layer1.0.bn2.weight Parameter containing:\n",
      "tensor([0.6456, 0.8370, 0.6814, 0.6158, 0.6724, 0.5619, 0.6454, 0.7175, 0.8102,\n",
      "        0.9589, 0.7641, 0.8381, 0.8542, 0.7080, 0.7021, 0.7013],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.0.bn2.bias Parameter containing:\n",
      "tensor([ 0.0814, -0.2853, -0.2153,  0.2974,  0.1565, -0.1263, -0.0602,  0.0249,\n",
      "        -0.1405, -0.0491,  0.0151,  0.0269, -0.1610, -0.0698, -0.0620,  0.0744],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.1.conv1.weight Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.1746,  0.1923],\n",
      "          [ 0.1328, -0.0000, -0.1473],\n",
      "          [ 0.0000,  0.1475,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.1939, -0.2150],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.2817, -0.2756]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.1437, -0.0949, -0.1915],\n",
      "          [-0.1189,  0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1065, -0.0000, -0.0000],\n",
      "          [ 0.2021,  0.0767,  0.0000],\n",
      "          [ 0.1152,  0.0763,  0.1834]],\n",
      "\n",
      "         [[ 0.3061, -0.1251, -0.0000],\n",
      "          [-0.0000, -0.2121,  0.1853],\n",
      "          [ 0.0000, -0.0000,  0.0882]],\n",
      "\n",
      "         [[ 0.0892, -0.0000,  0.2334],\n",
      "          [ 0.1206,  0.1702, -0.0000],\n",
      "          [-0.0935,  0.0000, -0.2443]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.1321,  0.0932,  0.1197],\n",
      "          [ 0.0000, -0.1506, -0.2968]],\n",
      "\n",
      "         [[-0.0000,  0.1566, -0.1509],\n",
      "          [-0.0000,  0.0000, -0.1777],\n",
      "          [ 0.0000,  0.0951, -0.1115]],\n",
      "\n",
      "         [[-0.1440, -0.0000, -0.0000],\n",
      "          [-0.0919, -0.1473, -0.1561],\n",
      "          [ 0.0000, -0.0000,  0.1092]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1371, -0.1199,  0.0773],\n",
      "          [ 0.0000, -0.0923,  0.0000],\n",
      "          [-0.0773,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1101, -0.1473, -0.0000],\n",
      "          [-0.1486, -0.0000, -0.1290],\n",
      "          [-0.1602, -0.0850,  0.0000]],\n",
      "\n",
      "         [[-0.1535,  0.0000,  0.0000],\n",
      "          [ 0.0896, -0.0000, -0.1408],\n",
      "          [-0.0000, -0.1463,  0.1586]]],\n",
      "\n",
      "\n",
      "        [[[-0.0841,  0.1041, -0.3244],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.3925,  0.0000, -0.1432]],\n",
      "\n",
      "         [[-0.1294,  0.1371,  0.1961],\n",
      "          [-0.0931, -0.2127,  0.0000],\n",
      "          [ 0.1165,  0.0799,  0.1737]],\n",
      "\n",
      "         [[ 0.1359,  0.3626, -0.1641],\n",
      "          [ 0.1669, -0.0000, -0.0000],\n",
      "          [-0.0959, -0.0000, -0.3212]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1510,  0.1520, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.2751],\n",
      "          [-0.1282, -0.1157, -0.2222]],\n",
      "\n",
      "         [[ 0.0000, -0.1326,  0.0000],\n",
      "          [-0.1675,  0.0000, -0.0000],\n",
      "          [-0.1345,  0.0000, -0.1997]],\n",
      "\n",
      "         [[-0.1870, -0.1349, -0.2666],\n",
      "          [ 0.0000, -0.0927,  0.1283],\n",
      "          [-0.1288,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1841,  0.0000,  0.1100],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.1198]],\n",
      "\n",
      "         [[ 0.0879,  0.1947,  0.0000],\n",
      "          [-0.0000, -0.1002, -0.2377],\n",
      "          [-0.2423,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1612,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.1665],\n",
      "          [-0.1414, -0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1332,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.1729, -0.0961],\n",
      "          [ 0.1160,  0.1075, -0.1905]],\n",
      "\n",
      "         [[-0.1177, -0.1512,  0.1522],\n",
      "          [-0.0000,  0.1792, -0.1680],\n",
      "          [-0.2433, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1399,  0.0000, -0.0960],\n",
      "          [-0.1853, -0.2052, -0.2033],\n",
      "          [ 0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2251,  0.1661,  0.1083],\n",
      "          [ 0.1321,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.2049,  0.1599]],\n",
      "\n",
      "         [[ 0.2501, -0.1269,  0.0791],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.1102, -0.1973]],\n",
      "\n",
      "         [[ 0.0000,  0.1611,  0.1014],\n",
      "          [-0.0000, -0.1832,  0.0000],\n",
      "          [ 0.2170, -0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.1339, -0.3493],\n",
      "          [-0.0000,  0.2258, -0.0943],\n",
      "          [-0.1121, -0.1965, -0.0000]],\n",
      "\n",
      "         [[ 0.0787, -0.0000, -0.2025],\n",
      "          [ 0.2019,  0.1938, -0.1538],\n",
      "          [-0.1424, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.2137, -0.0869, -0.1124],\n",
      "          [-0.0776,  0.2061,  0.1700],\n",
      "          [-0.1150,  0.1545, -0.2716]]],\n",
      "\n",
      "\n",
      "        [[[-0.1468, -0.0000, -0.0000],\n",
      "          [-0.1714, -0.1777, -0.1616],\n",
      "          [-0.1509, -0.3021, -0.2264]],\n",
      "\n",
      "         [[ 0.2133,  0.0000,  0.0000],\n",
      "          [ 0.1095, -0.0932, -0.1733],\n",
      "          [-0.2475,  0.0000, -0.1133]],\n",
      "\n",
      "         [[-0.3484,  0.0000, -0.1256],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.2571, -0.1388, -0.0000]],\n",
      "\n",
      "         [[-0.1373,  0.0000, -0.1645],\n",
      "          [-0.1643, -0.0000,  0.1909],\n",
      "          [ 0.2540, -0.1791,  0.1458]],\n",
      "\n",
      "         [[-0.0000,  0.2288,  0.0000],\n",
      "          [-0.1768,  0.2710,  0.0000],\n",
      "          [-0.2230,  0.1098,  0.0867]]]], device='cuda:0', requires_grad=True)\n",
      "layer1.1.bn1.weight Parameter containing:\n",
      "tensor([0.8842, 0.6954, 0.8351, 0.7396, 0.8229, 0.8248, 0.8791, 0.8267, 0.8125,\n",
      "        1.3687, 1.0426, 0.7769, 0.9901, 0.8737, 0.9338, 0.8714],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.1.bn1.bias Parameter containing:\n",
      "tensor([-0.0538, -0.1642, -0.1567,  0.0273, -0.0485, -0.0688,  0.0169, -0.0969,\n",
      "        -0.1088, -0.2311,  0.0660, -0.1336, -0.0901, -0.1433, -0.1639, -0.1504],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.1.conv2.weight Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.1866,  0.2455]],\n",
      "\n",
      "         [[-0.0000,  0.2684,  0.1219],\n",
      "          [-0.1318, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.2199,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.2174],\n",
      "          [-0.1748, -0.1219, -0.1419],\n",
      "          [-0.1060,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.1284,  0.0860,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.1244, -0.1080, -0.1583],\n",
      "          [-0.0000,  0.1102, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0989, -0.1559],\n",
      "          [-0.1162, -0.2082, -0.0000],\n",
      "          [-0.0000, -0.3019,  0.1399]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.1987, -0.1628,  0.3290],\n",
      "          [-0.0000, -0.1891,  0.0000]],\n",
      "\n",
      "         [[ 0.1352, -0.1421, -0.0000],\n",
      "          [ 0.1442, -0.1292, -0.1547],\n",
      "          [-0.1364,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0993, -0.0821, -0.0861],\n",
      "          [-0.0000, -0.1026, -0.0000],\n",
      "          [ 0.0000, -0.1081, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1660, -0.1892,  0.1718],\n",
      "          [ 0.2511,  0.2115,  0.1955],\n",
      "          [ 0.1338, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.1039, -0.1982],\n",
      "          [ 0.1252,  0.0000,  0.1269],\n",
      "          [ 0.0794,  0.3007,  0.0000]],\n",
      "\n",
      "         [[ 0.1542,  0.3656,  0.0833],\n",
      "          [ 0.0000,  0.2199, -0.1414],\n",
      "          [ 0.0000,  0.0000, -0.3174]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0802,  0.0995],\n",
      "          [ 0.0000, -0.1275,  0.2753],\n",
      "          [ 0.0000, -0.2078,  0.1125]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.1739,  0.0888, -0.1144],\n",
      "          [ 0.3140,  0.1119,  0.0768]],\n",
      "\n",
      "         [[ 0.0000,  0.0982,  0.0000],\n",
      "          [ 0.1500,  0.0000, -0.1622],\n",
      "          [ 0.2710,  0.0877,  0.1733]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.1787, -0.2931],\n",
      "          [ 0.0000, -0.2016, -0.1370],\n",
      "          [ 0.1809,  0.1147,  0.2390]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.1364, -0.0000, -0.0757],\n",
      "          [-0.1465, -0.0000,  0.1128]],\n",
      "\n",
      "         [[-0.1441, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0938,  0.1562],\n",
      "          [-0.2608, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0932,  0.2224, -0.0879],\n",
      "          [ 0.1506,  0.0925, -0.1655],\n",
      "          [-0.1083, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.2042],\n",
      "          [ 0.0875,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1783,  0.3076,  0.2707],\n",
      "          [-0.1209,  0.0000,  0.1268],\n",
      "          [-0.1748,  0.3458,  0.1473]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0787,  0.1928],\n",
      "          [ 0.3181,  0.1856,  0.1300],\n",
      "          [ 0.1074,  0.1600,  0.1395]],\n",
      "\n",
      "         [[ 0.2455,  0.0000,  0.0000],\n",
      "          [ 0.1451,  0.0941,  0.1791],\n",
      "          [-0.0000, -0.1087,  0.1409]],\n",
      "\n",
      "         [[ 0.1267, -0.0000, -0.0000],\n",
      "          [-0.1282, -0.0971,  0.0000],\n",
      "          [-0.1367,  0.0000, -0.0816]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2723,  0.0000, -0.1181],\n",
      "          [-0.0000, -0.1961,  0.0994],\n",
      "          [-0.1969, -0.1689,  0.0924]],\n",
      "\n",
      "         [[-0.0904, -0.0000, -0.0000],\n",
      "          [-0.2223,  0.0000, -0.0000],\n",
      "          [-0.2308, -0.1121, -0.1156]],\n",
      "\n",
      "         [[-0.1725, -0.0000,  0.0000],\n",
      "          [-0.1164, -0.1847,  0.0000],\n",
      "          [ 0.0000,  0.1158, -0.2077]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.1212, -0.2086],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0845]],\n",
      "\n",
      "         [[ 0.0827,  0.1029,  0.0000],\n",
      "          [-0.0000,  0.2189,  0.1092],\n",
      "          [ 0.0000, -0.0000,  0.1133]],\n",
      "\n",
      "         [[ 0.0000,  0.2115, -0.0000],\n",
      "          [-0.1463,  0.1933, -0.0000],\n",
      "          [-0.0000,  0.1154,  0.1779]]],\n",
      "\n",
      "\n",
      "        [[[-0.1130, -0.0000, -0.0000],\n",
      "          [ 0.1767,  0.0000,  0.0000],\n",
      "          [-0.1254,  0.0000, -0.1981]],\n",
      "\n",
      "         [[ 0.0963,  0.0896,  0.0799],\n",
      "          [-0.1083, -0.1739, -0.0842],\n",
      "          [-0.0000,  0.1466,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0809,  0.1890],\n",
      "          [ 0.0000, -0.0998, -0.0947],\n",
      "          [-0.1943, -0.1349, -0.1590]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0987,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.1435, -0.1567],\n",
      "          [ 0.1180, -0.0000,  0.2310]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.3057, -0.1979],\n",
      "          [-0.0000, -0.1868, -0.1195]],\n",
      "\n",
      "         [[-0.3439, -0.1113, -0.0000],\n",
      "          [-0.0000, -0.1225, -0.0000],\n",
      "          [ 0.2725,  0.0000, -0.0000]]]], device='cuda:0', requires_grad=True)\n",
      "layer1.1.bn2.weight Parameter containing:\n",
      "tensor([0.5396, 0.6205, 0.6325, 0.5261, 0.5419, 0.6213, 0.7149, 0.6643, 0.5886,\n",
      "        0.6377, 0.7206, 0.6260, 0.4878, 0.6954, 0.5718, 0.6857],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.1.bn2.bias Parameter containing:\n",
      "tensor([ 0.0266, -0.0194, -0.0062, -0.0525, -0.0374,  0.0173,  0.0428,  0.0400,\n",
      "        -0.0220,  0.0086, -0.0016, -0.3547, -0.0336, -0.2722, -0.0654, -0.1194],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.2.conv1.weight Parameter containing:\n",
      "tensor([[[[-0.2550,  0.1541,  0.1588],\n",
      "          [-0.0000, -0.0000, -0.1134],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0997,  0.2508,  0.2322],\n",
      "          [-0.2255, -0.1387,  0.2972],\n",
      "          [ 0.0000, -0.2439, -0.0000]],\n",
      "\n",
      "         [[-0.1636,  0.3280, -0.0000],\n",
      "          [-0.1136, -0.0871,  0.0953],\n",
      "          [-0.1765,  0.0000, -0.0999]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1910,  0.0000, -0.1195],\n",
      "          [ 0.1607,  0.0717, -0.1247],\n",
      "          [-0.0913, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.1518, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.1614],\n",
      "          [ 0.1391, -0.0859, -0.1734]],\n",
      "\n",
      "         [[ 0.3131,  0.1569,  0.0000],\n",
      "          [-0.0000, -0.1048,  0.1005],\n",
      "          [ 0.0000, -0.0794,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0897, -0.0000],\n",
      "          [-0.1555, -0.0000, -0.1653],\n",
      "          [-0.1376,  0.0980,  0.2438]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.2203],\n",
      "          [-0.1871, -0.0918,  0.0000],\n",
      "          [-0.1405, -0.0000, -0.0885]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.2104],\n",
      "          [ 0.0000,  0.0859, -0.1037],\n",
      "          [-0.1675, -0.0000, -0.1702]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0721,  0.3148,  0.0000],\n",
      "          [-0.0000,  0.1389,  0.1367],\n",
      "          [-0.0000, -0.0000, -0.1651]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0903],\n",
      "          [-0.1034, -0.1505, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.2689, -0.0000,  0.3221],\n",
      "          [-0.1390, -0.2994,  0.1714]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.1505,  0.1298],\n",
      "          [ 0.0783,  0.1097,  0.3604]],\n",
      "\n",
      "         [[ 0.0000,  0.1493,  0.0000],\n",
      "          [ 0.1314, -0.1538, -0.1735],\n",
      "          [ 0.0000,  0.1507,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.1358, -0.1130],\n",
      "          [-0.1665,  0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1915,  0.1772,  0.3119],\n",
      "          [ 0.3228,  0.0919,  0.1559],\n",
      "          [ 0.0000, -0.0000, -0.0799]],\n",
      "\n",
      "         [[ 0.1438,  0.1020,  0.1638],\n",
      "          [ 0.2020, -0.0000,  0.0849],\n",
      "          [ 0.2760, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0808,  0.0000, -0.0000],\n",
      "          [ 0.0840, -0.0781,  0.0000],\n",
      "          [-0.1280,  0.0736, -0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4318, -0.3468, -0.0000],\n",
      "          [-0.1821, -0.1867, -0.1531],\n",
      "          [-0.0868,  0.0859, -0.1391]],\n",
      "\n",
      "         [[-0.2341, -0.0000, -0.1847],\n",
      "          [-0.1072, -0.2144,  0.0000],\n",
      "          [-0.0000, -0.1295,  0.1862]],\n",
      "\n",
      "         [[-0.1772, -0.0000,  0.0000],\n",
      "          [-0.2829,  0.0983, -0.0000],\n",
      "          [-0.1772, -0.2662, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.1842],\n",
      "          [-0.0000,  0.0000, -0.1253],\n",
      "          [ 0.1353, -0.1128, -0.1268]],\n",
      "\n",
      "         [[ 0.1934,  0.2919,  0.0000],\n",
      "          [ 0.0000, -0.0982,  0.0953],\n",
      "          [-0.3042, -0.3018, -0.1325]],\n",
      "\n",
      "         [[ 0.0000,  0.2154,  0.1451],\n",
      "          [-0.0000,  0.1304,  0.1301],\n",
      "          [-0.0000, -0.1470, -0.0822]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.1545,  0.1048],\n",
      "          [ 0.0000, -0.1637,  0.2035],\n",
      "          [ 0.0000, -0.0000,  0.1654]],\n",
      "\n",
      "         [[ 0.2434, -0.0000, -0.0985],\n",
      "          [ 0.0000, -0.1268, -0.1011],\n",
      "          [-0.0953, -0.1035, -0.1299]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.1134],\n",
      "          [ 0.0969, -0.1546, -0.0848],\n",
      "          [-0.0000, -0.0000,  0.1305]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0777,  0.0886, -0.1861],\n",
      "          [-0.1396, -0.3058, -0.0783],\n",
      "          [-0.0000, -0.1875, -0.0899]],\n",
      "\n",
      "         [[-0.1689,  0.0000,  0.0729],\n",
      "          [-0.1156, -0.0000, -0.0000],\n",
      "          [-0.3028, -0.0000, -0.3466]],\n",
      "\n",
      "         [[-0.0994, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0900],\n",
      "          [-0.2031,  0.1457,  0.1124]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.1349, -0.3578, -0.2702],\n",
      "          [-0.0939, -0.1147, -0.0848]],\n",
      "\n",
      "         [[-0.0000,  0.0877, -0.0000],\n",
      "          [-0.1029, -0.0000, -0.0000],\n",
      "          [ 0.1658,  0.3722,  0.3107]],\n",
      "\n",
      "         [[-0.0000, -0.1601, -0.3338],\n",
      "          [-0.2198, -0.2114,  0.0000],\n",
      "          [-0.0000,  0.0896,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.1767,  0.0758],\n",
      "          [-0.1847, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1443, -0.2087]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.2625],\n",
      "          [-0.0000, -0.2168,  0.0000],\n",
      "          [-0.0000,  0.1660,  0.0000]],\n",
      "\n",
      "         [[ 0.1123,  0.1000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0873, -0.0899, -0.1097]]]], device='cuda:0', requires_grad=True)\n",
      "layer1.2.bn1.weight Parameter containing:\n",
      "tensor([0.6990, 0.8799, 0.7698, 0.8990, 0.8486, 0.8363, 0.7490, 1.0753, 0.9324,\n",
      "        1.2434, 0.8167, 0.8651, 0.8266, 0.9171, 0.8636, 0.8326],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.2.bn1.bias Parameter containing:\n",
      "tensor([ 0.0239, -0.1517, -0.1139, -0.0785, -0.0756, -0.0729, -0.0930, -0.0921,\n",
      "        -0.0485, -0.3407, -0.0180,  0.1395, -0.2075, -0.1443, -0.1799, -0.2105],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.2.conv2.weight Parameter containing:\n",
      "tensor([[[[-0.1866,  0.0000,  0.1485],\n",
      "          [-0.0000, -0.0000,  0.1646],\n",
      "          [-0.1058,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0939, -0.1867, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.2259],\n",
      "          [-0.0000,  0.1183,  0.0886]],\n",
      "\n",
      "         [[-0.2479,  0.0000, -0.1073],\n",
      "          [-0.0794, -0.0000, -0.0920],\n",
      "          [ 0.1462,  0.0716, -0.0824]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0840,  0.0825, -0.0000],\n",
      "          [ 0.1384,  0.0752, -0.2400],\n",
      "          [ 0.2012, -0.0000, -0.3119]],\n",
      "\n",
      "         [[-0.0762, -0.0000, -0.1062],\n",
      "          [-0.1052, -0.0000, -0.2926],\n",
      "          [ 0.0000, -0.2239, -0.2548]],\n",
      "\n",
      "         [[ 0.0837,  0.0000,  0.0000],\n",
      "          [ 0.2572,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.1764, -0.0988]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.2122, -0.0000, -0.0945],\n",
      "          [-0.2325, -0.1368, -0.1083]],\n",
      "\n",
      "         [[ 0.1590,  0.1134,  0.0000],\n",
      "          [-0.0000, -0.0832,  0.0000],\n",
      "          [ 0.1040,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0716,  0.2270, -0.0000],\n",
      "          [-0.0950,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1352, -0.0668, -0.0686],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.2165],\n",
      "          [-0.0000,  0.1609, -0.2585]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.1820],\n",
      "          [-0.0000, -0.2208, -0.2619]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1398, -0.1381, -0.0746],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.2145, -0.1773,  0.2040]],\n",
      "\n",
      "         [[ 0.1198,  0.1684,  0.1861],\n",
      "          [ 0.1772,  0.1700,  0.2342],\n",
      "          [ 0.2573,  0.2771,  0.2074]],\n",
      "\n",
      "         [[ 0.0892,  0.0000,  0.1647],\n",
      "          [ 0.1425,  0.1267,  0.0923],\n",
      "          [ 0.0000, -0.0715, -0.1478]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0705,  0.0000],\n",
      "          [ 0.0775,  0.0000, -0.0664],\n",
      "          [ 0.1802,  0.0996, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.1878, -0.2865],\n",
      "          [ 0.0000,  0.0000, -0.1387],\n",
      "          [ 0.0753,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.1406, -0.1868, -0.0000],\n",
      "          [-0.0000,  0.0723, -0.0000],\n",
      "          [ 0.2401,  0.1236,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0704,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1040, -0.1001, -0.1295],\n",
      "          [-0.1526, -0.2563,  0.0000],\n",
      "          [-0.1167, -0.1068, -0.0000]],\n",
      "\n",
      "         [[ 0.0746,  0.1326,  0.0000],\n",
      "          [-0.0000,  0.0894,  0.0725],\n",
      "          [-0.0938,  0.0000, -0.1175]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.1164, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.1355],\n",
      "          [-0.1152, -0.0000,  0.1237]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.1012,  0.0000,  0.1970],\n",
      "          [ 0.2089,  0.0716,  0.0728]],\n",
      "\n",
      "         [[-0.0000, -0.1160,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.1181, -0.1141, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2922,  0.1747,  0.0000],\n",
      "          [-0.0814,  0.1014,  0.0000],\n",
      "          [-0.0000,  0.1468, -0.1623]],\n",
      "\n",
      "         [[ 0.0000,  0.1401,  0.0000],\n",
      "          [-0.1816, -0.1875,  0.0945],\n",
      "          [ 0.1148, -0.1039, -0.0000]],\n",
      "\n",
      "         [[ 0.1683,  0.3048,  0.0000],\n",
      "          [-0.0000,  0.1519,  0.0000],\n",
      "          [-0.2630, -0.2258, -0.1754]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0951,  0.0847, -0.0000],\n",
      "          [-0.0000, -0.1018, -0.1824],\n",
      "          [-0.1666, -0.1597, -0.1068]],\n",
      "\n",
      "         [[-0.0000, -0.0694,  0.1502],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.1213, -0.0000]],\n",
      "\n",
      "         [[ 0.1811,  0.1825,  0.1871],\n",
      "          [ 0.2305,  0.1568,  0.2581],\n",
      "          [ 0.1360,  0.1033,  0.1337]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0786, -0.1987],\n",
      "          [-0.0000,  0.2348,  0.1176],\n",
      "          [ 0.0000,  0.1374, -0.0723]],\n",
      "\n",
      "         [[-0.1125, -0.0000,  0.0000],\n",
      "          [-0.1143, -0.1727,  0.0000],\n",
      "          [ 0.0662, -0.1913, -0.1253]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0680],\n",
      "          [-0.0000,  0.0000,  0.2057],\n",
      "          [-0.0826, -0.1408,  0.0696]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.2403, -0.1188],\n",
      "          [-0.1158, -0.1194, -0.0000],\n",
      "          [-0.0000, -0.0955,  0.0761]],\n",
      "\n",
      "         [[-0.1035, -0.1938, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.1292,  0.0884, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1083,  0.0000],\n",
      "          [-0.0000,  0.2440,  0.0000],\n",
      "          [ 0.0000,  0.1310,  0.2125]]]], device='cuda:0', requires_grad=True)\n",
      "layer1.2.bn2.weight Parameter containing:\n",
      "tensor([0.8113, 0.7144, 0.7733, 0.5591, 0.7426, 0.5406, 0.6274, 0.5598, 0.7876,\n",
      "        0.8304, 0.5736, 0.6109, 0.6232, 0.7977, 0.7295, 0.7012],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer1.2.bn2.bias Parameter containing:\n",
      "tensor([-1.2357e-01, -6.7764e-05, -1.8557e-01,  1.7845e-02,  9.3674e-02,\n",
      "         5.9172e-02, -4.1693e-02,  2.4197e-02,  2.7786e-02, -1.2110e-01,\n",
      "        -8.7790e-02,  6.7086e-02,  1.0871e-01, -1.9682e-01,  7.7399e-02,\n",
      "        -3.0376e-02], device='cuda:0', requires_grad=True)\n",
      "layer2.0.conv1.weight Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.2610,  0.0000, -0.1115],\n",
      "          [ 0.0000,  0.0000, -0.1490]],\n",
      "\n",
      "         [[-0.0000, -0.0777,  0.0737],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0898, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0866,  0.0000],\n",
      "          [-0.0000,  0.1328,  0.2548],\n",
      "          [ 0.0659,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1077, -0.1432,  0.0989],\n",
      "          [-0.0000,  0.0912,  0.2206],\n",
      "          [ 0.0000,  0.1307, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0652,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.2648],\n",
      "          [-0.0000,  0.1198,  0.3236]],\n",
      "\n",
      "         [[-0.0000,  0.2151, -0.0000],\n",
      "          [ 0.1367,  0.0937,  0.0906],\n",
      "          [ 0.0000,  0.0855, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.1903],\n",
      "          [-0.0000,  0.0000,  0.1388],\n",
      "          [ 0.3262,  0.0935,  0.1844]],\n",
      "\n",
      "         [[-0.1334, -0.2227, -0.1113],\n",
      "          [-0.3179, -0.3823, -0.0920],\n",
      "          [-0.1516, -0.1669,  0.0000]],\n",
      "\n",
      "         [[-0.0790,  0.1555,  0.0000],\n",
      "          [ 0.0000,  0.1990, -0.0000],\n",
      "          [-0.0649, -0.1483, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1387, -0.0699, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.1257,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.2605, -0.0000, -0.1321],\n",
      "          [ 0.2251, -0.1733, -0.1286],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.2333, -0.0000,  0.0000],\n",
      "          [-0.1244,  0.1904, -0.0925],\n",
      "          [ 0.1952,  0.0000, -0.1551]]],\n",
      "\n",
      "\n",
      "        [[[-0.1890, -0.2234, -0.0000],\n",
      "          [-0.2175,  0.0798, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1880,  0.0939],\n",
      "          [-0.0000,  0.0000,  0.0668],\n",
      "          [-0.0000, -0.1734, -0.1111]],\n",
      "\n",
      "         [[-0.1017, -0.1860,  0.0683],\n",
      "          [-0.0748, -0.0000, -0.0000],\n",
      "          [-0.1452, -0.1398, -0.2016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1153,  0.1561, -0.1143],\n",
      "          [ 0.1359,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.2388],\n",
      "          [-0.0000,  0.3214,  0.2314],\n",
      "          [ 0.0945,  0.0806,  0.1650]],\n",
      "\n",
      "         [[ 0.0000, -0.1178, -0.0921],\n",
      "          [ 0.0927,  0.0000, -0.1805],\n",
      "          [ 0.0684,  0.0661,  0.2052]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0815,  0.0000, -0.0692],\n",
      "          [ 0.0000, -0.2223,  0.2333],\n",
      "          [ 0.0000, -0.1399,  0.0000]],\n",
      "\n",
      "         [[-0.1523, -0.1631, -0.2974],\n",
      "          [-0.0744, -0.1496, -0.1979],\n",
      "          [ 0.0000,  0.2765, -0.0824]],\n",
      "\n",
      "         [[-0.0000,  0.1105, -0.1874],\n",
      "          [-0.1252, -0.1491, -0.0874],\n",
      "          [ 0.0000, -0.0000,  0.1337]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.2437,  0.0000],\n",
      "          [-0.0000,  0.1517, -0.2434],\n",
      "          [-0.0000, -0.1176, -0.0000]],\n",
      "\n",
      "         [[-0.1955,  0.0000, -0.0658],\n",
      "          [-0.0753, -0.2385, -0.2000],\n",
      "          [ 0.0000, -0.1051, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0942],\n",
      "          [-0.0000, -0.0000, -0.1282],\n",
      "          [ 0.1092, -0.0669, -0.0727]]],\n",
      "\n",
      "\n",
      "        [[[-0.2512,  0.0000,  0.2176],\n",
      "          [-0.1658,  0.1621,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.1289]],\n",
      "\n",
      "         [[ 0.1212,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.1096,  0.1168],\n",
      "          [ 0.0000, -0.0654,  0.1228]],\n",
      "\n",
      "         [[-0.0886, -0.0000, -0.1459],\n",
      "          [ 0.0000,  0.0000,  0.0672],\n",
      "          [-0.0745,  0.1062,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.2377],\n",
      "          [ 0.1775,  0.0000, -0.0000],\n",
      "          [ 0.1164,  0.2508,  0.0692]],\n",
      "\n",
      "         [[ 0.0000, -0.1801, -0.0679],\n",
      "          [-0.3285, -0.0000,  0.0000],\n",
      "          [-0.1834, -0.1535,  0.1422]],\n",
      "\n",
      "         [[-0.1199, -0.0000,  0.2592],\n",
      "          [ 0.0000,  0.0000,  0.3658],\n",
      "          [ 0.0769,  0.1309, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.2191],\n",
      "          [-0.0000,  0.1318, -0.0000],\n",
      "          [ 0.0728, -0.1220,  0.0929]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0707],\n",
      "          [ 0.0000,  0.0000,  0.0834],\n",
      "          [ 0.0000,  0.0763,  0.1673]],\n",
      "\n",
      "         [[-0.0000, -0.0999,  0.1472],\n",
      "          [ 0.0000,  0.0000, -0.1844],\n",
      "          [-0.2115,  0.3046,  0.0977]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1066, -0.1007, -0.1787],\n",
      "          [-0.0000, -0.0821, -0.0000],\n",
      "          [-0.0736,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0884, -0.1218],\n",
      "          [ 0.0000, -0.0867, -0.1934],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.1140,  0.0000],\n",
      "          [-0.0762,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]]], device='cuda:0', requires_grad=True)\n",
      "layer2.0.bn1.weight Parameter containing:\n",
      "tensor([0.8266, 0.8936, 0.9134, 0.8213, 0.8324, 0.8920, 0.9175, 0.8393, 0.8912,\n",
      "        0.8502, 0.9001, 0.8853, 1.0005, 0.9355, 0.9006, 0.8893, 0.8662, 0.8498,\n",
      "        0.8556, 0.8711, 0.8431, 0.9810, 0.8646, 0.8425, 0.8243, 0.8912, 0.9412,\n",
      "        0.9226, 0.7072, 0.7951, 0.8393, 0.8446], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "layer2.0.bn1.bias Parameter containing:\n",
      "tensor([-0.1059, -0.0200,  0.1326, -0.0192, -0.0508,  0.0953, -0.0271,  0.0649,\n",
      "        -0.0966, -0.0142, -0.1612,  0.0027, -0.1312,  0.0269, -0.0389, -0.0624,\n",
      "        -0.0478, -0.0875, -0.0231,  0.0946, -0.0423, -0.0355, -0.1083, -0.1208,\n",
      "        -0.0425, -0.1014, -0.0157, -0.0484, -0.0604, -0.0242,  0.0497, -0.0839],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer2.0.conv2.weight Parameter containing:\n",
      "tensor([[[[ 0.0586,  0.0698, -0.0000],\n",
      "          [ 0.0000, -0.0519, -0.1864],\n",
      "          [-0.0000,  0.0901, -0.0000]],\n",
      "\n",
      "         [[ 0.0706,  0.0891,  0.0000],\n",
      "          [ 0.1063, -0.0000,  0.1535],\n",
      "          [ 0.1631,  0.1519,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0941],\n",
      "          [ 0.0713,  0.0000,  0.0000],\n",
      "          [ 0.0603,  0.0000,  0.0616]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.1629,  0.0000],\n",
      "          [-0.1047,  0.0957,  0.0000],\n",
      "          [-0.0693, -0.0856,  0.0000]],\n",
      "\n",
      "         [[-0.0887, -0.0601,  0.0000],\n",
      "          [-0.0905,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0906, -0.0501, -0.1913],\n",
      "          [-0.1763, -0.1091,  0.0739],\n",
      "          [-0.0879, -0.1023, -0.0542]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.1711,  0.0814],\n",
      "          [ 0.0961,  0.1702, -0.0000],\n",
      "          [-0.0804,  0.0000, -0.0816]],\n",
      "\n",
      "         [[ 0.1098,  0.0857,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0543,  0.0000,  0.0677]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.1020],\n",
      "          [ 0.0000,  0.0513, -0.0881],\n",
      "          [ 0.1381, -0.0610, -0.1290]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0622, -0.0000,  0.1643],\n",
      "          [ 0.0000,  0.0563,  0.0910],\n",
      "          [ 0.0000,  0.0000,  0.0557]],\n",
      "\n",
      "         [[ 0.0503,  0.0000, -0.1260],\n",
      "          [ 0.0000, -0.1206, -0.1576],\n",
      "          [-0.0696,  0.0000,  0.2279]],\n",
      "\n",
      "         [[ 0.1884,  0.0959,  0.1256],\n",
      "          [-0.0000,  0.0675,  0.1819],\n",
      "          [ 0.0000,  0.0676,  0.1758]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0947,  0.0800, -0.0000],\n",
      "          [ 0.1284,  0.0000,  0.0000],\n",
      "          [ 0.1167,  0.1289,  0.0915]],\n",
      "\n",
      "         [[-0.0995,  0.0731,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.1135],\n",
      "          [ 0.0000, -0.0884, -0.0817]],\n",
      "\n",
      "         [[ 0.1301,  0.0772,  0.1388],\n",
      "          [-0.0000, -0.0000, -0.1042],\n",
      "          [ 0.2152,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.1042,  0.1098],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0612]],\n",
      "\n",
      "         [[ 0.0000,  0.1634, -0.0637],\n",
      "          [-0.1325, -0.0930, -0.1043],\n",
      "          [-0.0928,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0678,  0.0000],\n",
      "          [ 0.0687, -0.1685, -0.1160],\n",
      "          [-0.0000,  0.0537, -0.1248]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1332, -0.0000, -0.0000],\n",
      "          [-0.0803,  0.0000,  0.0736],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1102,  0.0814,  0.0894],\n",
      "          [-0.0732, -0.0000, -0.0000],\n",
      "          [-0.0599, -0.1710,  0.0000]],\n",
      "\n",
      "         [[ 0.0551, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0915, -0.0997],\n",
      "          [ 0.1702, -0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1909, -0.0881, -0.2045],\n",
      "          [ 0.0000, -0.0000, -0.0783],\n",
      "          [-0.0647,  0.1204,  0.0571]],\n",
      "\n",
      "         [[ 0.0000,  0.0691, -0.0732],\n",
      "          [-0.1620,  0.1929,  0.0822],\n",
      "          [ 0.0000,  0.0586,  0.1247]],\n",
      "\n",
      "         [[-0.0000, -0.0980, -0.0000],\n",
      "          [ 0.0519, -0.1869,  0.0000],\n",
      "          [ 0.1047,  0.1310, -0.2272]]],\n",
      "\n",
      "\n",
      "        [[[-0.1900,  0.2334, -0.1175],\n",
      "          [ 0.0000,  0.0000, -0.0627],\n",
      "          [-0.0583, -0.0000, -0.0889]],\n",
      "\n",
      "         [[-0.0503, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0877,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0881, -0.0000],\n",
      "          [-0.0000, -0.0620,  0.0534],\n",
      "          [ 0.0000,  0.0000,  0.1613]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.0692,  0.0000],\n",
      "          [-0.0714,  0.1254,  0.0000],\n",
      "          [ 0.0710, -0.0000,  0.0892]],\n",
      "\n",
      "         [[-0.0000,  0.1521,  0.0549],\n",
      "          [-0.0000,  0.1258,  0.0869],\n",
      "          [ 0.0000, -0.0000,  0.0855]],\n",
      "\n",
      "         [[ 0.0776, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0622],\n",
      "          [ 0.1091, -0.0689,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0744,  0.1674,  0.0000],\n",
      "          [ 0.0966,  0.0569,  0.0724],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0522,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0534, -0.0000,  0.0868],\n",
      "          [ 0.0917, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0665, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0609,  0.0000],\n",
      "          [ 0.1130, -0.0000, -0.0000],\n",
      "          [-0.0521, -0.0899,  0.0000]],\n",
      "\n",
      "         [[-0.0565,  0.0719, -0.0552],\n",
      "          [ 0.0890,  0.0000, -0.1018],\n",
      "          [ 0.0750,  0.0000,  0.2522]],\n",
      "\n",
      "         [[-0.0696, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.1004, -0.1138],\n",
      "          [-0.0000, -0.0641,  0.0515]]]], device='cuda:0', requires_grad=True)\n",
      "layer2.0.bn2.weight Parameter containing:\n",
      "tensor([0.8242, 0.8578, 0.7901, 0.9794, 0.7972, 0.8766, 0.9160, 0.8270, 0.7967,\n",
      "        0.7305, 0.9141, 0.7134, 0.6351, 0.5836, 0.6168, 0.7207, 0.8673, 0.8062,\n",
      "        0.7485, 0.7539, 0.7389, 0.9079, 0.6744, 0.6226, 0.9738, 0.9271, 0.8876,\n",
      "        0.9058, 0.8339, 0.8348, 0.8715, 0.9251], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "layer2.0.bn2.bias Parameter containing:\n",
      "tensor([-0.0589, -0.2693, -0.0381,  0.0032, -0.1741, -0.1286, -0.1094, -0.0521,\n",
      "        -0.0480, -0.1262, -0.0483, -0.0368, -0.3014, -0.1773, -0.0459, -0.1498,\n",
      "        -0.0045, -0.0832, -0.1234, -0.0717, -0.0788, -0.2269, -0.0540,  0.2089,\n",
      "        -0.0611, -0.1069, -0.0951, -0.1471, -0.1942, -0.0676, -0.0774, -0.0168],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer2.1.conv1.weight Parameter containing:\n",
      "tensor([[[[-0.0000, -0.1252, -0.1453],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0617,  0.0000]],\n",
      "\n",
      "         [[-0.1512, -0.0000,  0.0000],\n",
      "          [-0.0606, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0846]],\n",
      "\n",
      "         [[-0.0716, -0.0529, -0.0000],\n",
      "          [-0.0916,  0.0000,  0.0000],\n",
      "          [ 0.0778,  0.0550,  0.1115]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0554,  0.0000],\n",
      "          [-0.0000, -0.0720, -0.0518],\n",
      "          [ 0.0000, -0.0995, -0.0536]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.1386,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0811,  0.1134]],\n",
      "\n",
      "         [[-0.0539,  0.0000, -0.0000],\n",
      "          [-0.0871, -0.1005, -0.1578],\n",
      "          [-0.0000, -0.0000, -0.0646]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0521, -0.0000, -0.0000],\n",
      "          [-0.1158,  0.0523,  0.0000],\n",
      "          [ 0.0711,  0.0000,  0.0549]],\n",
      "\n",
      "         [[-0.0000,  0.1439,  0.0000],\n",
      "          [ 0.1800,  0.0739, -0.0000],\n",
      "          [-0.0735, -0.0557,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.1134,  0.0000],\n",
      "          [ 0.0000, -0.1151,  0.1142],\n",
      "          [-0.1275,  0.0000, -0.0790]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.1374,  0.0000],\n",
      "          [-0.1137, -0.0610,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.1155]],\n",
      "\n",
      "         [[ 0.2293, -0.0000,  0.0548],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0558,  0.0541,  0.0000]],\n",
      "\n",
      "         [[ 0.1274, -0.1469, -0.0912],\n",
      "          [ 0.0000,  0.0749,  0.0727],\n",
      "          [-0.0000,  0.0000, -0.0856]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.1983,  0.0000],\n",
      "          [-0.0847,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0899,  0.0000,  0.1828],\n",
      "          [-0.1497, -0.1438,  0.0000],\n",
      "          [-0.0712,  0.0000,  0.1158]],\n",
      "\n",
      "         [[-0.0824, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.1739, -0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1249, -0.0528, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.1020],\n",
      "          [-0.1848, -0.0902,  0.1092]],\n",
      "\n",
      "         [[ 0.1425,  0.1654, -0.0541],\n",
      "          [ 0.0000, -0.0000, -0.0653],\n",
      "          [ 0.1123,  0.0909,  0.0000]],\n",
      "\n",
      "         [[ 0.1133,  0.2526, -0.0982],\n",
      "          [ 0.0516, -0.0000,  0.0000],\n",
      "          [-0.0934, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0759, -0.0000,  0.0000],\n",
      "          [-0.1718, -0.1062,  0.0000],\n",
      "          [-0.1229, -0.0874, -0.0746]],\n",
      "\n",
      "         [[-0.0000,  0.0741, -0.0541],\n",
      "          [ 0.0000, -0.0000,  0.0602],\n",
      "          [-0.0000, -0.1222, -0.1434]],\n",
      "\n",
      "         [[-0.0000,  0.0674,  0.0000],\n",
      "          [-0.0000,  0.0680,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0714, -0.0741,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0983, -0.0574]],\n",
      "\n",
      "         [[ 0.1099,  0.0792,  0.1468],\n",
      "          [ 0.1262,  0.0000, -0.1427],\n",
      "          [-0.1616, -0.1709, -0.3401]],\n",
      "\n",
      "         [[-0.1234, -0.0000,  0.1690],\n",
      "          [-0.0977,  0.0687,  0.0989],\n",
      "          [-0.1443, -0.0000,  0.1849]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.1817],\n",
      "          [ 0.1165,  0.0531, -0.0817],\n",
      "          [ 0.1761,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1411,  0.1622,  0.0534],\n",
      "          [ 0.0784, -0.0000, -0.0859],\n",
      "          [-0.0000, -0.1304, -0.1478]],\n",
      "\n",
      "         [[-0.1282, -0.1416, -0.0000],\n",
      "          [-0.0605,  0.0000,  0.0000],\n",
      "          [-0.0527, -0.0000,  0.0969]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0874,  0.0702,  0.0555],\n",
      "          [-0.1742, -0.0901, -0.1157],\n",
      "          [-0.0000, -0.0576, -0.0698]],\n",
      "\n",
      "         [[ 0.0000,  0.0620,  0.0572],\n",
      "          [-0.0588,  0.0000, -0.1080],\n",
      "          [ 0.1695,  0.0605, -0.1055]],\n",
      "\n",
      "         [[ 0.0733,  0.1190,  0.0000],\n",
      "          [-0.0000, -0.1182,  0.0000],\n",
      "          [-0.2473, -0.1057,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0935],\n",
      "          [ 0.0753, -0.0819,  0.0000],\n",
      "          [-0.1762, -0.0000, -0.0976]],\n",
      "\n",
      "         [[ 0.0521,  0.0000, -0.1221],\n",
      "          [ 0.1797, -0.0708, -0.0000],\n",
      "          [ 0.1112, -0.0683, -0.1044]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0646],\n",
      "          [-0.1207, -0.0000, -0.0000],\n",
      "          [ 0.0551, -0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.1050],\n",
      "          [ 0.0563,  0.0000,  0.0000],\n",
      "          [-0.0921, -0.1268,  0.0563]],\n",
      "\n",
      "         [[ 0.0000, -0.0854, -0.1890],\n",
      "          [ 0.2146,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0724,  0.1287]],\n",
      "\n",
      "         [[ 0.1284, -0.0000,  0.0000],\n",
      "          [ 0.1310,  0.1194, -0.0871],\n",
      "          [ 0.2700, -0.1604, -0.1482]]]], device='cuda:0', requires_grad=True)\n",
      "layer2.1.bn1.weight Parameter containing:\n",
      "tensor([0.8785, 0.9368, 0.7921, 0.8154, 0.8450, 0.9107, 0.9058, 0.9203, 0.9805,\n",
      "        0.8222, 0.8437, 0.8330, 0.8521, 0.8573, 0.9054, 0.8415, 0.8082, 0.8606,\n",
      "        0.8489, 0.8008, 0.8366, 0.8526, 0.7497, 0.8838, 0.8792, 0.9998, 0.8306,\n",
      "        0.9147, 0.8962, 0.8099, 0.7790, 0.9703], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "layer2.1.bn1.bias Parameter containing:\n",
      "tensor([-0.1396, -0.0824, -0.1085, -0.0635, -0.2325, -0.2510, -0.0908, -0.0874,\n",
      "        -0.2102, -0.1729, -0.0381, -0.0211, -0.3030, -0.2407, -0.1616, -0.1803,\n",
      "        -0.1565, -0.0552, -0.0863, -0.0883, -0.1269, -0.1627, -0.1684, -0.2212,\n",
      "        -0.3157, -0.1599, -0.1687, -0.1474, -0.0611, -0.2148, -0.0942, -0.2070],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer2.1.conv2.weight Parameter containing:\n",
      "tensor([[[[-0.0685, -0.1102, -0.0564],\n",
      "          [ 0.0000, -0.0000,  0.0828],\n",
      "          [ 0.0902,  0.1031,  0.0512]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.1124],\n",
      "          [-0.0790, -0.1163, -0.0627],\n",
      "          [ 0.0000,  0.0912, -0.0000]],\n",
      "\n",
      "         [[ 0.0989,  0.1697, -0.0861],\n",
      "          [-0.0000,  0.1091,  0.0000],\n",
      "          [-0.0000,  0.1065, -0.1027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0818,  0.0636,  0.0791],\n",
      "          [-0.0942, -0.1564, -0.0000],\n",
      "          [-0.1032,  0.0000,  0.1162]],\n",
      "\n",
      "         [[ 0.1959,  0.0620,  0.1715],\n",
      "          [-0.1250,  0.0000,  0.1246],\n",
      "          [ 0.0000, -0.1256, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0773,  0.1409],\n",
      "          [-0.0536, -0.0924, -0.1269],\n",
      "          [ 0.0000, -0.0000, -0.0878]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0531, -0.0000,  0.0000],\n",
      "          [-0.1523, -0.0823, -0.1469],\n",
      "          [-0.1590, -0.1018, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0733,  0.0000],\n",
      "          [-0.0785, -0.0853, -0.1164],\n",
      "          [-0.0524, -0.1111, -0.0869]],\n",
      "\n",
      "         [[-0.2639, -0.2002, -0.0000],\n",
      "          [-0.0547, -0.2178, -0.0904],\n",
      "          [-0.0554, -0.1401,  0.0738]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0768, -0.0602, -0.0000],\n",
      "          [-0.0797,  0.0000, -0.0000],\n",
      "          [ 0.1434,  0.2042,  0.0742]],\n",
      "\n",
      "         [[-0.0000,  0.0572,  0.0878],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0692,  0.0000, -0.1254]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0588,  0.1157,  0.1090],\n",
      "          [ 0.0597, -0.1118,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1977,  0.0000,  0.1148],\n",
      "          [-0.0000,  0.0000,  0.0763],\n",
      "          [ 0.0578, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0547,  0.2438],\n",
      "          [-0.0000, -0.1716,  0.0000],\n",
      "          [-0.1964, -0.2412,  0.0544]],\n",
      "\n",
      "         [[-0.0657, -0.0614,  0.0000],\n",
      "          [-0.1025,  0.0000,  0.0680],\n",
      "          [ 0.0955, -0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0904,  0.0593,  0.1559],\n",
      "          [ 0.0602,  0.0000, -0.0726],\n",
      "          [ 0.2317,  0.0000, -0.0905]],\n",
      "\n",
      "         [[-0.0000,  0.0632, -0.0000],\n",
      "          [-0.1193, -0.0659,  0.0000],\n",
      "          [ 0.0000, -0.1164, -0.0598]],\n",
      "\n",
      "         [[-0.0817, -0.0000, -0.0000],\n",
      "          [ 0.0998, -0.0929,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0621,  0.0706, -0.0000],\n",
      "          [ 0.0500,  0.1148,  0.0878]],\n",
      "\n",
      "         [[ 0.0528, -0.0816,  0.0808],\n",
      "          [-0.1289,  0.0000,  0.0735],\n",
      "          [ 0.0000,  0.0690,  0.1438]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.1657],\n",
      "          [ 0.1208, -0.1669, -0.0000],\n",
      "          [ 0.0000, -0.0743,  0.1316]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.1566],\n",
      "          [-0.0000,  0.0000, -0.0804],\n",
      "          [-0.1021,  0.0649,  0.0000]],\n",
      "\n",
      "         [[-0.0714,  0.0000,  0.0838],\n",
      "          [-0.0000,  0.0000,  0.0885],\n",
      "          [ 0.0963,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1849,  0.2131,  0.0843],\n",
      "          [ 0.0863, -0.0000, -0.1375],\n",
      "          [ 0.0831, -0.0682,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0714, -0.0000],\n",
      "          [ 0.0000,  0.1201,  0.0000],\n",
      "          [-0.0580,  0.0554, -0.0594]],\n",
      "\n",
      "         [[ 0.0864, -0.1152,  0.0000],\n",
      "          [-0.0570, -0.1044,  0.0000],\n",
      "          [-0.1082, -0.0862, -0.0000]],\n",
      "\n",
      "         [[-0.1804, -0.0568,  0.0000],\n",
      "          [-0.0000, -0.0989, -0.0961],\n",
      "          [-0.1119, -0.0607, -0.2089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0540],\n",
      "          [ 0.1063, -0.1294, -0.1740],\n",
      "          [-0.0722, -0.0926,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0628, -0.0575],\n",
      "          [ 0.0000,  0.0000, -0.0542],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0824,  0.0961],\n",
      "          [-0.1689, -0.0000, -0.0000],\n",
      "          [-0.1895, -0.1075, -0.0616]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0549,  0.0728],\n",
      "          [-0.0515, -0.0950,  0.1000],\n",
      "          [-0.1223, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0500, -0.0591],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.1662, -0.1323, -0.0802]],\n",
      "\n",
      "         [[ 0.0000,  0.0893, -0.1368],\n",
      "          [ 0.1434, -0.0872, -0.0817],\n",
      "          [ 0.0634, -0.0653,  0.1182]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2642, -0.0703, -0.1299],\n",
      "          [-0.0881, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.1721]],\n",
      "\n",
      "         [[-0.1765, -0.0000, -0.1176],\n",
      "          [-0.0000, -0.0752,  0.0505],\n",
      "          [-0.0588,  0.0844,  0.0607]],\n",
      "\n",
      "         [[ 0.1475,  0.0000, -0.0000],\n",
      "          [ 0.1104,  0.0676,  0.0000],\n",
      "          [ 0.0942, -0.0000,  0.1178]]]], device='cuda:0', requires_grad=True)\n",
      "layer2.1.bn2.weight Parameter containing:\n",
      "tensor([0.9538, 0.7904, 0.8438, 0.8964, 0.8363, 0.8093, 0.8573, 0.7955, 0.8095,\n",
      "        0.7353, 0.8116, 0.6289, 0.8057, 0.7989, 0.7692, 0.8046, 0.6464, 0.7794,\n",
      "        0.6999, 0.7646, 0.8250, 0.9068, 0.7699, 0.5802, 0.8776, 0.8212, 0.7542,\n",
      "        0.8172, 0.8495, 0.7844, 0.9484, 0.8672], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "layer2.1.bn2.bias Parameter containing:\n",
      "tensor([-0.1332, -0.0892,  0.0246, -0.0664, -0.1645, -0.2684, -0.2148, -0.0799,\n",
      "        -0.2126, -0.1012, -0.0952, -0.1580, -0.4256, -0.2988, -0.1515, -0.2292,\n",
      "        -0.0553, -0.0924, -0.1306, -0.2783, -0.0758, -0.2949, -0.2119, -0.0749,\n",
      "        -0.1853, -0.1900, -0.0521, -0.2030, -0.1495, -0.2036, -0.1414, -0.2008],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer2.2.conv1.weight Parameter containing:\n",
      "tensor([[[[-0.0633, -0.0000, -0.0630],\n",
      "          [ 0.0633, -0.1365, -0.1034],\n",
      "          [-0.1111, -0.0000, -0.1615]],\n",
      "\n",
      "         [[ 0.1367,  0.0951,  0.1507],\n",
      "          [ 0.2097, -0.0000,  0.0873],\n",
      "          [ 0.0000,  0.0000,  0.1782]],\n",
      "\n",
      "         [[-0.0000, -0.1426, -0.0919],\n",
      "          [ 0.0950, -0.0688,  0.0576],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.1359, -0.0000],\n",
      "          [ 0.0743, -0.0953, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0566, -0.0000],\n",
      "          [ 0.0940,  0.1009,  0.0000],\n",
      "          [ 0.0000,  0.0832,  0.1596]],\n",
      "\n",
      "         [[ 0.0000, -0.0583, -0.0500],\n",
      "          [-0.0000,  0.0813, -0.0000],\n",
      "          [ 0.0000,  0.1017,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0859,  0.1144],\n",
      "          [-0.1589, -0.2163,  0.0490],\n",
      "          [-0.0000, -0.1992,  0.0000]],\n",
      "\n",
      "         [[-0.0756, -0.0849, -0.1613],\n",
      "          [-0.0547, -0.1850, -0.1478],\n",
      "          [-0.0963,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1620, -0.0809, -0.0637],\n",
      "          [-0.1191,  0.1140,  0.0000],\n",
      "          [-0.2110, -0.1215, -0.1150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1889, -0.0000,  0.0978],\n",
      "          [-0.0881, -0.0000, -0.1841],\n",
      "          [-0.0000, -0.0000, -0.1226]],\n",
      "\n",
      "         [[-0.1217,  0.0000, -0.0546],\n",
      "          [-0.1355, -0.1943,  0.0640],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.1743, -0.0576,  0.1097],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.1591],\n",
      "          [ 0.0000, -0.0000,  0.1513],\n",
      "          [ 0.1181,  0.0994, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0938,  0.0000],\n",
      "          [ 0.1791,  0.0000, -0.0926],\n",
      "          [-0.0729, -0.0593, -0.0536]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0888, -0.0000],\n",
      "          [-0.1268, -0.0000,  0.1339]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1991,  0.1681,  0.0000],\n",
      "          [ 0.1171,  0.1188, -0.0000],\n",
      "          [ 0.0652,  0.0000,  0.0783]],\n",
      "\n",
      "         [[ 0.1033, -0.0502,  0.1784],\n",
      "          [-0.0000, -0.0000, -0.0802],\n",
      "          [ 0.0000,  0.1806,  0.0000]],\n",
      "\n",
      "         [[-0.0648, -0.0000, -0.0000],\n",
      "          [-0.0754, -0.1650, -0.0000],\n",
      "          [-0.0942, -0.0865,  0.0594]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0967],\n",
      "          [ 0.0552, -0.0530, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0731]],\n",
      "\n",
      "         [[-0.0865, -0.0000, -0.0570],\n",
      "          [-0.0000, -0.0882, -0.1888],\n",
      "          [-0.1248, -0.1476, -0.0779]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0631],\n",
      "          [-0.1003, -0.0685,  0.1081],\n",
      "          [-0.0508, -0.0972,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.2002,  0.0849],\n",
      "          [-0.0517, -0.0647,  0.0000],\n",
      "          [-0.0000, -0.1261,  0.1090]],\n",
      "\n",
      "         [[-0.2015, -0.0698,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.1162],\n",
      "          [ 0.0683,  0.0623, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0745,  0.1197, -0.0000],\n",
      "          [-0.0000, -0.0828, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0602, -0.1163, -0.0000],\n",
      "          [-0.0672, -0.0652, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.1228]],\n",
      "\n",
      "         [[-0.1229, -0.0758, -0.0000],\n",
      "          [-0.0981,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.1439,  0.0537]],\n",
      "\n",
      "         [[ 0.0000,  0.0681,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0526],\n",
      "          [-0.0733,  0.0488, -0.3007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.1276],\n",
      "          [-0.1086, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0580,  0.1331]],\n",
      "\n",
      "         [[-0.0726,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0617, -0.1013, -0.1033]],\n",
      "\n",
      "         [[-0.0000, -0.1381,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0789,  0.0631, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0855, -0.0000,  0.0000],\n",
      "          [-0.0719, -0.0000, -0.0000],\n",
      "          [ 0.0926, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0579, -0.1113],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.1219, -0.1782, -0.2000]],\n",
      "\n",
      "         [[ 0.1356,  0.0831, -0.0543],\n",
      "          [ 0.0000, -0.0691, -0.0000],\n",
      "          [-0.1499, -0.0596, -0.0499]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2073,  0.0523,  0.0699],\n",
      "          [ 0.0980,  0.1393,  0.0000],\n",
      "          [-0.0633,  0.0000, -0.0795]],\n",
      "\n",
      "         [[ 0.0739,  0.1295, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0945],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.1030],\n",
      "          [ 0.1017,  0.0791,  0.2390],\n",
      "          [-0.0000,  0.0569,  0.1246]]]], device='cuda:0', requires_grad=True)\n",
      "layer2.2.bn1.weight Parameter containing:\n",
      "tensor([0.8328, 0.9758, 0.8014, 0.9290, 0.9376, 0.9185, 0.7951, 0.8358, 0.8836,\n",
      "        0.9127, 0.9467, 0.8462, 0.8513, 0.7971, 0.9096, 0.8871, 0.8453, 0.8494,\n",
      "        0.7656, 0.8687, 0.8593, 0.8360, 0.8757, 0.8072, 0.7744, 0.8874, 0.7948,\n",
      "        0.9070, 0.9684, 0.8695, 0.9002, 0.7761], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "layer2.2.bn1.bias Parameter containing:\n",
      "tensor([-0.1645, -0.1389, -0.1063, -0.1712, -0.2198, -0.0840, -0.1996, -0.0710,\n",
      "        -0.2975, -0.1220, -0.0591, -0.2117, -0.1612, -0.2251, -0.1263, -0.1130,\n",
      "        -0.1109, -0.0290, -0.2379, -0.2427, -0.2113, -0.0042, -0.1650, -0.0671,\n",
      "        -0.1154, -0.1877, -0.2532, -0.0699, -0.1958, -0.0442, -0.0745, -0.2218],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer2.2.conv2.weight Parameter containing:\n",
      "tensor([[[[-0.2782,  0.0640,  0.0000],\n",
      "          [-0.0000, -0.1909, -0.0660],\n",
      "          [-0.0000, -0.0459, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0933,  0.1124,  0.1359],\n",
      "          [ 0.0000, -0.0499,  0.0704]],\n",
      "\n",
      "         [[-0.0708, -0.0460, -0.0744],\n",
      "          [-0.0000,  0.0000, -0.0725],\n",
      "          [-0.0000, -0.1381,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0593,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0489,  0.0961],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0477,  0.1284],\n",
      "          [-0.0974, -0.0000, -0.0000],\n",
      "          [-0.0951, -0.0907,  0.0818]],\n",
      "\n",
      "         [[ 0.0566, -0.0000, -0.0000],\n",
      "          [ 0.1566, -0.0000,  0.0000],\n",
      "          [ 0.0798, -0.0573, -0.0465]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0968, -0.0808],\n",
      "          [-0.1654, -0.0000,  0.0000],\n",
      "          [-0.1522, -0.0000, -0.0490]],\n",
      "\n",
      "         [[ 0.0000, -0.1119,  0.0000],\n",
      "          [ 0.0700,  0.0719, -0.0736],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.1295,  0.1304],\n",
      "          [ 0.1918,  0.0505,  0.0760],\n",
      "          [ 0.1946,  0.1562,  0.0779]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0576, -0.1197,  0.0828],\n",
      "          [-0.0613, -0.0000, -0.0532],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0672,  0.0707,  0.1661],\n",
      "          [ 0.0000, -0.0000, -0.1135],\n",
      "          [ 0.0000, -0.0809, -0.1825]],\n",
      "\n",
      "         [[-0.0838,  0.0000,  0.0778],\n",
      "          [-0.1149, -0.0000,  0.0000],\n",
      "          [ 0.0692,  0.2125,  0.0506]]],\n",
      "\n",
      "\n",
      "        [[[-0.0934, -0.0000,  0.0000],\n",
      "          [-0.0477,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0510,  0.0000]],\n",
      "\n",
      "         [[ 0.1257,  0.1898,  0.1288],\n",
      "          [ 0.0000,  0.0617,  0.1707],\n",
      "          [ 0.1582,  0.2058,  0.1040]],\n",
      "\n",
      "         [[-0.1142,  0.0846,  0.0000],\n",
      "          [ 0.0000,  0.1101,  0.1572],\n",
      "          [ 0.1548,  0.0945,  0.0547]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.1917, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000, -0.0999]],\n",
      "\n",
      "         [[-0.1040, -0.0493, -0.0531],\n",
      "          [-0.1035, -0.0919, -0.0750],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0478,  0.0735],\n",
      "          [-0.1362,  0.0683, -0.0000],\n",
      "          [-0.1004, -0.0989, -0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0862],\n",
      "          [-0.0561,  0.0000,  0.0471],\n",
      "          [-0.0844,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.1100, -0.1236],\n",
      "          [-0.0000, -0.0479, -0.0000],\n",
      "          [ 0.0457, -0.0738,  0.0764]],\n",
      "\n",
      "         [[ 0.1167,  0.0606, -0.0000],\n",
      "          [ 0.0811, -0.0000, -0.0538],\n",
      "          [-0.0000, -0.2027, -0.1163]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0854,  0.0774, -0.0797],\n",
      "          [-0.1004, -0.1452, -0.0875],\n",
      "          [-0.0000, -0.0000, -0.0626]],\n",
      "\n",
      "         [[-0.1388,  0.0000, -0.0556],\n",
      "          [-0.0828, -0.0000,  0.0000],\n",
      "          [-0.0534, -0.0742, -0.1907]],\n",
      "\n",
      "         [[ 0.0000,  0.0569, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0849],\n",
      "          [ 0.1007,  0.1261, -0.0554]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0485],\n",
      "          [ 0.1180,  0.0000, -0.0511],\n",
      "          [ 0.0549,  0.0693,  0.0727]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.1141,  0.1129]],\n",
      "\n",
      "         [[ 0.0672,  0.0755,  0.0000],\n",
      "          [ 0.1089,  0.1512,  0.0000],\n",
      "          [ 0.0858,  0.1693,  0.0570]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0735, -0.0934, -0.0000],\n",
      "          [ 0.0000, -0.1372, -0.0000],\n",
      "          [ 0.0627, -0.0609,  0.0000]],\n",
      "\n",
      "         [[-0.0669,  0.0697, -0.0000],\n",
      "          [-0.0802, -0.0874, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.1309]],\n",
      "\n",
      "         [[-0.0492, -0.0527,  0.1215],\n",
      "          [ 0.0000,  0.0000, -0.0564],\n",
      "          [-0.0521, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2743,  0.0000,  0.0000],\n",
      "          [-0.0912, -0.1654, -0.0613],\n",
      "          [-0.0000, -0.0841,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.1302,  0.0952],\n",
      "          [-0.0915, -0.0820,  0.1036],\n",
      "          [-0.1556, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.1168, -0.0538],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.1403,  0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1407,  0.0000,  0.1446],\n",
      "          [-0.0536,  0.1759,  0.1757],\n",
      "          [-0.0000,  0.1572,  0.1754]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0835],\n",
      "          [-0.1448, -0.0940, -0.1025],\n",
      "          [-0.1166,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0847, -0.0544],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.1212,  0.0895, -0.0000]]]], device='cuda:0', requires_grad=True)\n",
      "layer2.2.bn2.weight Parameter containing:\n",
      "tensor([0.8057, 0.7883, 0.8329, 0.8813, 0.9496, 0.9358, 0.7308, 0.9040, 0.9696,\n",
      "        0.8415, 0.8536, 0.7970, 0.8679, 0.9110, 0.8230, 0.8167, 0.7768, 0.9004,\n",
      "        1.0839, 0.8083, 0.8176, 0.8469, 0.9013, 0.8225, 0.9952, 0.9163, 0.8361,\n",
      "        0.8938, 0.9146, 0.7936, 0.9302, 0.8775], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "layer2.2.bn2.bias Parameter containing:\n",
      "tensor([-0.1564, -0.1521, -0.0931, -0.1018, -0.2349, -0.3366, -0.2184, -0.0215,\n",
      "        -0.1045, -0.1357, -0.2834, -0.1394, -0.3376, -0.1554, -0.1480, -0.1386,\n",
      "        -0.2637, -0.0915, -0.2658, -0.1017, -0.1324, -0.1605, -0.1364, -0.1198,\n",
      "        -0.0521, -0.1501, -0.1307, -0.1527, -0.1321, -0.1169, -0.0854, -0.1567],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer3.0.conv1.weight Parameter containing:\n",
      "tensor([[[[ 0.0583, -0.0622,  0.0000],\n",
      "          [ 0.0438,  0.1407,  0.0961],\n",
      "          [ 0.0674, -0.0000, -0.0501]],\n",
      "\n",
      "         [[-0.0863, -0.0000,  0.0000],\n",
      "          [-0.1251, -0.0000,  0.0687],\n",
      "          [ 0.0860,  0.0918,  0.0892]],\n",
      "\n",
      "         [[ 0.0000,  0.0840,  0.0000],\n",
      "          [-0.0000,  0.0690,  0.0535],\n",
      "          [ 0.0000,  0.0578,  0.1220]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0554, -0.1953, -0.0763],\n",
      "          [ 0.0000, -0.1604,  0.1309],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.1849, -0.0000, -0.0000],\n",
      "          [-0.0485, -0.1330, -0.0000],\n",
      "          [-0.0683,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0632,  0.0000,  0.0000],\n",
      "          [-0.0732,  0.0803,  0.0530],\n",
      "          [-0.0000,  0.0000,  0.1345]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000,  0.0648],\n",
      "          [-0.1260, -0.0918, -0.0000],\n",
      "          [-0.1609,  0.0646, -0.1772]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0634],\n",
      "          [-0.0544,  0.0000,  0.1300],\n",
      "          [ 0.0000, -0.0000,  0.0666]],\n",
      "\n",
      "         [[ 0.0963, -0.0000,  0.0679],\n",
      "          [-0.0609,  0.0000,  0.0601],\n",
      "          [-0.0000,  0.0960, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1023,  0.1386, -0.0000],\n",
      "          [-0.0000, -0.0853,  0.0720],\n",
      "          [-0.1561, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0966, -0.0651],\n",
      "          [ 0.0000,  0.0000, -0.0672],\n",
      "          [ 0.0000, -0.0476, -0.0000]],\n",
      "\n",
      "         [[-0.0674, -0.0947, -0.1188],\n",
      "          [-0.1068, -0.2149, -0.1388],\n",
      "          [-0.1028, -0.1081, -0.0542]]],\n",
      "\n",
      "\n",
      "        [[[-0.0651,  0.0000,  0.1057],\n",
      "          [ 0.0000, -0.0000,  0.0579],\n",
      "          [ 0.0491,  0.0000,  0.0975]],\n",
      "\n",
      "         [[-0.0000, -0.0569,  0.0000],\n",
      "          [-0.1368,  0.0000,  0.0000],\n",
      "          [-0.1090,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0551,  0.0855,  0.0922],\n",
      "          [ 0.1007,  0.0000,  0.1088],\n",
      "          [ 0.0821,  0.1406, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0564, -0.0683, -0.0613],\n",
      "          [ 0.0000, -0.0918,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0986,  0.0000],\n",
      "          [ 0.0435, -0.0507,  0.0000],\n",
      "          [-0.0000,  0.1065,  0.0000]],\n",
      "\n",
      "         [[ 0.0945,  0.1498, -0.1717],\n",
      "          [-0.0989,  0.0000, -0.0554],\n",
      "          [ 0.0000,  0.0566,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0710,  0.0789,  0.0674],\n",
      "          [-0.1216,  0.0468,  0.1207],\n",
      "          [-0.0692,  0.0000,  0.0749]],\n",
      "\n",
      "         [[-0.0902, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0816, -0.0000],\n",
      "          [ 0.0541,  0.0000,  0.1604]],\n",
      "\n",
      "         [[ 0.0483,  0.0557,  0.0656],\n",
      "          [ 0.0745, -0.0000, -0.1179],\n",
      "          [ 0.1496, -0.0000, -0.0542]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0496,  0.0000],\n",
      "          [-0.0471, -0.1233,  0.0928],\n",
      "          [ 0.0000, -0.0838, -0.0000]],\n",
      "\n",
      "         [[-0.0769,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0608, -0.1148],\n",
      "          [ 0.0000, -0.0738, -0.0473]],\n",
      "\n",
      "         [[-0.1011,  0.1846, -0.0000],\n",
      "          [-0.1216,  0.0466,  0.0000],\n",
      "          [-0.1106, -0.0000,  0.0638]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0487,  0.0603],\n",
      "          [-0.0000, -0.0717, -0.0459],\n",
      "          [-0.1088,  0.0000, -0.0485]],\n",
      "\n",
      "         [[ 0.0707,  0.0000, -0.0980],\n",
      "          [ 0.1128, -0.1429, -0.0529],\n",
      "          [-0.0530, -0.0845, -0.0514]],\n",
      "\n",
      "         [[-0.0558,  0.0938,  0.0000],\n",
      "          [-0.1206,  0.1522, -0.0000],\n",
      "          [ 0.0000, -0.1242,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0865,  0.1752,  0.1783],\n",
      "          [ 0.1402,  0.0810,  0.2047],\n",
      "          [-0.0000,  0.1514,  0.1215]],\n",
      "\n",
      "         [[ 0.1097, -0.0688,  0.0000],\n",
      "          [ 0.0955,  0.0625, -0.0000],\n",
      "          [-0.0787,  0.0000,  0.0852]],\n",
      "\n",
      "         [[-0.0907, -0.2126, -0.0000],\n",
      "          [-0.1123, -0.1308, -0.0456],\n",
      "          [ 0.0000, -0.0920,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0796, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0582,  0.0000]],\n",
      "\n",
      "         [[ 0.0546, -0.0461,  0.0000],\n",
      "          [ 0.0752,  0.0575,  0.0438],\n",
      "          [ 0.0905,  0.1288,  0.1267]],\n",
      "\n",
      "         [[-0.0000,  0.0873, -0.0690],\n",
      "          [ 0.0000,  0.0738, -0.0510],\n",
      "          [-0.1671, -0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0526,  0.1583],\n",
      "          [ 0.0968,  0.1312,  0.0842],\n",
      "          [-0.0000,  0.0000,  0.1250]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0740],\n",
      "          [-0.0594,  0.1070,  0.0743],\n",
      "          [ 0.0815,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1659, -0.0000, -0.0754],\n",
      "          [ 0.0000,  0.0446, -0.0634],\n",
      "          [-0.0603, -0.0848, -0.0974]]]], device='cuda:0', requires_grad=True)\n",
      "layer3.0.bn1.weight Parameter containing:\n",
      "tensor([0.9124, 0.8680, 0.8392, 0.8019, 0.8983, 0.9029, 0.9584, 0.8393, 0.9418,\n",
      "        0.8644, 0.8580, 0.8926, 0.8843, 0.8082, 0.8309, 0.8439, 0.8951, 0.8844,\n",
      "        0.7290, 0.8776, 0.8637, 0.8650, 0.7137, 0.8610, 0.8733, 0.7630, 0.8923,\n",
      "        0.8852, 0.8255, 0.8500, 0.9389, 0.8032, 0.8188, 0.9188, 0.8601, 0.9246,\n",
      "        0.8307, 0.8975, 0.8430, 0.9118, 0.8334, 0.8605, 0.8684, 0.8201, 0.8294,\n",
      "        0.8632, 0.8742, 0.8514, 0.8385, 0.8424, 0.7728, 0.8597, 0.7886, 0.9603,\n",
      "        0.9028, 0.8349, 0.8848, 0.9031, 0.8072, 0.8508, 0.9022, 0.8457, 0.8509,\n",
      "        0.8441], device='cuda:0', requires_grad=True)\n",
      "layer3.0.bn1.bias Parameter containing:\n",
      "tensor([-0.0197, -0.0194, -0.1045, -0.1141, -0.1955, -0.0739, -0.1732, -0.0437,\n",
      "        -0.0683, -0.0903, -0.1374, -0.1323, -0.0710,  0.0347, -0.2206, -0.1139,\n",
      "        -0.1260, -0.0385, -0.0166, -0.1116, -0.1534, -0.0795, -0.1704, -0.0737,\n",
      "        -0.0962, -0.1621, -0.0541, -0.0608, -0.1635, -0.1565, -0.0263, -0.0531,\n",
      "        -0.1658, -0.0330, -0.1202, -0.1589, -0.1672, -0.1677, -0.0873, -0.0922,\n",
      "        -0.1059, -0.1396, -0.1717, -0.0655, -0.1528, -0.1819, -0.0807, -0.0725,\n",
      "        -0.0746, -0.0475, -0.1625, -0.0243, -0.1099, -0.0297, -0.1333, -0.1295,\n",
      "        -0.1863, -0.0080, -0.1534, -0.0785, -0.0454, -0.2307, -0.0937, -0.0854],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer3.0.conv2.weight Parameter containing:\n",
      "tensor([[[[-0.0000, -0.0458, -0.0691],\n",
      "          [-0.1401, -0.0737, -0.0000],\n",
      "          [-0.0000, -0.0989,  0.0401]],\n",
      "\n",
      "         [[ 0.0000,  0.1126, -0.0808],\n",
      "          [-0.1091,  0.0000, -0.0000],\n",
      "          [-0.1947, -0.0887, -0.1353]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0444],\n",
      "          [-0.1397,  0.0513, -0.0920],\n",
      "          [ 0.0439,  0.1370,  0.1135]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.1004, -0.0000, -0.0684],\n",
      "          [-0.0557,  0.0000,  0.0674]],\n",
      "\n",
      "         [[ 0.0000,  0.0494,  0.0000],\n",
      "          [ 0.0979, -0.0669,  0.0000],\n",
      "          [ 0.0622, -0.0377, -0.0531]],\n",
      "\n",
      "         [[ 0.0000,  0.0896,  0.1318],\n",
      "          [-0.1052,  0.0000,  0.0606],\n",
      "          [-0.0926,  0.0817,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0345,  0.0000,  0.0850],\n",
      "          [ 0.0000,  0.0977,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0539]],\n",
      "\n",
      "         [[ 0.0776,  0.0000, -0.0000],\n",
      "          [-0.0529, -0.0000, -0.0000],\n",
      "          [-0.0373, -0.0703, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0501],\n",
      "          [ 0.1187,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.1493,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0475, -0.0000,  0.0000],\n",
      "          [ 0.0336,  0.0000, -0.0343]],\n",
      "\n",
      "         [[-0.1592, -0.1725, -0.1318],\n",
      "          [ 0.0928, -0.0604, -0.0999],\n",
      "          [-0.0000, -0.0000, -0.0541]],\n",
      "\n",
      "         [[ 0.0889,  0.0000,  0.0000],\n",
      "          [ 0.0380,  0.0441,  0.0501],\n",
      "          [-0.0552, -0.0795,  0.0458]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0410, -0.0644],\n",
      "          [ 0.0000, -0.0000, -0.1035],\n",
      "          [ 0.0511,  0.0000, -0.1098]],\n",
      "\n",
      "         [[ 0.0000,  0.0925, -0.1067],\n",
      "          [-0.1134, -0.1152, -0.0362],\n",
      "          [ 0.0000,  0.0000, -0.0940]],\n",
      "\n",
      "         [[-0.0000,  0.0411,  0.0704],\n",
      "          [ 0.0000,  0.0000,  0.0606],\n",
      "          [-0.0000, -0.0338,  0.0398]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0505,  0.0000,  0.0422],\n",
      "          [-0.0771,  0.1206,  0.0000],\n",
      "          [-0.0639, -0.0607, -0.0353]],\n",
      "\n",
      "         [[ 0.0588, -0.0874, -0.1481],\n",
      "          [-0.0984, -0.0438, -0.0000],\n",
      "          [-0.0471,  0.0683,  0.0638]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0401],\n",
      "          [-0.0690, -0.0665, -0.0445]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0476,  0.0398],\n",
      "          [ 0.0345,  0.0000, -0.0764]],\n",
      "\n",
      "         [[-0.0000, -0.0557, -0.0361],\n",
      "          [ 0.0000, -0.0000, -0.0511],\n",
      "          [ 0.0000,  0.1103,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.1625,  0.0000],\n",
      "          [-0.0000,  0.0588,  0.0685],\n",
      "          [-0.0337, -0.0365, -0.0399]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.1301, -0.0000],\n",
      "          [ 0.1840,  0.1106,  0.0699],\n",
      "          [ 0.0694,  0.0909,  0.0446]],\n",
      "\n",
      "         [[-0.0691, -0.0000, -0.0721],\n",
      "          [-0.0000,  0.0525, -0.0000],\n",
      "          [ 0.0506,  0.1417,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0382,  0.0354],\n",
      "          [ 0.0000,  0.0811, -0.0451],\n",
      "          [ 0.1019,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0450],\n",
      "          [-0.0000,  0.0434, -0.0631],\n",
      "          [ 0.0815,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0391, -0.0477,  0.1055],\n",
      "          [-0.0524, -0.1219,  0.0467],\n",
      "          [-0.0437,  0.0817, -0.0000]],\n",
      "\n",
      "         [[ 0.0547, -0.0827, -0.0772],\n",
      "          [ 0.0000, -0.0804,  0.0000],\n",
      "          [ 0.0000,  0.0819,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0372, -0.0902,  0.0718],\n",
      "          [-0.0000, -0.0000, -0.0797],\n",
      "          [-0.0000, -0.0345,  0.0000]],\n",
      "\n",
      "         [[ 0.0990,  0.0943,  0.0963],\n",
      "          [-0.0000, -0.0907, -0.0893],\n",
      "          [-0.0708, -0.0520, -0.0000]],\n",
      "\n",
      "         [[ 0.1492,  0.0718, -0.0419],\n",
      "          [-0.0000,  0.0000,  0.1076],\n",
      "          [ 0.0000,  0.0469,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0725,  0.1195],\n",
      "          [-0.0000,  0.0909, -0.0000],\n",
      "          [ 0.0000,  0.0669, -0.0000]],\n",
      "\n",
      "         [[ 0.0645,  0.1311, -0.0000],\n",
      "          [ 0.0405,  0.0000, -0.0000],\n",
      "          [ 0.0381,  0.0000,  0.0665]],\n",
      "\n",
      "         [[-0.0955, -0.0626, -0.0582],\n",
      "          [-0.0784, -0.1157, -0.1034],\n",
      "          [-0.0574,  0.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0840,  0.0360, -0.0945],\n",
      "          [ 0.0000, -0.0809,  0.0774],\n",
      "          [ 0.0000, -0.0611, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0631],\n",
      "          [-0.0000,  0.0000, -0.0487],\n",
      "          [-0.0000, -0.0391,  0.0000]],\n",
      "\n",
      "         [[-0.0445, -0.0336, -0.0000],\n",
      "          [-0.0000,  0.0597, -0.0000],\n",
      "          [ 0.0000,  0.0364, -0.0476]]]], device='cuda:0', requires_grad=True)\n",
      "layer3.0.bn2.weight Parameter containing:\n",
      "tensor([1.1006, 1.0106, 1.0800, 1.0509, 0.9559, 1.0814, 0.9386, 0.9665, 1.0822,\n",
      "        1.0743, 1.0793, 0.9965, 0.9665, 0.9832, 0.9848, 0.9739, 0.9530, 0.9612,\n",
      "        1.0294, 0.9169, 0.9545, 0.9073, 0.9191, 1.0831, 0.8867, 0.9741, 0.9294,\n",
      "        1.0896, 0.9654, 0.9920, 0.8418, 0.9192, 0.9549, 0.9719, 0.9321, 0.8114,\n",
      "        0.8894, 0.7874, 0.8712, 1.0026, 0.8252, 0.9382, 0.8889, 0.9468, 0.9298,\n",
      "        0.9263, 0.9629, 0.9528, 1.0401, 1.0971, 1.0802, 0.9764, 1.0442, 1.0095,\n",
      "        0.9792, 1.0536, 1.0221, 1.0719, 0.9629, 0.9697, 0.9739, 1.0812, 0.9529,\n",
      "        0.8995], device='cuda:0', requires_grad=True)\n",
      "layer3.0.bn2.bias Parameter containing:\n",
      "tensor([-0.0565, -0.1147, -0.0966, -0.1244, -0.1920, -0.1414, -0.0801, -0.1294,\n",
      "        -0.0451, -0.1217, -0.0956, -0.1351, -0.0215, -0.1131, -0.0835, -0.2317,\n",
      "        -0.1953, -0.1699, -0.1649, -0.1830, -0.2053, -0.1931, -0.1990, -0.1199,\n",
      "        -0.0350, -0.1921, -0.2622, -0.2104, -0.2610, -0.0896, -0.1658, -0.1191,\n",
      "        -0.2931, -0.1358, -0.2211, -0.2046, -0.2158, -0.1921, -0.1099, -0.2203,\n",
      "        -0.1496, -0.1163, -0.1953, -0.2082, -0.2339, -0.1025, -0.1265, -0.2401,\n",
      "        -0.1169, -0.1902, -0.0915, -0.0784, -0.1255, -0.0931, -0.2266, -0.0973,\n",
      "        -0.1323,  0.0112, -0.2213, -0.1670, -0.1526, -0.1957, -0.1358, -0.1596],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer3.1.conv1.weight Parameter containing:\n",
      "tensor([[[[ 0.0000,  0.0720,  0.1026],\n",
      "          [ 0.0000,  0.0693, -0.0506],\n",
      "          [-0.0712, -0.0465, -0.0710]],\n",
      "\n",
      "         [[ 0.0509,  0.0000, -0.0753],\n",
      "          [ 0.1261, -0.0000, -0.0000],\n",
      "          [-0.0970, -0.0895, -0.0400]],\n",
      "\n",
      "         [[ 0.0000, -0.0815,  0.1151],\n",
      "          [-0.0725, -0.0000, -0.0799],\n",
      "          [-0.0000, -0.0565,  0.0703]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0804, -0.0921, -0.1481],\n",
      "          [ 0.0000,  0.0419,  0.0000],\n",
      "          [ 0.0593,  0.0914,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0346,  0.1281],\n",
      "          [ 0.0000, -0.0000,  0.0643],\n",
      "          [ 0.0383, -0.0701,  0.0470]],\n",
      "\n",
      "         [[-0.0697, -0.0000, -0.0463],\n",
      "          [-0.0000,  0.0000, -0.0861],\n",
      "          [ 0.0000, -0.0380,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.1148],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0600,  0.0604]],\n",
      "\n",
      "         [[ 0.1066, -0.0480, -0.0691],\n",
      "          [-0.0000, -0.0000,  0.0395],\n",
      "          [ 0.0000,  0.0669, -0.0000]],\n",
      "\n",
      "         [[-0.1721,  0.0531,  0.0650],\n",
      "          [-0.0489,  0.0000, -0.0345],\n",
      "          [ 0.0000,  0.0000, -0.1542]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0438,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0627],\n",
      "          [-0.0444,  0.0000,  0.0515]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0498],\n",
      "          [ 0.0000,  0.0718, -0.0000],\n",
      "          [ 0.0362,  0.0880,  0.0781]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0762],\n",
      "          [-0.0000,  0.0000,  0.0537],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1163,  0.0542,  0.0605],\n",
      "          [-0.0660,  0.0000,  0.0853],\n",
      "          [ 0.0629,  0.0504, -0.0000]],\n",
      "\n",
      "         [[-0.0898, -0.0818, -0.0000],\n",
      "          [-0.1318,  0.0553,  0.0445],\n",
      "          [-0.0000, -0.0000,  0.0711]],\n",
      "\n",
      "         [[-0.0000, -0.0000,  0.0704],\n",
      "          [ 0.0000, -0.0394, -0.0357],\n",
      "          [-0.0434,  0.0542,  0.1601]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.1004],\n",
      "          [ 0.0976,  0.1109,  0.0404],\n",
      "          [ 0.0000,  0.0869,  0.0000]],\n",
      "\n",
      "         [[-0.0681,  0.0413, -0.0885],\n",
      "          [-0.0872, -0.0000, -0.0000],\n",
      "          [ 0.0835,  0.0808,  0.0690]],\n",
      "\n",
      "         [[-0.0487,  0.0000, -0.0000],\n",
      "          [-0.0586,  0.0000,  0.0601],\n",
      "          [ 0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.1542, -0.0000,  0.0651]],\n",
      "\n",
      "         [[ 0.0550,  0.0918,  0.2502],\n",
      "          [ 0.0858, -0.0519,  0.1667],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0407],\n",
      "          [ 0.0000,  0.0439,  0.0000],\n",
      "          [ 0.0843, -0.0629,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.1082],\n",
      "          [ 0.0000,  0.0832,  0.0000],\n",
      "          [-0.1127, -0.1128,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0400],\n",
      "          [ 0.0512, -0.0478,  0.0603],\n",
      "          [ 0.0000,  0.0528, -0.0348]],\n",
      "\n",
      "         [[ 0.0453, -0.1061, -0.1128],\n",
      "          [-0.0000,  0.0000, -0.0447],\n",
      "          [-0.0000, -0.1068, -0.0535]]],\n",
      "\n",
      "\n",
      "        [[[-0.0448,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0735,  0.0000],\n",
      "          [ 0.0687,  0.0391,  0.0815]],\n",
      "\n",
      "         [[-0.0633, -0.0839,  0.0000],\n",
      "          [-0.0397, -0.0000, -0.0557],\n",
      "          [ 0.1153,  0.0643,  0.0580]],\n",
      "\n",
      "         [[-0.0456, -0.0000,  0.0368],\n",
      "          [-0.0446,  0.0448, -0.0000],\n",
      "          [ 0.0614, -0.0000, -0.0594]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.1027],\n",
      "          [ 0.0857,  0.0548, -0.0787],\n",
      "          [ 0.0441,  0.0366, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0350, -0.0433],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.1223, -0.1006],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.1576,  0.0397, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0575,  0.1547,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0449],\n",
      "          [ 0.0539,  0.0495,  0.0538]],\n",
      "\n",
      "         [[-0.0000,  0.1063,  0.0365],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0854,  0.1178,  0.0494]],\n",
      "\n",
      "         [[ 0.0000, -0.1086, -0.0000],\n",
      "          [ 0.0000,  0.0561,  0.0000],\n",
      "          [-0.0000, -0.0622, -0.0660]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0524, -0.0000],\n",
      "          [ 0.0430, -0.0425,  0.0000],\n",
      "          [ 0.0653, -0.0851,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0469, -0.0621],\n",
      "          [-0.0000,  0.0676,  0.0000],\n",
      "          [ 0.0886,  0.0000,  0.0677]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.1000, -0.1127],\n",
      "          [ 0.1009, -0.0360, -0.0000]]]], device='cuda:0', requires_grad=True)\n",
      "layer3.1.bn1.weight Parameter containing:\n",
      "tensor([0.8338, 0.8551, 0.8463, 0.8746, 0.8554, 0.8743, 0.8971, 0.8531, 0.8622,\n",
      "        0.8401, 0.8587, 0.8508, 0.8358, 0.8741, 0.8365, 0.8860, 0.9106, 0.8154,\n",
      "        0.7845, 0.9016, 0.8344, 0.8780, 0.8952, 0.7518, 0.7677, 0.7687, 0.8453,\n",
      "        0.7305, 0.7754, 0.7992, 0.8371, 0.7458, 0.8100, 0.8564, 0.8275, 0.8544,\n",
      "        0.7140, 0.8059, 0.8554, 0.7832, 0.7960, 0.8657, 0.8256, 0.7888, 0.8291,\n",
      "        0.8088, 0.8426, 0.9573, 0.7351, 0.8054, 0.8621, 0.7375, 0.9181, 0.8212,\n",
      "        0.8231, 0.9809, 0.8725, 0.8385, 0.8840, 0.8430, 0.8375, 0.8810, 0.9412,\n",
      "        0.8401], device='cuda:0', requires_grad=True)\n",
      "layer3.1.bn1.bias Parameter containing:\n",
      "tensor([-0.3696, -0.1001, -0.2111, -0.1406, -0.3235, -0.1255, -0.1675, -0.2120,\n",
      "        -0.1861, -0.3638, -0.2955, -0.2313, -0.1607, -0.1565, -0.2463, -0.2279,\n",
      "        -0.1005, -0.2590, -0.2029, -0.1545, -0.2301, -0.2395, -0.1586, -0.1644,\n",
      "        -0.3634, -0.3021, -0.0861, -0.2394, -0.2201, -0.2382, -0.2760, -0.2247,\n",
      "        -0.3331, -0.1565, -0.3687, -0.1746, -0.2036, -0.1392, -0.2411, -0.4038,\n",
      "        -0.2523, -0.1112, -0.0487, -0.1837, -0.2017, -0.2259, -0.1807, -0.0834,\n",
      "        -0.3193, -0.1913, -0.3239, -0.1662, -0.1213, -0.1885, -0.1651, -0.0769,\n",
      "        -0.2067, -0.2440, -0.3248, -0.1736, -0.3804, -0.2142,  0.0014, -0.2513],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer3.1.conv2.weight Parameter containing:\n",
      "tensor([[[[ 0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0615,  0.0329],\n",
      "          [-0.0000,  0.0576,  0.1033]],\n",
      "\n",
      "         [[ 0.0398,  0.0933,  0.0630],\n",
      "          [ 0.1131, -0.0000, -0.1076],\n",
      "          [ 0.0817,  0.0379,  0.0000]],\n",
      "\n",
      "         [[ 0.0983,  0.0457, -0.0000],\n",
      "          [-0.0315,  0.0000, -0.0485],\n",
      "          [-0.0626, -0.0364,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0623, -0.0000,  0.0362],\n",
      "          [-0.0000, -0.0744, -0.0000],\n",
      "          [-0.0316, -0.0447,  0.0733]],\n",
      "\n",
      "         [[ 0.0330, -0.0445, -0.0434],\n",
      "          [ 0.0344, -0.0897, -0.1577],\n",
      "          [ 0.0484, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0809, -0.0421],\n",
      "          [-0.0000,  0.0325,  0.0813],\n",
      "          [-0.0385,  0.0439,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0508, -0.0526, -0.0476],\n",
      "          [-0.0428, -0.0703, -0.0000],\n",
      "          [ 0.0000,  0.0490, -0.1310]],\n",
      "\n",
      "         [[-0.0470, -0.0534, -0.0651],\n",
      "          [ 0.0816,  0.0430,  0.0541],\n",
      "          [ 0.0000, -0.0585, -0.0368]],\n",
      "\n",
      "         [[-0.0369, -0.0351,  0.0000],\n",
      "          [ 0.1490, -0.0000, -0.0336],\n",
      "          [ 0.0000, -0.1260, -0.0691]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1670, -0.0000, -0.0000],\n",
      "          [-0.0338,  0.0000,  0.0524],\n",
      "          [ 0.0478, -0.0382,  0.0000]],\n",
      "\n",
      "         [[-0.0531, -0.0765, -0.0943],\n",
      "          [ 0.0000, -0.0740, -0.0000],\n",
      "          [ 0.0504,  0.0000, -0.0554]],\n",
      "\n",
      "         [[ 0.0518,  0.0498,  0.0975],\n",
      "          [ 0.0000, -0.0000,  0.1013],\n",
      "          [-0.0480,  0.0387, -0.0456]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0337, -0.0380,  0.0000],\n",
      "          [ 0.0657, -0.0000, -0.0539],\n",
      "          [-0.1114, -0.0000,  0.0551]],\n",
      "\n",
      "         [[-0.0836, -0.0000, -0.1369],\n",
      "          [ 0.0830,  0.0515, -0.0475],\n",
      "          [-0.0410,  0.0000, -0.0548]],\n",
      "\n",
      "         [[-0.0476, -0.0759, -0.0914],\n",
      "          [ 0.0675, -0.0767, -0.1059],\n",
      "          [ 0.0468, -0.0492, -0.0756]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0528,  0.0000,  0.0000],\n",
      "          [ 0.0652, -0.0405,  0.0582],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0697, -0.0451,  0.0000],\n",
      "          [-0.0689, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0420, -0.0437]],\n",
      "\n",
      "         [[-0.0482, -0.0000,  0.0638],\n",
      "          [-0.0000, -0.0000, -0.0408],\n",
      "          [-0.0363, -0.0437, -0.1038]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0893,  0.1473],\n",
      "          [ 0.0000,  0.0000,  0.0548],\n",
      "          [ 0.0566, -0.0000, -0.0671]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0408,  0.0000],\n",
      "          [-0.0000,  0.0904,  0.0546]],\n",
      "\n",
      "         [[ 0.0616,  0.1367, -0.1031],\n",
      "          [ 0.0364, -0.0000, -0.1249],\n",
      "          [-0.0329, -0.0669,  0.0526]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0348, -0.0323],\n",
      "          [ 0.0000,  0.0389, -0.0625],\n",
      "          [-0.1105, -0.0337, -0.0558]],\n",
      "\n",
      "         [[ 0.0928,  0.0852, -0.0368],\n",
      "          [-0.0000, -0.0697,  0.0000],\n",
      "          [-0.0613,  0.0516, -0.0000]],\n",
      "\n",
      "         [[-0.0996,  0.0000, -0.1135],\n",
      "          [-0.0000,  0.0508,  0.0912],\n",
      "          [ 0.0429,  0.1003, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0578, -0.0589, -0.1580],\n",
      "          [-0.0693, -0.0000, -0.0000],\n",
      "          [-0.1220, -0.0613,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.1328, -0.0000],\n",
      "          [-0.0000, -0.1005, -0.0000],\n",
      "          [ 0.0658,  0.0000, -0.0493]],\n",
      "\n",
      "         [[ 0.0000,  0.0894, -0.0618],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0771, -0.0844, -0.0568]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000,  0.1609,  0.0344],\n",
      "          [-0.0367,  0.0000,  0.0351],\n",
      "          [-0.0347,  0.0000, -0.1315]],\n",
      "\n",
      "         [[ 0.0524,  0.0000,  0.0000],\n",
      "          [ 0.1320,  0.0323,  0.0827],\n",
      "          [ 0.0685,  0.0923,  0.0000]],\n",
      "\n",
      "         [[ 0.1109, -0.0000,  0.0000],\n",
      "          [ 0.0725, -0.0403,  0.1386],\n",
      "          [ 0.0000,  0.1266, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0463,  0.0868, -0.0000],\n",
      "          [ 0.0000, -0.0421, -0.0000],\n",
      "          [ 0.0313,  0.0727,  0.0636]],\n",
      "\n",
      "         [[-0.0688, -0.0392,  0.0637],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-0.0651, -0.0766, -0.0457]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0654,  0.0347,  0.0375],\n",
      "          [-0.0809, -0.0878,  0.0445]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0455, -0.0336,  0.0000],\n",
      "          [ 0.0000,  0.0696,  0.0719]],\n",
      "\n",
      "         [[-0.0486,  0.0000, -0.0523],\n",
      "          [ 0.1442, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0833, -0.1121]],\n",
      "\n",
      "         [[ 0.0367,  0.0581,  0.0422],\n",
      "          [-0.0601,  0.1781,  0.0508],\n",
      "          [-0.0498,  0.0000,  0.0610]]]], device='cuda:0', requires_grad=True)\n",
      "layer3.1.bn2.weight Parameter containing:\n",
      "tensor([0.9826, 1.0834, 1.0205, 1.0831, 1.0751, 1.0476, 1.1056, 1.1252, 0.9992,\n",
      "        1.0299, 1.0639, 1.0051, 1.0638, 1.0382, 0.9606, 1.0095, 1.0362, 1.1249,\n",
      "        1.0955, 1.0839, 1.0763, 1.0832, 1.0613, 1.1068, 0.9649, 1.0446, 1.0009,\n",
      "        1.0828, 1.0992, 1.0465, 1.0621, 0.8992, 1.0123, 1.0079, 0.9882, 1.1282,\n",
      "        1.1169, 1.1919, 0.9913, 0.9992, 1.0663, 1.0365, 1.0172, 1.0788, 1.0571,\n",
      "        0.9764, 0.9432, 1.0550, 1.0829, 1.1042, 0.9910, 1.0234, 1.0674, 1.0739,\n",
      "        1.1464, 1.1146, 1.0404, 1.0663, 1.1470, 1.0521, 1.1106, 1.1225, 1.0611,\n",
      "        1.0679], device='cuda:0', requires_grad=True)\n",
      "layer3.1.bn2.bias Parameter containing:\n",
      "tensor([-0.1044, -0.0955, -0.0512, -0.1385, -0.2062, -0.1045, -0.1215, -0.0521,\n",
      "        -0.0843, -0.1185, -0.0150, -0.0768, -0.0969, -0.0725, -0.0834, -0.0410,\n",
      "        -0.1269, -0.1749, -0.1264, -0.1177, -0.2226, -0.1084, -0.0973, -0.0679,\n",
      "        -0.0277, -0.1079, -0.1364, -0.2031, -0.1918, -0.0959, -0.1050, -0.1398,\n",
      "        -0.1532, -0.1750, -0.1388, -0.1456, -0.1318, -0.2053, -0.1455, -0.1184,\n",
      "        -0.0801, -0.1641, -0.0765, -0.1213, -0.2362, -0.0564, -0.0883, -0.1439,\n",
      "        -0.1064, -0.0876, -0.0390, -0.0458, -0.0987, -0.1309, -0.2449, -0.0966,\n",
      "        -0.0837, -0.0401, -0.1193, -0.1480, -0.1487, -0.1548, -0.1276, -0.2321],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer3.2.conv1.weight Parameter containing:\n",
      "tensor([[[[-0.0460,  0.0802,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.1086, -0.1403, -0.0457]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0483],\n",
      "          [-0.0401, -0.0000, -0.0000],\n",
      "          [-0.0000,  0.0694, -0.0000]],\n",
      "\n",
      "         [[ 0.0605, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0577,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0618,  0.0564,  0.0950],\n",
      "          [-0.0867,  0.0470,  0.0369],\n",
      "          [-0.0441, -0.0468, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0524, -0.0627],\n",
      "          [ 0.0859,  0.0000, -0.0000],\n",
      "          [-0.1076, -0.0000, -0.0465]],\n",
      "\n",
      "         [[-0.0686,  0.0000, -0.0356],\n",
      "          [-0.0000,  0.1015,  0.0560],\n",
      "          [ 0.0000,  0.0405, -0.0614]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0338,  0.0000],\n",
      "          [ 0.0000,  0.1154, -0.0664],\n",
      "          [ 0.0000,  0.1139,  0.0000]],\n",
      "\n",
      "         [[ 0.0366, -0.0000, -0.0577],\n",
      "          [ 0.0884,  0.1404, -0.1158],\n",
      "          [-0.0000,  0.0760,  0.0931]],\n",
      "\n",
      "         [[ 0.0333,  0.0347,  0.0545],\n",
      "          [ 0.0421,  0.0638,  0.0659],\n",
      "          [ 0.1376,  0.0677,  0.0551]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0322, -0.0442, -0.0881],\n",
      "          [ 0.0798,  0.0000,  0.0000],\n",
      "          [-0.0345,  0.0000,  0.0673]],\n",
      "\n",
      "         [[ 0.1052,  0.0000, -0.0406],\n",
      "          [ 0.0530,  0.0397,  0.0000],\n",
      "          [-0.0586,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.1120,  0.0424, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0477],\n",
      "          [-0.0547, -0.0000,  0.0434]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0366, -0.0563],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0675,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0453,  0.0338,  0.0651],\n",
      "          [-0.0352,  0.1664,  0.0849],\n",
      "          [ 0.1191,  0.0991,  0.0848]],\n",
      "\n",
      "         [[-0.1008, -0.0497, -0.1539],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0978,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0469,  0.0314,  0.0695],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0496, -0.0338,  0.0000]],\n",
      "\n",
      "         [[-0.0328, -0.0862,  0.1028],\n",
      "          [ 0.0742, -0.1099,  0.0765],\n",
      "          [ 0.0891,  0.0394,  0.0327]],\n",
      "\n",
      "         [[-0.0000,  0.0693,  0.1445],\n",
      "          [ 0.0000,  0.0000, -0.0716],\n",
      "          [ 0.0484, -0.0000,  0.0716]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.1144,  0.0000],\n",
      "          [ 0.0000,  0.1105,  0.0559],\n",
      "          [ 0.0000, -0.0314, -0.0000]],\n",
      "\n",
      "         [[ 0.0932,  0.1062,  0.0391],\n",
      "          [ 0.0692,  0.0468, -0.0000],\n",
      "          [ 0.1038,  0.0925,  0.0872]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0529],\n",
      "          [ 0.0416, -0.0411, -0.0685],\n",
      "          [ 0.0000, -0.0990, -0.0311]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0519, -0.0000, -0.0622],\n",
      "          [-0.0550, -0.0000,  0.0590],\n",
      "          [ 0.0916, -0.0414,  0.1091]],\n",
      "\n",
      "         [[ 0.0645,  0.0997,  0.0000],\n",
      "          [-0.0000,  0.0686,  0.0493],\n",
      "          [-0.0000,  0.0320, -0.0000]],\n",
      "\n",
      "         [[ 0.0593, -0.0000,  0.0582],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0394, -0.0344,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000,  0.0889],\n",
      "          [-0.1343, -0.0427, -0.0352],\n",
      "          [-0.0489,  0.0000,  0.0406]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0487],\n",
      "          [-0.0372,  0.0000, -0.0000],\n",
      "          [ 0.0701,  0.0000,  0.0419]],\n",
      "\n",
      "         [[-0.0376,  0.0585, -0.0475],\n",
      "          [-0.0000,  0.0430, -0.0870],\n",
      "          [ 0.0000, -0.0000, -0.0836]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0468,  0.0855,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0315,  0.0432, -0.0552]],\n",
      "\n",
      "         [[ 0.0429,  0.0318, -0.0000],\n",
      "          [ 0.0355, -0.0000,  0.0720],\n",
      "          [-0.0412, -0.1453, -0.0491]],\n",
      "\n",
      "         [[ 0.0000,  0.0569,  0.0528],\n",
      "          [ 0.0929,  0.0000, -0.0542],\n",
      "          [-0.0000, -0.0406, -0.0563]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0434,  0.0000],\n",
      "          [ 0.0000,  0.0382, -0.0383],\n",
      "          [-0.0000,  0.0000, -0.0754]],\n",
      "\n",
      "         [[ 0.0764, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0312],\n",
      "          [ 0.0000, -0.0000,  0.0388]],\n",
      "\n",
      "         [[-0.0649, -0.0000,  0.0000],\n",
      "          [-0.0600, -0.0468, -0.0000],\n",
      "          [-0.0442, -0.0000, -0.0401]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0483, -0.0620,  0.0530],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0833,  0.0000, -0.0726]],\n",
      "\n",
      "         [[-0.0379, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.1281]],\n",
      "\n",
      "         [[-0.0379,  0.0410,  0.0568],\n",
      "          [ 0.0592,  0.0440,  0.0987],\n",
      "          [ 0.0000,  0.0462,  0.0853]]]], device='cuda:0', requires_grad=True)\n",
      "layer3.2.bn1.weight Parameter containing:\n",
      "tensor([0.7620, 0.9314, 0.8479, 0.7999, 0.8830, 0.8205, 0.8471, 0.7936, 0.8528,\n",
      "        0.9585, 0.8188, 0.8953, 0.7973, 0.9726, 0.8093, 0.7727, 0.7753, 0.7683,\n",
      "        0.8380, 0.8261, 0.7938, 0.8950, 0.7861, 0.8790, 0.8065, 0.9448, 0.9635,\n",
      "        0.7610, 0.8116, 0.7537, 0.8094, 0.8000, 0.8639, 0.7553, 0.8542, 0.7650,\n",
      "        0.9353, 0.8033, 0.7909, 0.9241, 0.7933, 0.8821, 0.7669, 0.8536, 0.7361,\n",
      "        0.8584, 0.8793, 0.9009, 0.9236, 0.9204, 0.8191, 0.9011, 0.8255, 0.7993,\n",
      "        0.7786, 0.8442, 0.8751, 0.8566, 0.7972, 0.9095, 0.8869, 0.8675, 0.8336,\n",
      "        0.7409], device='cuda:0', requires_grad=True)\n",
      "layer3.2.bn1.bias Parameter containing:\n",
      "tensor([-0.2170, -0.2006, -0.2700, -0.1249, -0.3281, -0.0436, -0.2771, -0.1181,\n",
      "        -0.2208, -0.1694, -0.1712, -0.2149, -0.1888, -0.3782, -0.0641, -0.1572,\n",
      "        -0.1316, -0.2472, -0.1725, -0.1050, -0.1853, -0.1620, -0.2408, -0.2687,\n",
      "        -0.0937, -0.3208, -0.2839, -0.0939, -0.2116, -0.0908, -0.2048, -0.2147,\n",
      "        -0.1322, -0.1546, -0.2089, -0.2639, -0.1877, -0.1759, -0.2429, -0.0747,\n",
      "        -0.1910, -0.1871, -0.1436, -0.0943, -0.2773, -0.1351, -0.2182, -0.1726,\n",
      "        -0.2163, -0.3159, -0.0996, -0.1333, -0.3158, -0.1614, -0.1226, -0.1632,\n",
      "        -0.3128, -0.1845, -0.2149, -0.2322, -0.2237, -0.2473, -0.1185, -0.1296],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "layer3.2.conv2.weight Parameter containing:\n",
      "tensor([[[[-0.0754,  0.0000,  0.0341],\n",
      "          [ 0.0636, -0.0320,  0.0000],\n",
      "          [ 0.0547, -0.0740,  0.0585]],\n",
      "\n",
      "         [[ 0.0435,  0.0471, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0948,  0.0000]],\n",
      "\n",
      "         [[ 0.0303, -0.0599,  0.0000],\n",
      "          [-0.0000, -0.0647, -0.0917],\n",
      "          [-0.0632, -0.0670, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0309, -0.0626, -0.0323],\n",
      "          [-0.0428, -0.0744, -0.0304],\n",
      "          [-0.0915, -0.0483, -0.0477]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0612, -0.0379,  0.1064],\n",
      "          [ 0.0000, -0.0729, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0376],\n",
      "          [-0.1124,  0.0694, -0.0000],\n",
      "          [ 0.0000,  0.0803,  0.0389]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0461],\n",
      "          [-0.1051,  0.0000, -0.0656],\n",
      "          [-0.0491, -0.0000, -0.0422]],\n",
      "\n",
      "         [[-0.1097, -0.0342, -0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0434],\n",
      "          [-0.0921, -0.0000, -0.0792]],\n",
      "\n",
      "         [[ 0.0539,  0.0387,  0.1300],\n",
      "          [ 0.0641,  0.0914,  0.0000],\n",
      "          [ 0.1161,  0.1564,  0.0915]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0594,  0.0340, -0.0589],\n",
      "          [-0.0602, -0.0000,  0.0653],\n",
      "          [ 0.0433, -0.0000,  0.1109]],\n",
      "\n",
      "         [[-0.0329, -0.0799,  0.0457],\n",
      "          [-0.0734,  0.0284,  0.0551],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0646,  0.0832],\n",
      "          [ 0.0323, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0452,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.1021,  0.0644, -0.0000]],\n",
      "\n",
      "         [[ 0.0354,  0.0841,  0.0000],\n",
      "          [ 0.0339,  0.0553, -0.0000],\n",
      "          [ 0.0361, -0.0000,  0.0661]],\n",
      "\n",
      "         [[-0.1700,  0.0000, -0.0000],\n",
      "          [-0.0304,  0.0397, -0.0771],\n",
      "          [-0.0000,  0.0383, -0.0440]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0507, -0.0784, -0.0000],\n",
      "          [-0.0621,  0.0553, -0.0000],\n",
      "          [-0.0951, -0.0684,  0.0450]],\n",
      "\n",
      "         [[-0.0556, -0.0297, -0.0818],\n",
      "          [-0.0658, -0.0673, -0.0366],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0603,  0.0376,  0.0000],\n",
      "          [ 0.0000, -0.0387, -0.0770],\n",
      "          [-0.0411,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0350, -0.0931,  0.1189],\n",
      "          [-0.0000,  0.0641, -0.0000],\n",
      "          [-0.0000,  0.0446,  0.0876]],\n",
      "\n",
      "         [[-0.0555, -0.0383, -0.0000],\n",
      "          [-0.0681,  0.0740, -0.0000],\n",
      "          [-0.0364, -0.0517, -0.0000]],\n",
      "\n",
      "         [[-0.1022, -0.0000, -0.0461],\n",
      "          [-0.0000, -0.0801, -0.1010],\n",
      "          [-0.0000, -0.1159, -0.0719]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0947, -0.0355,  0.0948],\n",
      "          [ 0.0401, -0.0000,  0.0000],\n",
      "          [ 0.0365,  0.0848, -0.0545]],\n",
      "\n",
      "         [[ 0.0761, -0.0306,  0.0000],\n",
      "          [-0.0736,  0.1835,  0.0706],\n",
      "          [ 0.0000,  0.0623, -0.0000]],\n",
      "\n",
      "         [[ 0.0453, -0.0798,  0.0735],\n",
      "          [ 0.0422, -0.0473,  0.0000],\n",
      "          [ 0.0625,  0.0308, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0727, -0.1388],\n",
      "          [-0.0360,  0.0490, -0.0000],\n",
      "          [ 0.0000,  0.0700,  0.0422]],\n",
      "\n",
      "         [[ 0.0292,  0.0560,  0.0000],\n",
      "          [ 0.0465,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0820,  0.1337]],\n",
      "\n",
      "         [[ 0.0711,  0.0517,  0.0773],\n",
      "          [ 0.0519,  0.0000,  0.0374],\n",
      "          [ 0.0354, -0.0000,  0.0756]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0591],\n",
      "          [-0.0786, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0428,  0.0583]],\n",
      "\n",
      "         [[-0.0645,  0.0474,  0.0620],\n",
      "          [ 0.0000,  0.0379, -0.0304],\n",
      "          [ 0.0527,  0.1039, -0.0000]],\n",
      "\n",
      "         [[ 0.0525,  0.0000, -0.0000],\n",
      "          [ 0.1423,  0.0409, -0.0000],\n",
      "          [ 0.0834,  0.1399,  0.0389]]],\n",
      "\n",
      "\n",
      "        [[[-0.0885,  0.0000, -0.0000],\n",
      "          [-0.0570, -0.0322, -0.0472],\n",
      "          [-0.1429, -0.0482, -0.0654]],\n",
      "\n",
      "         [[ 0.0619,  0.0907,  0.0870],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0828,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0931, -0.0515, -0.0000],\n",
      "          [ 0.0000,  0.1142,  0.0437],\n",
      "          [ 0.0000,  0.0376,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0000, -0.0375,  0.0581],\n",
      "          [ 0.0425,  0.0506,  0.0000],\n",
      "          [-0.0526,  0.0591,  0.0000]],\n",
      "\n",
      "         [[-0.0582, -0.0000, -0.0500],\n",
      "          [-0.0730, -0.0419, -0.1037],\n",
      "          [-0.0530, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0496,  0.0399],\n",
      "          [-0.0000,  0.0673,  0.0000],\n",
      "          [ 0.0483, -0.0000, -0.0630]]]], device='cuda:0', requires_grad=True)\n",
      "layer3.2.bn2.weight Parameter containing:\n",
      "tensor([1.3652, 1.2650, 1.1348, 1.2475, 1.1440, 1.0753, 1.0384, 1.1657, 1.1708,\n",
      "        1.2182, 1.5170, 1.1305, 1.3534, 1.2528, 1.2124, 1.3685, 1.3934, 1.5430,\n",
      "        1.5818, 1.3168, 1.3841, 1.2201, 1.3528, 1.1594, 1.5507, 1.2579, 1.1513,\n",
      "        1.3545, 1.3460, 1.2866, 1.3760, 1.0596, 1.2253, 1.4965, 1.1482, 1.3132,\n",
      "        1.6232, 1.4277, 1.1632, 1.2271, 1.3102, 1.2526, 1.2658, 1.3471, 1.2403,\n",
      "        1.3970, 1.2730, 1.3073, 1.1588, 1.2388, 1.3589, 1.3059, 1.2016, 1.4273,\n",
      "        1.1955, 1.3332, 1.3847, 1.2838, 1.5066, 1.4054, 1.3908, 1.5422, 1.1963,\n",
      "        1.2055], device='cuda:0', requires_grad=True)\n",
      "layer3.2.bn2.bias Parameter containing:\n",
      "tensor([ 0.0265,  0.0370,  0.0221,  0.0665,  0.0322, -0.0431,  0.0105,  0.0474,\n",
      "         0.0463,  0.0017,  0.1337,  0.0506,  0.0583,  0.0737,  0.0554,  0.0975,\n",
      "         0.0434,  0.0411, -0.0002, -0.0047, -0.0103,  0.0034,  0.0261, -0.0279,\n",
      "         0.0273, -0.0154, -0.0450,  0.0225,  0.0217, -0.0230,  0.0176, -0.0509,\n",
      "         0.0028, -0.0564, -0.0280, -0.0235,  0.0136, -0.0240, -0.0096, -0.0582,\n",
      "         0.0299,  0.0039,  0.0743,  0.0039, -0.0521, -0.0178, -0.0017,  0.0291,\n",
      "         0.0051,  0.0404,  0.0845,  0.0781,  0.0679,  0.0135, -0.0127,  0.1122,\n",
      "         0.0693,  0.0595,  0.0833,  0.0884,  0.0501,  0.0092,  0.0362,  0.0143],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "linear.weight Parameter containing:\n",
      "tensor([[ 0.4479, -0.4563,  0.0000,  0.2608, -0.2381,  0.2408, -0.3052, -0.3728,\n",
      "         -0.2761, -0.0000,  0.0000, -0.4275, -0.0000, -0.4923,  0.0000, -0.3065,\n",
      "          0.7938, -0.5015, -0.0000, -0.0000,  0.5151, -0.3066, -0.0000, -0.3414,\n",
      "         -0.9806,  0.0000, -0.3481, -0.0000,  0.0000, -0.4823,  0.4858, -0.0000,\n",
      "          0.5342,  1.0786,  0.0000, -0.0000, -0.4992, -0.3223, -0.0000,  0.3608,\n",
      "         -0.4901, -0.3116, -0.0000,  0.0000, -0.2372,  1.0970,  0.0000,  0.4957,\n",
      "          0.4677,  0.7880, -0.5200,  0.4237,  0.5229,  0.0000,  0.6661,  0.0000,\n",
      "         -0.2340, -0.6389, -0.4453, -0.4489, -0.0000, -0.5531,  0.7661,  0.0000],\n",
      "        [ 0.3806, -0.0000, -0.4158, -0.0000, -0.2601, -0.0000,  0.2455,  0.7961,\n",
      "         -0.4765, -0.0000, -0.0000, -0.0000, -0.3405, -0.6274,  0.0000,  0.2603,\n",
      "         -0.0000, -0.0000, -0.2814, -0.0000, -0.0000,  0.0000,  0.8331,  0.7036,\n",
      "         -0.3204, -0.4299,  0.0000,  0.4652,  0.0000,  0.5840, -0.3730, -0.6231,\n",
      "          0.0000, -0.5922,  0.5412,  0.8806, -0.4888,  0.5676, -0.2922,  0.3835,\n",
      "          0.0000, -0.3472,  0.0000,  1.1335,  0.2723, -0.0000, -0.0000, -0.4048,\n",
      "         -0.3785,  0.0000,  0.0000,  0.2838,  0.0000, -0.0000,  0.2710, -0.0000,\n",
      "         -0.5290, -0.0000, -0.3263, -0.0000, -0.4062, -0.3679,  0.3776, -0.3212],\n",
      "        [-0.2991,  0.0000,  0.0000,  0.5742,  0.3542,  0.2739, -0.0000,  0.0000,\n",
      "         -0.0000,  0.6355,  0.0000, -0.0000, -0.0000, -0.0000,  0.6954, -0.8677,\n",
      "         -0.0000,  0.3042,  0.3870,  0.9094, -0.2973, -0.6230, -0.0000, -0.0000,\n",
      "          0.0000,  0.5638, -0.0000, -0.8781, -0.0000,  0.0000, -0.0000,  0.3794,\n",
      "          0.0000,  0.3164, -0.0000, -0.4127, -0.6479, -0.3895,  0.5412,  0.0000,\n",
      "          0.6432, -0.0000,  0.2785, -0.2915,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         -0.3874, -0.5316,  0.7871,  0.3520, -0.4071,  0.8120, -0.3177, -0.5983,\n",
      "         -0.3298,  0.0000, -0.7173, -0.6884,  0.5109,  0.0000, -0.2309,  0.0000],\n",
      "        [ 0.8696, -0.5759,  0.0000,  0.3479,  0.0000,  0.2622, -0.0000, -0.0000,\n",
      "          0.3812,  0.0000, -0.0000,  0.0000, -0.0000,  0.5271,  0.2681,  0.4290,\n",
      "         -0.5188, -0.7310,  0.3777, -0.0000, -0.9395,  0.0000, -0.6414,  0.0000,\n",
      "          0.3104,  0.0000, -0.0000, -0.3531,  0.8808,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.3845, -0.0000,  0.2748,\n",
      "          0.0000, -0.4365, -0.6233, -0.0000, -0.4220, -0.7213,  0.8858, -0.0000,\n",
      "         -0.4029,  0.4132, -0.3785, -0.0000,  0.2410, -0.3658, -0.0000,  0.6238,\n",
      "          0.3483,  0.3942,  0.0000,  0.0000, -0.0000,  0.8766, -0.4673, -0.0000],\n",
      "        [ 0.0000,  0.0000,  0.2677, -0.6181, -0.7138,  0.5623, -0.3873, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000,  0.0000,  1.2206,  0.0000,  0.2624,  0.0000,\n",
      "         -0.0000,  0.2985, -0.4728,  0.5043, -0.2549,  0.3927, -0.0000, -0.5091,\n",
      "          0.0000,  0.5751, -0.0000,  0.0000, -0.4124, -0.0000,  0.0000, -0.2857,\n",
      "          0.0000, -0.6926, -0.0000, -0.0000, -0.7058,  0.0000,  0.5093, -0.7504,\n",
      "          0.0000,  0.6793,  0.0000, -0.0000, -0.2838,  0.2495, -0.4672, -0.4645,\n",
      "          0.3036,  0.0000, -0.4236,  0.0000, -0.4716,  0.6825, -0.2778,  0.7383,\n",
      "          0.7182, -0.6820, -0.0000,  0.6733, -0.4491,  0.7320,  0.5110, -0.2685],\n",
      "        [-0.2334,  0.0000, -0.0000,  0.0000,  0.3363, -0.4454,  0.0000, -0.0000,\n",
      "          0.5394, -0.0000,  1.2541,  0.2935,  0.2527, -0.0000, -0.4284,  0.8437,\n",
      "         -0.3331, -0.3701, -0.4591,  0.2760,  0.4237,  0.0000, -0.0000, -0.2500,\n",
      "          1.3298,  0.3310, -0.0000, -0.3967,  0.0000, -0.0000,  0.2558, -0.0000,\n",
      "         -0.4371, -0.0000, -0.4885, -0.0000, -0.4370,  0.3494,  0.3277, -0.0000,\n",
      "         -0.4576,  0.0000, -0.0000, -0.4260,  0.4278, -0.0000,  0.0000, -0.3376,\n",
      "          0.0000,  0.0000,  0.2955, -0.6582, -0.0000, -0.4606,  0.6003,  0.2693,\n",
      "         -0.0000,  0.3387,  0.5720, -0.4208, -0.0000, -0.8498, -0.3029, -0.2439],\n",
      "        [-0.3245,  0.7231, -0.3988, -0.6777, -0.0000, -0.0000,  0.0000, -0.4503,\n",
      "         -0.3289, -0.5422, -0.6532, -0.3567,  0.2809, -0.4575, -0.3420,  0.0000,\n",
      "         -0.0000, -0.5861,  1.4276, -0.5202,  0.2575, -0.0000,  0.0000, -0.3214,\n",
      "         -0.0000, -0.0000,  0.0000, -0.2957,  0.3791, -0.0000, -0.3879,  0.0000,\n",
      "          0.6417, -0.3350, -0.4771,  0.2509,  0.0000, -0.0000,  0.5801, -0.2574,\n",
      "          0.3108,  0.3011, -0.6762, -0.0000, -0.0000,  0.0000,  0.0000,  0.4384,\n",
      "          0.2406,  0.0000,  0.2916,  0.3742, -0.2554, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000,  0.3998,  0.6203,  0.4451, -0.0000, -0.0000, -0.0000,  0.6977],\n",
      "        [-0.4918, -0.4244,  0.0000, -0.2650,  0.2792, -0.0000, -0.5092, -0.0000,\n",
      "          0.0000,  0.0000,  0.2861,  0.6041,  0.0000, -0.4382, -0.2429, -0.0000,\n",
      "          0.3983,  1.0790, -0.0000,  0.0000,  0.0000,  0.5259, -0.2767, -0.0000,\n",
      "         -0.0000, -0.3181, -0.0000,  0.0000, -0.6578,  0.6583, -0.7869,  0.0000,\n",
      "         -0.8477,  0.0000,  0.0000, -0.2300,  0.2549, -0.0000, -0.2875, -0.0000,\n",
      "         -0.4097, -0.5204, -0.4104, -0.5388,  0.8648, -0.2954, -0.0000,  0.5116,\n",
      "          0.0000,  0.4849, -0.0000,  0.5413,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "          0.8398, -0.0000,  0.7825,  0.5646,  0.9647, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.5151,  0.0000, -0.0000, -0.0000, -0.2727, -0.2583,\n",
      "         -0.0000,  0.5098, -0.0000, -0.4932, -0.2438,  0.2500, -0.2676, -0.3259,\n",
      "          0.5998,  0.2407, -0.0000, -0.0000,  0.0000, -0.4524,  0.5467,  0.2401,\n",
      "         -0.4341, -0.4813, -0.7274,  0.8322, -0.4538, -0.3164,  0.9338, -0.0000,\n",
      "         -0.0000, -0.3555, -0.0000, -0.5258,  1.0565,  0.0000, -0.0000,  0.3751,\n",
      "         -0.5511,  0.3516,  0.6978,  0.3236, -0.0000,  0.2980,  0.2466,  0.2925,\n",
      "         -0.3615, -0.4471,  0.3834, -0.3790,  0.6103, -0.6246, -0.0000, -0.5022,\n",
      "         -0.0000,  0.3418, -0.0000,  0.3179, -0.4355,  0.2821,  0.0000, -0.2644],\n",
      "        [-0.0000, -0.3302,  0.3987,  0.0000, -0.0000, -0.2333,  0.3650,  0.3120,\n",
      "          0.4020, -0.0000, -0.3490,  0.0000, -0.0000,  0.3748,  0.2964, -0.0000,\n",
      "          0.8123, -0.0000, -0.0000, -0.5139, -0.2480,  0.3657, -0.4835, -0.0000,\n",
      "         -0.4670, -0.2283,  0.6055,  0.3745,  0.0000, -0.5965, -0.3193, -0.0000,\n",
      "          0.2506,  0.0000,  0.2922,  0.7845,  0.3975,  1.3729, -0.3159, -0.4781,\n",
      "          0.4752, -0.0000,  0.4846, -0.2961,  0.0000, -0.3165, -0.3837,  0.3783,\n",
      "         -0.0000,  0.0000, -0.5208, -0.2613,  0.3786, -0.2415, -0.0000, -0.5853,\n",
      "         -0.3199, -0.0000, -0.5379, -0.3124,  0.0000, -0.0000, -0.0000,  0.5331]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "linear.bias Parameter containing:\n",
      "tensor([ 0.0503, -0.0900,  0.0713,  0.0552,  0.0804,  0.0000, -0.0385, -0.0000,\n",
      "        -0.0000,  0.0000], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = resnet20()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay = 0.0001, momentum=0.9)\n",
    "#for name, param in model.named_parameters():\n",
    " # print(name)\n",
    "model.load_state_dict(torch.load(\"/notebooks/resnet5k\"))\n",
    "test_model(model)\n",
    "for name, module in model.named_modules():\n",
    "  if isinstance(module, torch.nn.Conv2d):\n",
    "    prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "    #prune.l1_unstructured(module, name='bias', amount=0.2)\n",
    "  elif isinstance(module, torch.nn.Linear):\n",
    "    prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "    prune.l1_unstructured(module, name='bias', amount=0.4)  \n",
    "test_model(model)\n",
    "for name, module in model.named_modules():\n",
    "  if isinstance(module, torch.nn.Conv2d):\n",
    "    prune.remove(module, 'weight')\n",
    "    #prune.l1_unstructured(module, name='bias', amount=0.2)\n",
    "  elif isinstance(module, torch.nn.Linear):\n",
    "    prune.remove(module, 'weight')\n",
    "    prune.remove(module, 'bias')\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d938924f-982d-4a0b-b42f-d791b1f5f8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T02:43:33.701498Z",
     "iopub.status.busy": "2023-10-12T02:43:33.701091Z",
     "iopub.status.idle": "2023-10-12T02:43:33.706769Z",
     "shell.execute_reply": "2023-10-12T02:43:33.705667Z",
     "shell.execute_reply.started": "2023-10-12T02:43:33.701457Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_params(model):\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.view(-1))\n",
    "    params = torch.cat(params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c99be45-02d7-4c46-baee-8945fb145b4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T04:43:55.320254Z",
     "iopub.status.busy": "2023-10-10T04:43:55.319665Z",
     "iopub.status.idle": "2023-10-10T05:31:24.264984Z",
     "shell.execute_reply": "2023-10-10T05:31:24.263988Z",
     "shell.execute_reply.started": "2023-10-10T04:43:55.320233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "#########################################################\n",
      "trial: 0\n",
      "tensor([ 1546,  2515, 33064,  ..., 30894,  7113,  6395])\n",
      "Epoch: 0 \tTraining Loss: 1.200565 \tValidation Loss: 1.049041\n",
      "Training Accuracy (Overall): 74% (18686/25000)\n",
      "Validation Accuracy (Overall): 77% (7704/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.830751 \tValidation Loss: 0.756604\n",
      "Training Accuracy (Overall): 76% (19205/25000)\n",
      "Validation Accuracy (Overall): 78% (7824/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.682459 \tValidation Loss: 0.647191\n",
      "Training Accuracy (Overall): 78% (19546/25000)\n",
      "Validation Accuracy (Overall): 79% (7961/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.630958 \tValidation Loss: 0.598232\n",
      "Training Accuracy (Overall): 79% (19754/25000)\n",
      "Validation Accuracy (Overall): 80% (8072/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.596431 \tValidation Loss: 0.577510\n",
      "Training Accuracy (Overall): 79% (19936/25000)\n",
      "Validation Accuracy (Overall): 81% (8109/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.569912 \tValidation Loss: 0.568636\n",
      "Training Accuracy (Overall): 80% (20110/25000)\n",
      "Validation Accuracy (Overall): 81% (8113/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.548202 \tValidation Loss: 0.538378\n",
      "Training Accuracy (Overall): 81% (20300/25000)\n",
      "Validation Accuracy (Overall): 82% (8227/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.547361 \tValidation Loss: 0.527628\n",
      "Training Accuracy (Overall): 81% (20297/25000)\n",
      "Validation Accuracy (Overall): 82% (8227/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.525399 \tValidation Loss: 0.513309\n",
      "Training Accuracy (Overall): 81% (20475/25000)\n",
      "Validation Accuracy (Overall): 82% (8283/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.516595 \tValidation Loss: 0.513683\n",
      "Training Accuracy (Overall): 82% (20623/25000)\n",
      "Validation Accuracy (Overall): 83% (8301/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.500537 \tValidation Loss: 0.505608\n",
      "Training Accuracy (Overall): 83% (20750/25000)\n",
      "Validation Accuracy (Overall): 82% (8299/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.491928 \tValidation Loss: 0.506527\n",
      "Training Accuracy (Overall): 83% (20778/25000)\n",
      "Validation Accuracy (Overall): 83% (8333/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.483891 \tValidation Loss: 0.494488\n",
      "Training Accuracy (Overall): 83% (20905/25000)\n",
      "Validation Accuracy (Overall): 83% (8326/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.479165 \tValidation Loss: 0.478205\n",
      "Training Accuracy (Overall): 83% (20890/25000)\n",
      "Validation Accuracy (Overall): 83% (8384/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.476735 \tValidation Loss: 0.475895\n",
      "Training Accuracy (Overall): 83% (20947/25000)\n",
      "Validation Accuracy (Overall): 83% (8375/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.464724 \tValidation Loss: 0.475995\n",
      "Training Accuracy (Overall): 83% (20997/25000)\n",
      "Validation Accuracy (Overall): 83% (8357/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.456967 \tValidation Loss: 0.475519\n",
      "Training Accuracy (Overall): 84% (21155/25000)\n",
      "Validation Accuracy (Overall): 83% (8398/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.446433 \tValidation Loss: 0.460956\n",
      "Training Accuracy (Overall): 85% (21259/25000)\n",
      "Validation Accuracy (Overall): 84% (8447/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.446962 \tValidation Loss: 0.458588\n",
      "Training Accuracy (Overall): 84% (21204/25000)\n",
      "Validation Accuracy (Overall): 84% (8453/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.433275 \tValidation Loss: 0.444265\n",
      "Training Accuracy (Overall): 85% (21356/25000)\n",
      "Validation Accuracy (Overall): 85% (8505/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.436112 \tValidation Loss: 0.447829\n",
      "Training Accuracy (Overall): 85% (21372/25000)\n",
      "Validation Accuracy (Overall): 84% (8482/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.427887 \tValidation Loss: 0.438161\n",
      "Training Accuracy (Overall): 85% (21407/25000)\n",
      "Validation Accuracy (Overall): 85% (8531/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.425562 \tValidation Loss: 0.437875\n",
      "Training Accuracy (Overall): 85% (21497/25000)\n",
      "Validation Accuracy (Overall): 85% (8522/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.423711 \tValidation Loss: 0.436206\n",
      "Training Accuracy (Overall): 85% (21499/25000)\n",
      "Validation Accuracy (Overall): 85% (8527/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.414964 \tValidation Loss: 0.427361\n",
      "Training Accuracy (Overall): 86% (21607/25000)\n",
      "Validation Accuracy (Overall): 85% (8561/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.413751 \tValidation Loss: 0.430503\n",
      "Training Accuracy (Overall): 86% (21628/25000)\n",
      "Validation Accuracy (Overall): 85% (8540/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.414274 \tValidation Loss: 0.419278\n",
      "Training Accuracy (Overall): 86% (21619/25000)\n",
      "Validation Accuracy (Overall): 85% (8582/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.412283 \tValidation Loss: 0.419395\n",
      "Training Accuracy (Overall): 86% (21619/25000)\n",
      "Validation Accuracy (Overall): 85% (8598/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.404981 \tValidation Loss: 0.416407\n",
      "Training Accuracy (Overall): 86% (21695/25000)\n",
      "Validation Accuracy (Overall): 85% (8586/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.405533 \tValidation Loss: 0.417706\n",
      "Training Accuracy (Overall): 86% (21749/25000)\n",
      "Validation Accuracy (Overall): 86% (8601/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.402051 \tValidation Loss: 0.433551\n",
      "Training Accuracy (Overall): 87% (21764/25000)\n",
      "Validation Accuracy (Overall): 85% (8564/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.396715 \tValidation Loss: 0.440207\n",
      "Training Accuracy (Overall): 87% (21783/25000)\n",
      "Validation Accuracy (Overall): 85% (8508/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.394121 \tValidation Loss: 0.431664\n",
      "Training Accuracy (Overall): 87% (21896/25000)\n",
      "Validation Accuracy (Overall): 85% (8532/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.392142 \tValidation Loss: 0.444013\n",
      "Training Accuracy (Overall): 87% (21885/25000)\n",
      "Validation Accuracy (Overall): 85% (8508/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.387292 \tValidation Loss: 0.435830\n",
      "Training Accuracy (Overall): 87% (21891/25000)\n",
      "Validation Accuracy (Overall): 85% (8544/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.387778 \tValidation Loss: 0.414461\n",
      "Training Accuracy (Overall): 87% (21891/25000)\n",
      "Validation Accuracy (Overall): 86% (8640/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.383761 \tValidation Loss: 0.431856\n",
      "Training Accuracy (Overall): 88% (22022/25000)\n",
      "Validation Accuracy (Overall): 85% (8522/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.379654 \tValidation Loss: 0.409527\n",
      "Training Accuracy (Overall): 88% (22006/25000)\n",
      "Validation Accuracy (Overall): 86% (8657/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.378651 \tValidation Loss: 0.437608\n",
      "Training Accuracy (Overall): 87% (21996/25000)\n",
      "Validation Accuracy (Overall): 85% (8531/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.377818 \tValidation Loss: 0.399685\n",
      "Training Accuracy (Overall): 88% (22058/25000)\n",
      "Validation Accuracy (Overall): 86% (8670/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.383025 \tValidation Loss: 0.425423\n",
      "Training Accuracy (Overall): 87% (21968/25000)\n",
      "Validation Accuracy (Overall): 85% (8582/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.373159 \tValidation Loss: 0.398884\n",
      "Training Accuracy (Overall): 88% (22109/25000)\n",
      "Validation Accuracy (Overall): 86% (8662/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.373634 \tValidation Loss: 0.379918\n",
      "Training Accuracy (Overall): 88% (22135/25000)\n",
      "Validation Accuracy (Overall): 87% (8744/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.372915 \tValidation Loss: 0.437728\n",
      "Training Accuracy (Overall): 88% (22107/25000)\n",
      "Validation Accuracy (Overall): 85% (8544/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.369686 \tValidation Loss: 0.416777\n",
      "Training Accuracy (Overall): 88% (22091/25000)\n",
      "Validation Accuracy (Overall): 86% (8625/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.368758 \tValidation Loss: 0.408137\n",
      "Training Accuracy (Overall): 88% (22245/25000)\n",
      "Validation Accuracy (Overall): 86% (8651/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.366551 \tValidation Loss: 0.415351\n",
      "Training Accuracy (Overall): 88% (22209/25000)\n",
      "Validation Accuracy (Overall): 86% (8647/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.368459 \tValidation Loss: 0.411147\n",
      "Training Accuracy (Overall): 88% (22203/25000)\n",
      "Validation Accuracy (Overall): 86% (8653/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.368331 \tValidation Loss: 0.447532\n",
      "Training Accuracy (Overall): 88% (22181/25000)\n",
      "Validation Accuracy (Overall): 85% (8522/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.371597 \tValidation Loss: 0.428113\n",
      "Training Accuracy (Overall): 88% (22199/25000)\n",
      "Validation Accuracy (Overall): 85% (8557/10000)\n",
      "Test Loss: 0.226995\n",
      "\n",
      "Test Accuracy of airplane: 76% (761/1000)\n",
      "Test Accuracy of automobile: 83% (834/1000)\n",
      "Test Accuracy of  bird: 64% (649/1000)\n",
      "Test Accuracy of   cat: 58% (586/1000)\n",
      "Test Accuracy of  deer: 82% (820/1000)\n",
      "Test Accuracy of   dog: 83% (833/1000)\n",
      "Test Accuracy of  frog: 89% (897/1000)\n",
      "Test Accuracy of horse: 86% (865/1000)\n",
      "Test Accuracy of  ship: 93% (933/1000)\n",
      "Test Accuracy of truck: 93% (935/1000)\n",
      "\n",
      "Test Accuracy (Overall): 81% (8113/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 1\n",
      "tensor([37702, 26164, 14923,  ..., 15145, 28097, 31426])\n",
      "Epoch: 0 \tTraining Loss: 1.207753 \tValidation Loss: 1.068794\n",
      "Training Accuracy (Overall): 74% (18749/25000)\n",
      "Validation Accuracy (Overall): 76% (7693/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.857538 \tValidation Loss: 0.804326\n",
      "Training Accuracy (Overall): 76% (19081/25000)\n",
      "Validation Accuracy (Overall): 77% (7779/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.697150 \tValidation Loss: 0.657349\n",
      "Training Accuracy (Overall): 77% (19465/25000)\n",
      "Validation Accuracy (Overall): 79% (7942/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.638790 \tValidation Loss: 0.610147\n",
      "Training Accuracy (Overall): 79% (19759/25000)\n",
      "Validation Accuracy (Overall): 79% (7973/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.601194 \tValidation Loss: 0.574301\n",
      "Training Accuracy (Overall): 79% (19919/25000)\n",
      "Validation Accuracy (Overall): 80% (8094/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.581056 \tValidation Loss: 0.561154\n",
      "Training Accuracy (Overall): 80% (20123/25000)\n",
      "Validation Accuracy (Overall): 81% (8127/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.565874 \tValidation Loss: 0.548219\n",
      "Training Accuracy (Overall): 81% (20260/25000)\n",
      "Validation Accuracy (Overall): 81% (8167/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.548721 \tValidation Loss: 0.533897\n",
      "Training Accuracy (Overall): 81% (20372/25000)\n",
      "Validation Accuracy (Overall): 82% (8230/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.535106 \tValidation Loss: 0.535291\n",
      "Training Accuracy (Overall): 81% (20446/25000)\n",
      "Validation Accuracy (Overall): 82% (8218/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.520419 \tValidation Loss: 0.511528\n",
      "Training Accuracy (Overall): 82% (20595/25000)\n",
      "Validation Accuracy (Overall): 82% (8294/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.505228 \tValidation Loss: 0.505186\n",
      "Training Accuracy (Overall): 82% (20719/25000)\n",
      "Validation Accuracy (Overall): 83% (8314/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.501003 \tValidation Loss: 0.495893\n",
      "Training Accuracy (Overall): 83% (20785/25000)\n",
      "Validation Accuracy (Overall): 83% (8340/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.490991 \tValidation Loss: 0.487777\n",
      "Training Accuracy (Overall): 83% (20807/25000)\n",
      "Validation Accuracy (Overall): 83% (8356/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.480917 \tValidation Loss: 0.478700\n",
      "Training Accuracy (Overall): 83% (20930/25000)\n",
      "Validation Accuracy (Overall): 83% (8398/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.474368 \tValidation Loss: 0.478657\n",
      "Training Accuracy (Overall): 84% (21001/25000)\n",
      "Validation Accuracy (Overall): 83% (8397/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.466426 \tValidation Loss: 0.470341\n",
      "Training Accuracy (Overall): 84% (21027/25000)\n",
      "Validation Accuracy (Overall): 84% (8440/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.459366 \tValidation Loss: 0.463375\n",
      "Training Accuracy (Overall): 84% (21105/25000)\n",
      "Validation Accuracy (Overall): 84% (8425/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.453221 \tValidation Loss: 0.464349\n",
      "Training Accuracy (Overall): 84% (21162/25000)\n",
      "Validation Accuracy (Overall): 84% (8445/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.445683 \tValidation Loss: 0.464751\n",
      "Training Accuracy (Overall): 85% (21270/25000)\n",
      "Validation Accuracy (Overall): 84% (8425/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.439336 \tValidation Loss: 0.461146\n",
      "Training Accuracy (Overall): 85% (21354/25000)\n",
      "Validation Accuracy (Overall): 84% (8421/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.438707 \tValidation Loss: 0.457526\n",
      "Training Accuracy (Overall): 85% (21351/25000)\n",
      "Validation Accuracy (Overall): 84% (8446/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.437294 \tValidation Loss: 0.435419\n",
      "Training Accuracy (Overall): 85% (21399/25000)\n",
      "Validation Accuracy (Overall): 85% (8545/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.423633 \tValidation Loss: 0.451271\n",
      "Training Accuracy (Overall): 86% (21541/25000)\n",
      "Validation Accuracy (Overall): 84% (8483/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.426310 \tValidation Loss: 0.439888\n",
      "Training Accuracy (Overall): 86% (21509/25000)\n",
      "Validation Accuracy (Overall): 85% (8509/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.423457 \tValidation Loss: 0.443196\n",
      "Training Accuracy (Overall): 86% (21519/25000)\n",
      "Validation Accuracy (Overall): 85% (8529/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.418100 \tValidation Loss: 0.442928\n",
      "Training Accuracy (Overall): 86% (21631/25000)\n",
      "Validation Accuracy (Overall): 85% (8514/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.413448 \tValidation Loss: 0.441943\n",
      "Training Accuracy (Overall): 86% (21686/25000)\n",
      "Validation Accuracy (Overall): 85% (8510/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.413825 \tValidation Loss: 0.438565\n",
      "Training Accuracy (Overall): 86% (21682/25000)\n",
      "Validation Accuracy (Overall): 85% (8515/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.409296 \tValidation Loss: 0.431876\n",
      "Training Accuracy (Overall): 86% (21683/25000)\n",
      "Validation Accuracy (Overall): 85% (8567/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.404923 \tValidation Loss: 0.422653\n",
      "Training Accuracy (Overall): 87% (21772/25000)\n",
      "Validation Accuracy (Overall): 86% (8612/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.398384 \tValidation Loss: 0.430284\n",
      "Training Accuracy (Overall): 87% (21814/25000)\n",
      "Validation Accuracy (Overall): 85% (8545/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.401465 \tValidation Loss: 0.454398\n",
      "Training Accuracy (Overall): 86% (21736/25000)\n",
      "Validation Accuracy (Overall): 84% (8466/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.396347 \tValidation Loss: 0.424500\n",
      "Training Accuracy (Overall): 87% (21846/25000)\n",
      "Validation Accuracy (Overall): 85% (8582/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.396070 \tValidation Loss: 0.441370\n",
      "Training Accuracy (Overall): 87% (21881/25000)\n",
      "Validation Accuracy (Overall): 85% (8517/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.392477 \tValidation Loss: 0.422252\n",
      "Training Accuracy (Overall): 87% (21931/25000)\n",
      "Validation Accuracy (Overall): 85% (8584/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.390513 \tValidation Loss: 0.418768\n",
      "Training Accuracy (Overall): 87% (21937/25000)\n",
      "Validation Accuracy (Overall): 86% (8603/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.387268 \tValidation Loss: 0.414973\n",
      "Training Accuracy (Overall): 87% (21936/25000)\n",
      "Validation Accuracy (Overall): 86% (8642/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.387181 \tValidation Loss: 0.414439\n",
      "Training Accuracy (Overall): 87% (21996/25000)\n",
      "Validation Accuracy (Overall): 86% (8631/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.387344 \tValidation Loss: 0.418932\n",
      "Training Accuracy (Overall): 87% (21956/25000)\n",
      "Validation Accuracy (Overall): 86% (8604/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.383525 \tValidation Loss: 0.418842\n",
      "Training Accuracy (Overall): 88% (22015/25000)\n",
      "Validation Accuracy (Overall): 86% (8622/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.380897 \tValidation Loss: 0.419519\n",
      "Training Accuracy (Overall): 88% (22059/25000)\n",
      "Validation Accuracy (Overall): 86% (8608/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.382422 \tValidation Loss: 0.396687\n",
      "Training Accuracy (Overall): 88% (22055/25000)\n",
      "Validation Accuracy (Overall): 86% (8689/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.379665 \tValidation Loss: 0.453101\n",
      "Training Accuracy (Overall): 88% (22082/25000)\n",
      "Validation Accuracy (Overall): 85% (8509/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.379631 \tValidation Loss: 0.490697\n",
      "Training Accuracy (Overall): 88% (22066/25000)\n",
      "Validation Accuracy (Overall): 83% (8367/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.376306 \tValidation Loss: 0.415368\n",
      "Training Accuracy (Overall): 88% (22091/25000)\n",
      "Validation Accuracy (Overall): 86% (8622/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.377565 \tValidation Loss: 0.425305\n",
      "Training Accuracy (Overall): 88% (22102/25000)\n",
      "Validation Accuracy (Overall): 85% (8556/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.374037 \tValidation Loss: 0.422229\n",
      "Training Accuracy (Overall): 88% (22114/25000)\n",
      "Validation Accuracy (Overall): 86% (8607/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.371660 \tValidation Loss: 0.392349\n",
      "Training Accuracy (Overall): 88% (22146/25000)\n",
      "Validation Accuracy (Overall): 87% (8734/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.366220 \tValidation Loss: 0.410823\n",
      "Training Accuracy (Overall): 89% (22257/25000)\n",
      "Validation Accuracy (Overall): 86% (8646/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.370061 \tValidation Loss: 0.438347\n",
      "Training Accuracy (Overall): 88% (22226/25000)\n",
      "Validation Accuracy (Overall): 85% (8529/10000)\n",
      "Test Loss: 0.232243\n",
      "\n",
      "Test Accuracy of airplane: 79% (797/1000)\n",
      "Test Accuracy of automobile: 90% (901/1000)\n",
      "Test Accuracy of  bird: 68% (681/1000)\n",
      "Test Accuracy of   cat: 50% (506/1000)\n",
      "Test Accuracy of  deer: 82% (824/1000)\n",
      "Test Accuracy of   dog: 86% (865/1000)\n",
      "Test Accuracy of  frog: 84% (842/1000)\n",
      "Test Accuracy of horse: 87% (873/1000)\n",
      "Test Accuracy of  ship: 85% (859/1000)\n",
      "Test Accuracy of truck: 91% (917/1000)\n",
      "\n",
      "Test Accuracy (Overall): 80% (8065/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 2\n",
      "tensor([46114,  4039, 30924,  ...,  3073, 39768, 27263])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/1429271593.py:215: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if params_all == []:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Loss: 1.196212 \tValidation Loss: 1.073666\n",
      "Training Accuracy (Overall): 74% (18677/25000)\n",
      "Validation Accuracy (Overall): 76% (7669/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.861051 \tValidation Loss: 0.837008\n",
      "Training Accuracy (Overall): 76% (19014/25000)\n",
      "Validation Accuracy (Overall): 77% (7731/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.708338 \tValidation Loss: 0.659438\n",
      "Training Accuracy (Overall): 77% (19310/25000)\n",
      "Validation Accuracy (Overall): 79% (7929/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.635411 \tValidation Loss: 0.610263\n",
      "Training Accuracy (Overall): 78% (19628/25000)\n",
      "Validation Accuracy (Overall): 80% (8040/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.596084 \tValidation Loss: 0.578207\n",
      "Training Accuracy (Overall): 79% (19927/25000)\n",
      "Validation Accuracy (Overall): 81% (8113/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.579187 \tValidation Loss: 0.556874\n",
      "Training Accuracy (Overall): 80% (20133/25000)\n",
      "Validation Accuracy (Overall): 81% (8164/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.555501 \tValidation Loss: 0.542279\n",
      "Training Accuracy (Overall): 81% (20284/25000)\n",
      "Validation Accuracy (Overall): 82% (8208/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.543432 \tValidation Loss: 0.521950\n",
      "Training Accuracy (Overall): 81% (20390/25000)\n",
      "Validation Accuracy (Overall): 82% (8282/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.533308 \tValidation Loss: 0.526257\n",
      "Training Accuracy (Overall): 82% (20500/25000)\n",
      "Validation Accuracy (Overall): 82% (8245/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.518818 \tValidation Loss: 0.521829\n",
      "Training Accuracy (Overall): 82% (20542/25000)\n",
      "Validation Accuracy (Overall): 82% (8253/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.506045 \tValidation Loss: 0.504808\n",
      "Training Accuracy (Overall): 82% (20737/25000)\n",
      "Validation Accuracy (Overall): 83% (8332/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.495028 \tValidation Loss: 0.498024\n",
      "Training Accuracy (Overall): 83% (20772/25000)\n",
      "Validation Accuracy (Overall): 83% (8312/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.488063 \tValidation Loss: 0.493142\n",
      "Training Accuracy (Overall): 83% (20835/25000)\n",
      "Validation Accuracy (Overall): 83% (8332/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.480587 \tValidation Loss: 0.488626\n",
      "Training Accuracy (Overall): 83% (20908/25000)\n",
      "Validation Accuracy (Overall): 83% (8357/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.472032 \tValidation Loss: 0.492867\n",
      "Training Accuracy (Overall): 84% (21040/25000)\n",
      "Validation Accuracy (Overall): 83% (8353/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.463164 \tValidation Loss: 0.475351\n",
      "Training Accuracy (Overall): 84% (21081/25000)\n",
      "Validation Accuracy (Overall): 84% (8409/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.463845 \tValidation Loss: 0.469569\n",
      "Training Accuracy (Overall): 84% (21092/25000)\n",
      "Validation Accuracy (Overall): 84% (8413/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.456894 \tValidation Loss: 0.475247\n",
      "Training Accuracy (Overall): 84% (21096/25000)\n",
      "Validation Accuracy (Overall): 84% (8406/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.448854 \tValidation Loss: 0.461916\n",
      "Training Accuracy (Overall): 85% (21263/25000)\n",
      "Validation Accuracy (Overall): 84% (8443/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.443936 \tValidation Loss: 0.452240\n",
      "Training Accuracy (Overall): 85% (21316/25000)\n",
      "Validation Accuracy (Overall): 84% (8487/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.440249 \tValidation Loss: 0.464673\n",
      "Training Accuracy (Overall): 85% (21319/25000)\n",
      "Validation Accuracy (Overall): 84% (8446/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.432859 \tValidation Loss: 0.441500\n",
      "Training Accuracy (Overall): 85% (21360/25000)\n",
      "Validation Accuracy (Overall): 85% (8538/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.431039 \tValidation Loss: 0.463185\n",
      "Training Accuracy (Overall): 85% (21427/25000)\n",
      "Validation Accuracy (Overall): 84% (8454/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.427163 \tValidation Loss: 0.438678\n",
      "Training Accuracy (Overall): 85% (21446/25000)\n",
      "Validation Accuracy (Overall): 85% (8527/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.418830 \tValidation Loss: 0.439919\n",
      "Training Accuracy (Overall): 86% (21561/25000)\n",
      "Validation Accuracy (Overall): 85% (8519/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.418548 \tValidation Loss: 0.434497\n",
      "Training Accuracy (Overall): 86% (21568/25000)\n",
      "Validation Accuracy (Overall): 85% (8540/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.412059 \tValidation Loss: 0.438575\n",
      "Training Accuracy (Overall): 86% (21641/25000)\n",
      "Validation Accuracy (Overall): 84% (8492/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.411489 \tValidation Loss: 0.439645\n",
      "Training Accuracy (Overall): 86% (21618/25000)\n",
      "Validation Accuracy (Overall): 85% (8524/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.410847 \tValidation Loss: 0.442071\n",
      "Training Accuracy (Overall): 86% (21658/25000)\n",
      "Validation Accuracy (Overall): 85% (8548/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.403980 \tValidation Loss: 0.448529\n",
      "Training Accuracy (Overall): 87% (21750/25000)\n",
      "Validation Accuracy (Overall): 84% (8494/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.404185 \tValidation Loss: 0.436941\n",
      "Training Accuracy (Overall): 86% (21733/25000)\n",
      "Validation Accuracy (Overall): 85% (8559/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.398666 \tValidation Loss: 0.420056\n",
      "Training Accuracy (Overall): 87% (21787/25000)\n",
      "Validation Accuracy (Overall): 85% (8579/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.399361 \tValidation Loss: 0.418271\n",
      "Training Accuracy (Overall): 87% (21787/25000)\n",
      "Validation Accuracy (Overall): 86% (8645/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.396408 \tValidation Loss: 0.419320\n",
      "Training Accuracy (Overall): 87% (21851/25000)\n",
      "Validation Accuracy (Overall): 86% (8621/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.388393 \tValidation Loss: 0.421954\n",
      "Training Accuracy (Overall): 87% (21918/25000)\n",
      "Validation Accuracy (Overall): 86% (8605/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.390701 \tValidation Loss: 0.428665\n",
      "Training Accuracy (Overall): 87% (21820/25000)\n",
      "Validation Accuracy (Overall): 85% (8566/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.386063 \tValidation Loss: 0.408355\n",
      "Training Accuracy (Overall): 87% (21930/25000)\n",
      "Validation Accuracy (Overall): 86% (8660/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.387084 \tValidation Loss: 0.479314\n",
      "Training Accuracy (Overall): 87% (21950/25000)\n",
      "Validation Accuracy (Overall): 83% (8369/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.384515 \tValidation Loss: 0.411552\n",
      "Training Accuracy (Overall): 87% (21952/25000)\n",
      "Validation Accuracy (Overall): 86% (8606/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.383392 \tValidation Loss: 0.433726\n",
      "Training Accuracy (Overall): 88% (22042/25000)\n",
      "Validation Accuracy (Overall): 85% (8540/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.380559 \tValidation Loss: 0.424739\n",
      "Training Accuracy (Overall): 87% (21969/25000)\n",
      "Validation Accuracy (Overall): 85% (8594/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.378628 \tValidation Loss: 0.413468\n",
      "Training Accuracy (Overall): 88% (22092/25000)\n",
      "Validation Accuracy (Overall): 86% (8654/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.372812 \tValidation Loss: 0.427335\n",
      "Training Accuracy (Overall): 88% (22094/25000)\n",
      "Validation Accuracy (Overall): 86% (8612/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.376431 \tValidation Loss: 0.439469\n",
      "Training Accuracy (Overall): 88% (22066/25000)\n",
      "Validation Accuracy (Overall): 85% (8531/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.372719 \tValidation Loss: 0.439959\n",
      "Training Accuracy (Overall): 88% (22127/25000)\n",
      "Validation Accuracy (Overall): 85% (8553/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.372989 \tValidation Loss: 0.460580\n",
      "Training Accuracy (Overall): 88% (22145/25000)\n",
      "Validation Accuracy (Overall): 84% (8478/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.372299 \tValidation Loss: 0.425373\n",
      "Training Accuracy (Overall): 88% (22129/25000)\n",
      "Validation Accuracy (Overall): 86% (8609/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.364514 \tValidation Loss: 0.419597\n",
      "Training Accuracy (Overall): 88% (22249/25000)\n",
      "Validation Accuracy (Overall): 86% (8619/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.361947 \tValidation Loss: 0.424574\n",
      "Training Accuracy (Overall): 89% (22278/25000)\n",
      "Validation Accuracy (Overall): 85% (8587/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.365507 \tValidation Loss: 0.421300\n",
      "Training Accuracy (Overall): 88% (22221/25000)\n",
      "Validation Accuracy (Overall): 86% (8651/10000)\n",
      "Test Loss: 0.221971\n",
      "\n",
      "Test Accuracy of airplane: 80% (808/1000)\n",
      "Test Accuracy of automobile: 94% (949/1000)\n",
      "Test Accuracy of  bird: 59% (593/1000)\n",
      "Test Accuracy of   cat: 64% (645/1000)\n",
      "Test Accuracy of  deer: 83% (837/1000)\n",
      "Test Accuracy of   dog: 81% (814/1000)\n",
      "Test Accuracy of  frog: 88% (886/1000)\n",
      "Test Accuracy of horse: 86% (867/1000)\n",
      "Test Accuracy of  ship: 91% (916/1000)\n",
      "Test Accuracy of truck: 84% (848/1000)\n",
      "\n",
      "Test Accuracy (Overall): 81% (8163/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 3\n",
      "tensor([10612, 45938, 18710,  ..., 28589, 41414, 17894])\n",
      "Epoch: 0 \tTraining Loss: 1.193109 \tValidation Loss: 1.076911\n",
      "Training Accuracy (Overall): 75% (18772/25000)\n",
      "Validation Accuracy (Overall): 76% (7675/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.849280 \tValidation Loss: 0.791330\n",
      "Training Accuracy (Overall): 76% (19192/25000)\n",
      "Validation Accuracy (Overall): 78% (7818/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.698971 \tValidation Loss: 0.654755\n",
      "Training Accuracy (Overall): 77% (19425/25000)\n",
      "Validation Accuracy (Overall): 79% (7945/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.633447 \tValidation Loss: 0.608120\n",
      "Training Accuracy (Overall): 78% (19732/25000)\n",
      "Validation Accuracy (Overall): 80% (8029/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.603385 \tValidation Loss: 0.582001\n",
      "Training Accuracy (Overall): 79% (19900/25000)\n",
      "Validation Accuracy (Overall): 80% (8062/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.572599 \tValidation Loss: 0.560912\n",
      "Training Accuracy (Overall): 80% (20206/25000)\n",
      "Validation Accuracy (Overall): 81% (8147/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.553088 \tValidation Loss: 0.553422\n",
      "Training Accuracy (Overall): 81% (20320/25000)\n",
      "Validation Accuracy (Overall): 81% (8180/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.537096 \tValidation Loss: 0.553189\n",
      "Training Accuracy (Overall): 81% (20451/25000)\n",
      "Validation Accuracy (Overall): 81% (8153/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.527136 \tValidation Loss: 0.537700\n",
      "Training Accuracy (Overall): 82% (20545/25000)\n",
      "Validation Accuracy (Overall): 81% (8176/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.515387 \tValidation Loss: 0.519186\n",
      "Training Accuracy (Overall): 82% (20588/25000)\n",
      "Validation Accuracy (Overall): 82% (8245/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.504914 \tValidation Loss: 0.509309\n",
      "Training Accuracy (Overall): 82% (20742/25000)\n",
      "Validation Accuracy (Overall): 83% (8315/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.495007 \tValidation Loss: 0.496536\n",
      "Training Accuracy (Overall): 83% (20808/25000)\n",
      "Validation Accuracy (Overall): 83% (8301/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.484928 \tValidation Loss: 0.508924\n",
      "Training Accuracy (Overall): 83% (20912/25000)\n",
      "Validation Accuracy (Overall): 82% (8289/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.477849 \tValidation Loss: 0.482235\n",
      "Training Accuracy (Overall): 83% (20953/25000)\n",
      "Validation Accuracy (Overall): 83% (8337/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.464063 \tValidation Loss: 0.483419\n",
      "Training Accuracy (Overall): 84% (21085/25000)\n",
      "Validation Accuracy (Overall): 83% (8358/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.458542 \tValidation Loss: 0.486702\n",
      "Training Accuracy (Overall): 84% (21179/25000)\n",
      "Validation Accuracy (Overall): 83% (8355/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.454301 \tValidation Loss: 0.480030\n",
      "Training Accuracy (Overall): 84% (21173/25000)\n",
      "Validation Accuracy (Overall): 84% (8401/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.447154 \tValidation Loss: 0.474868\n",
      "Training Accuracy (Overall): 85% (21254/25000)\n",
      "Validation Accuracy (Overall): 84% (8403/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.442571 \tValidation Loss: 0.461339\n",
      "Training Accuracy (Overall): 85% (21303/25000)\n",
      "Validation Accuracy (Overall): 84% (8452/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.439482 \tValidation Loss: 0.464884\n",
      "Training Accuracy (Overall): 85% (21319/25000)\n",
      "Validation Accuracy (Overall): 84% (8464/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.437124 \tValidation Loss: 0.470090\n",
      "Training Accuracy (Overall): 85% (21377/25000)\n",
      "Validation Accuracy (Overall): 83% (8398/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.432066 \tValidation Loss: 0.436507\n",
      "Training Accuracy (Overall): 85% (21455/25000)\n",
      "Validation Accuracy (Overall): 85% (8505/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.426343 \tValidation Loss: 0.439657\n",
      "Training Accuracy (Overall): 85% (21468/25000)\n",
      "Validation Accuracy (Overall): 85% (8525/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.419867 \tValidation Loss: 0.451221\n",
      "Training Accuracy (Overall): 86% (21589/25000)\n",
      "Validation Accuracy (Overall): 84% (8460/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.420543 \tValidation Loss: 0.448584\n",
      "Training Accuracy (Overall): 86% (21539/25000)\n",
      "Validation Accuracy (Overall): 84% (8458/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.421047 \tValidation Loss: 0.429727\n",
      "Training Accuracy (Overall): 86% (21608/25000)\n",
      "Validation Accuracy (Overall): 85% (8546/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.411832 \tValidation Loss: 0.432570\n",
      "Training Accuracy (Overall): 86% (21656/25000)\n",
      "Validation Accuracy (Overall): 85% (8538/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.410573 \tValidation Loss: 0.437741\n",
      "Training Accuracy (Overall): 86% (21629/25000)\n",
      "Validation Accuracy (Overall): 85% (8519/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.407692 \tValidation Loss: 0.443793\n",
      "Training Accuracy (Overall): 86% (21728/25000)\n",
      "Validation Accuracy (Overall): 85% (8511/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.402453 \tValidation Loss: 0.446816\n",
      "Training Accuracy (Overall): 87% (21754/25000)\n",
      "Validation Accuracy (Overall): 84% (8482/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.404679 \tValidation Loss: 0.420083\n",
      "Training Accuracy (Overall): 86% (21721/25000)\n",
      "Validation Accuracy (Overall): 85% (8587/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.399224 \tValidation Loss: 0.443039\n",
      "Training Accuracy (Overall): 87% (21797/25000)\n",
      "Validation Accuracy (Overall): 85% (8500/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.396059 \tValidation Loss: 0.429032\n",
      "Training Accuracy (Overall): 87% (21865/25000)\n",
      "Validation Accuracy (Overall): 85% (8548/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.393445 \tValidation Loss: 0.425989\n",
      "Training Accuracy (Overall): 87% (21859/25000)\n",
      "Validation Accuracy (Overall): 85% (8563/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.386065 \tValidation Loss: 0.428503\n",
      "Training Accuracy (Overall): 87% (21923/25000)\n",
      "Validation Accuracy (Overall): 85% (8551/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.386515 \tValidation Loss: 0.418588\n",
      "Training Accuracy (Overall): 87% (21890/25000)\n",
      "Validation Accuracy (Overall): 85% (8598/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.385539 \tValidation Loss: 0.416844\n",
      "Training Accuracy (Overall): 87% (21976/25000)\n",
      "Validation Accuracy (Overall): 85% (8582/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.380896 \tValidation Loss: 0.410589\n",
      "Training Accuracy (Overall): 88% (22081/25000)\n",
      "Validation Accuracy (Overall): 86% (8634/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.384069 \tValidation Loss: 0.413548\n",
      "Training Accuracy (Overall): 88% (22032/25000)\n",
      "Validation Accuracy (Overall): 86% (8621/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.373434 \tValidation Loss: 0.412506\n",
      "Training Accuracy (Overall): 88% (22069/25000)\n",
      "Validation Accuracy (Overall): 86% (8625/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.379533 \tValidation Loss: 0.414940\n",
      "Training Accuracy (Overall): 88% (22046/25000)\n",
      "Validation Accuracy (Overall): 86% (8655/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.374878 \tValidation Loss: 0.427526\n",
      "Training Accuracy (Overall): 88% (22097/25000)\n",
      "Validation Accuracy (Overall): 85% (8576/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.375958 \tValidation Loss: 0.408535\n",
      "Training Accuracy (Overall): 88% (22084/25000)\n",
      "Validation Accuracy (Overall): 86% (8611/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.374954 \tValidation Loss: 0.397686\n",
      "Training Accuracy (Overall): 88% (22146/25000)\n",
      "Validation Accuracy (Overall): 86% (8697/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.371449 \tValidation Loss: 0.434776\n",
      "Training Accuracy (Overall): 88% (22149/25000)\n",
      "Validation Accuracy (Overall): 85% (8589/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.367007 \tValidation Loss: 0.437125\n",
      "Training Accuracy (Overall): 88% (22170/25000)\n",
      "Validation Accuracy (Overall): 85% (8536/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.362957 \tValidation Loss: 0.413461\n",
      "Training Accuracy (Overall): 88% (22224/25000)\n",
      "Validation Accuracy (Overall): 86% (8636/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.365780 \tValidation Loss: 0.444890\n",
      "Training Accuracy (Overall): 88% (22202/25000)\n",
      "Validation Accuracy (Overall): 85% (8513/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.369815 \tValidation Loss: 0.398016\n",
      "Training Accuracy (Overall): 88% (22238/25000)\n",
      "Validation Accuracy (Overall): 87% (8720/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.363848 \tValidation Loss: 0.449174\n",
      "Training Accuracy (Overall): 88% (22241/25000)\n",
      "Validation Accuracy (Overall): 84% (8497/10000)\n",
      "Test Loss: 0.231625\n",
      "\n",
      "Test Accuracy of airplane: 84% (846/1000)\n",
      "Test Accuracy of automobile: 86% (867/1000)\n",
      "Test Accuracy of  bird: 69% (690/1000)\n",
      "Test Accuracy of   cat: 67% (677/1000)\n",
      "Test Accuracy of  deer: 85% (852/1000)\n",
      "Test Accuracy of   dog: 55% (556/1000)\n",
      "Test Accuracy of  frog: 91% (914/1000)\n",
      "Test Accuracy of horse: 89% (895/1000)\n",
      "Test Accuracy of  ship: 90% (909/1000)\n",
      "Test Accuracy of truck: 86% (864/1000)\n",
      "\n",
      "Test Accuracy (Overall): 80% (8070/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 4\n",
      "tensor([40554, 27383, 45781,  ...,  4090,  7153,  7365])\n",
      "Epoch: 0 \tTraining Loss: 1.180259 \tValidation Loss: 1.061263\n",
      "Training Accuracy (Overall): 75% (18834/25000)\n",
      "Validation Accuracy (Overall): 77% (7740/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.847403 \tValidation Loss: 0.783140\n",
      "Training Accuracy (Overall): 76% (19212/25000)\n",
      "Validation Accuracy (Overall): 78% (7852/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.688205 \tValidation Loss: 0.664395\n",
      "Training Accuracy (Overall): 78% (19573/25000)\n",
      "Validation Accuracy (Overall): 79% (7935/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.623637 \tValidation Loss: 0.607901\n",
      "Training Accuracy (Overall): 79% (19778/25000)\n",
      "Validation Accuracy (Overall): 80% (8040/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.595010 \tValidation Loss: 0.578900\n",
      "Training Accuracy (Overall): 79% (19971/25000)\n",
      "Validation Accuracy (Overall): 80% (8078/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.573736 \tValidation Loss: 0.559687\n",
      "Training Accuracy (Overall): 80% (20159/25000)\n",
      "Validation Accuracy (Overall): 81% (8133/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.554804 \tValidation Loss: 0.559491\n",
      "Training Accuracy (Overall): 81% (20286/25000)\n",
      "Validation Accuracy (Overall): 81% (8151/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.535606 \tValidation Loss: 0.536577\n",
      "Training Accuracy (Overall): 81% (20417/25000)\n",
      "Validation Accuracy (Overall): 81% (8199/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.526906 \tValidation Loss: 0.523538\n",
      "Training Accuracy (Overall): 82% (20519/25000)\n",
      "Validation Accuracy (Overall): 82% (8239/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.513879 \tValidation Loss: 0.517521\n",
      "Training Accuracy (Overall): 82% (20634/25000)\n",
      "Validation Accuracy (Overall): 82% (8243/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.508402 \tValidation Loss: 0.506410\n",
      "Training Accuracy (Overall): 82% (20637/25000)\n",
      "Validation Accuracy (Overall): 82% (8283/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.489797 \tValidation Loss: 0.505064\n",
      "Training Accuracy (Overall): 83% (20848/25000)\n",
      "Validation Accuracy (Overall): 82% (8299/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.482918 \tValidation Loss: 0.510799\n",
      "Training Accuracy (Overall): 83% (20919/25000)\n",
      "Validation Accuracy (Overall): 82% (8256/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.473533 \tValidation Loss: 0.492088\n",
      "Training Accuracy (Overall): 84% (21005/25000)\n",
      "Validation Accuracy (Overall): 83% (8325/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.470436 \tValidation Loss: 0.475293\n",
      "Training Accuracy (Overall): 84% (21071/25000)\n",
      "Validation Accuracy (Overall): 83% (8372/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.459497 \tValidation Loss: 0.472830\n",
      "Training Accuracy (Overall): 84% (21125/25000)\n",
      "Validation Accuracy (Overall): 84% (8403/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.457504 \tValidation Loss: 0.461504\n",
      "Training Accuracy (Overall): 84% (21113/25000)\n",
      "Validation Accuracy (Overall): 84% (8453/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.451251 \tValidation Loss: 0.464746\n",
      "Training Accuracy (Overall): 84% (21179/25000)\n",
      "Validation Accuracy (Overall): 84% (8427/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.442564 \tValidation Loss: 0.462782\n",
      "Training Accuracy (Overall): 85% (21254/25000)\n",
      "Validation Accuracy (Overall): 84% (8403/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.444626 \tValidation Loss: 0.451681\n",
      "Training Accuracy (Overall): 85% (21266/25000)\n",
      "Validation Accuracy (Overall): 84% (8468/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.432991 \tValidation Loss: 0.446524\n",
      "Training Accuracy (Overall): 85% (21356/25000)\n",
      "Validation Accuracy (Overall): 85% (8500/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.432215 \tValidation Loss: 0.441653\n",
      "Training Accuracy (Overall): 85% (21393/25000)\n",
      "Validation Accuracy (Overall): 84% (8489/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.422800 \tValidation Loss: 0.444349\n",
      "Training Accuracy (Overall): 86% (21518/25000)\n",
      "Validation Accuracy (Overall): 84% (8455/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.420716 \tValidation Loss: 0.435193\n",
      "Training Accuracy (Overall): 86% (21605/25000)\n",
      "Validation Accuracy (Overall): 85% (8525/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.421242 \tValidation Loss: 0.439179\n",
      "Training Accuracy (Overall): 86% (21549/25000)\n",
      "Validation Accuracy (Overall): 85% (8512/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.416003 \tValidation Loss: 0.433607\n",
      "Training Accuracy (Overall): 86% (21599/25000)\n",
      "Validation Accuracy (Overall): 85% (8532/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.414051 \tValidation Loss: 0.445370\n",
      "Training Accuracy (Overall): 86% (21692/25000)\n",
      "Validation Accuracy (Overall): 84% (8461/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.408961 \tValidation Loss: 0.423954\n",
      "Training Accuracy (Overall): 86% (21653/25000)\n",
      "Validation Accuracy (Overall): 85% (8563/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.402398 \tValidation Loss: 0.418967\n",
      "Training Accuracy (Overall): 86% (21713/25000)\n",
      "Validation Accuracy (Overall): 85% (8587/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.406696 \tValidation Loss: 0.431211\n",
      "Training Accuracy (Overall): 86% (21717/25000)\n",
      "Validation Accuracy (Overall): 85% (8518/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.400276 \tValidation Loss: 0.412765\n",
      "Training Accuracy (Overall): 87% (21797/25000)\n",
      "Validation Accuracy (Overall): 85% (8590/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.397919 \tValidation Loss: 0.434823\n",
      "Training Accuracy (Overall): 87% (21793/25000)\n",
      "Validation Accuracy (Overall): 85% (8552/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.393950 \tValidation Loss: 0.439384\n",
      "Training Accuracy (Overall): 87% (21862/25000)\n",
      "Validation Accuracy (Overall): 85% (8518/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.393119 \tValidation Loss: 0.429417\n",
      "Training Accuracy (Overall): 87% (21881/25000)\n",
      "Validation Accuracy (Overall): 85% (8532/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.388718 \tValidation Loss: 0.420965\n",
      "Training Accuracy (Overall): 87% (21925/25000)\n",
      "Validation Accuracy (Overall): 85% (8568/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.381852 \tValidation Loss: 0.414570\n",
      "Training Accuracy (Overall): 88% (22022/25000)\n",
      "Validation Accuracy (Overall): 86% (8608/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.381999 \tValidation Loss: 0.418445\n",
      "Training Accuracy (Overall): 88% (22009/25000)\n",
      "Validation Accuracy (Overall): 85% (8579/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.384841 \tValidation Loss: 0.438415\n",
      "Training Accuracy (Overall): 87% (21966/25000)\n",
      "Validation Accuracy (Overall): 85% (8503/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.379573 \tValidation Loss: 0.417430\n",
      "Training Accuracy (Overall): 87% (21991/25000)\n",
      "Validation Accuracy (Overall): 86% (8637/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.377722 \tValidation Loss: 0.395407\n",
      "Training Accuracy (Overall): 88% (22045/25000)\n",
      "Validation Accuracy (Overall): 86% (8668/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.374799 \tValidation Loss: 0.432016\n",
      "Training Accuracy (Overall): 88% (22108/25000)\n",
      "Validation Accuracy (Overall): 85% (8543/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.372624 \tValidation Loss: 0.438060\n",
      "Training Accuracy (Overall): 88% (22104/25000)\n",
      "Validation Accuracy (Overall): 85% (8514/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.373719 \tValidation Loss: 0.399846\n",
      "Training Accuracy (Overall): 88% (22121/25000)\n",
      "Validation Accuracy (Overall): 86% (8669/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.368239 \tValidation Loss: 0.414078\n",
      "Training Accuracy (Overall): 88% (22217/25000)\n",
      "Validation Accuracy (Overall): 86% (8607/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.367637 \tValidation Loss: 0.429710\n",
      "Training Accuracy (Overall): 88% (22183/25000)\n",
      "Validation Accuracy (Overall): 85% (8571/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.368095 \tValidation Loss: 0.447600\n",
      "Training Accuracy (Overall): 88% (22171/25000)\n",
      "Validation Accuracy (Overall): 84% (8461/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.364085 \tValidation Loss: 0.409279\n",
      "Training Accuracy (Overall): 89% (22268/25000)\n",
      "Validation Accuracy (Overall): 86% (8625/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.367604 \tValidation Loss: 0.428794\n",
      "Training Accuracy (Overall): 88% (22184/25000)\n",
      "Validation Accuracy (Overall): 85% (8553/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.367290 \tValidation Loss: 0.420171\n",
      "Training Accuracy (Overall): 88% (22190/25000)\n",
      "Validation Accuracy (Overall): 85% (8574/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.364476 \tValidation Loss: 0.406742\n",
      "Training Accuracy (Overall): 88% (22234/25000)\n",
      "Validation Accuracy (Overall): 86% (8666/10000)\n",
      "Test Loss: 0.215618\n",
      "\n",
      "Test Accuracy of airplane: 81% (814/1000)\n",
      "Test Accuracy of automobile: 91% (919/1000)\n",
      "Test Accuracy of  bird: 70% (704/1000)\n",
      "Test Accuracy of   cat: 68% (683/1000)\n",
      "Test Accuracy of  deer: 88% (884/1000)\n",
      "Test Accuracy of   dog: 70% (707/1000)\n",
      "Test Accuracy of  frog: 87% (876/1000)\n",
      "Test Accuracy of horse: 88% (886/1000)\n",
      "Test Accuracy of  ship: 92% (922/1000)\n",
      "Test Accuracy of truck: 88% (883/1000)\n",
      "\n",
      "Test Accuracy (Overall): 82% (8278/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 5\n",
      "tensor([44473, 32071,  1977,  ..., 14148,  3314, 34041])\n",
      "Epoch: 0 \tTraining Loss: 1.185469 \tValidation Loss: 1.061140\n",
      "Training Accuracy (Overall): 75% (18798/25000)\n",
      "Validation Accuracy (Overall): 77% (7724/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.840343 \tValidation Loss: 0.751901\n",
      "Training Accuracy (Overall): 76% (19169/25000)\n",
      "Validation Accuracy (Overall): 78% (7897/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.693052 \tValidation Loss: 0.652339\n",
      "Training Accuracy (Overall): 77% (19474/25000)\n",
      "Validation Accuracy (Overall): 79% (7977/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.630276 \tValidation Loss: 0.612877\n",
      "Training Accuracy (Overall): 79% (19752/25000)\n",
      "Validation Accuracy (Overall): 80% (8025/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.592744 \tValidation Loss: 0.580882\n",
      "Training Accuracy (Overall): 79% (19912/25000)\n",
      "Validation Accuracy (Overall): 81% (8120/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.574220 \tValidation Loss: 0.569871\n",
      "Training Accuracy (Overall): 80% (20117/25000)\n",
      "Validation Accuracy (Overall): 81% (8155/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.556894 \tValidation Loss: 0.555082\n",
      "Training Accuracy (Overall): 81% (20267/25000)\n",
      "Validation Accuracy (Overall): 81% (8167/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.547481 \tValidation Loss: 0.535560\n",
      "Training Accuracy (Overall): 81% (20366/25000)\n",
      "Validation Accuracy (Overall): 82% (8256/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.528048 \tValidation Loss: 0.524628\n",
      "Training Accuracy (Overall): 81% (20467/25000)\n",
      "Validation Accuracy (Overall): 82% (8260/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.514362 \tValidation Loss: 0.516308\n",
      "Training Accuracy (Overall): 82% (20598/25000)\n",
      "Validation Accuracy (Overall): 82% (8286/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.504472 \tValidation Loss: 0.500763\n",
      "Training Accuracy (Overall): 82% (20744/25000)\n",
      "Validation Accuracy (Overall): 83% (8336/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.496100 \tValidation Loss: 0.491121\n",
      "Training Accuracy (Overall): 83% (20761/25000)\n",
      "Validation Accuracy (Overall): 83% (8396/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.490221 \tValidation Loss: 0.498997\n",
      "Training Accuracy (Overall): 83% (20850/25000)\n",
      "Validation Accuracy (Overall): 83% (8345/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.473389 \tValidation Loss: 0.493697\n",
      "Training Accuracy (Overall): 83% (20956/25000)\n",
      "Validation Accuracy (Overall): 83% (8359/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.467708 \tValidation Loss: 0.486195\n",
      "Training Accuracy (Overall): 84% (21059/25000)\n",
      "Validation Accuracy (Overall): 83% (8383/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.460572 \tValidation Loss: 0.481262\n",
      "Training Accuracy (Overall): 84% (21118/25000)\n",
      "Validation Accuracy (Overall): 83% (8387/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.455244 \tValidation Loss: 0.468603\n",
      "Training Accuracy (Overall): 84% (21167/25000)\n",
      "Validation Accuracy (Overall): 84% (8437/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.447754 \tValidation Loss: 0.479694\n",
      "Training Accuracy (Overall): 85% (21304/25000)\n",
      "Validation Accuracy (Overall): 83% (8379/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.447545 \tValidation Loss: 0.461082\n",
      "Training Accuracy (Overall): 84% (21229/25000)\n",
      "Validation Accuracy (Overall): 84% (8456/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.435536 \tValidation Loss: 0.461008\n",
      "Training Accuracy (Overall): 85% (21387/25000)\n",
      "Validation Accuracy (Overall): 84% (8469/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.434541 \tValidation Loss: 0.453547\n",
      "Training Accuracy (Overall): 85% (21393/25000)\n",
      "Validation Accuracy (Overall): 84% (8490/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.430380 \tValidation Loss: 0.444286\n",
      "Training Accuracy (Overall): 85% (21411/25000)\n",
      "Validation Accuracy (Overall): 85% (8520/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.421911 \tValidation Loss: 0.440376\n",
      "Training Accuracy (Overall): 86% (21521/25000)\n",
      "Validation Accuracy (Overall): 85% (8513/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.424884 \tValidation Loss: 0.445398\n",
      "Training Accuracy (Overall): 86% (21505/25000)\n",
      "Validation Accuracy (Overall): 85% (8504/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.416320 \tValidation Loss: 0.450739\n",
      "Training Accuracy (Overall): 86% (21569/25000)\n",
      "Validation Accuracy (Overall): 84% (8485/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.415933 \tValidation Loss: 0.448631\n",
      "Training Accuracy (Overall): 86% (21573/25000)\n",
      "Validation Accuracy (Overall): 84% (8487/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.409249 \tValidation Loss: 0.427016\n",
      "Training Accuracy (Overall): 86% (21719/25000)\n",
      "Validation Accuracy (Overall): 85% (8572/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.410884 \tValidation Loss: 0.441913\n",
      "Training Accuracy (Overall): 86% (21686/25000)\n",
      "Validation Accuracy (Overall): 85% (8538/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.404071 \tValidation Loss: 0.424644\n",
      "Training Accuracy (Overall): 87% (21766/25000)\n",
      "Validation Accuracy (Overall): 85% (8572/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.401289 \tValidation Loss: 0.443060\n",
      "Training Accuracy (Overall): 86% (21717/25000)\n",
      "Validation Accuracy (Overall): 85% (8519/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.397315 \tValidation Loss: 0.449021\n",
      "Training Accuracy (Overall): 87% (21802/25000)\n",
      "Validation Accuracy (Overall): 84% (8491/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.393745 \tValidation Loss: 0.430916\n",
      "Training Accuracy (Overall): 87% (21870/25000)\n",
      "Validation Accuracy (Overall): 85% (8568/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.391805 \tValidation Loss: 0.458100\n",
      "Training Accuracy (Overall): 87% (21872/25000)\n",
      "Validation Accuracy (Overall): 84% (8454/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.391058 \tValidation Loss: 0.430581\n",
      "Training Accuracy (Overall): 87% (21896/25000)\n",
      "Validation Accuracy (Overall): 85% (8572/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.386265 \tValidation Loss: 0.441163\n",
      "Training Accuracy (Overall): 87% (21951/25000)\n",
      "Validation Accuracy (Overall): 85% (8544/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.388201 \tValidation Loss: 0.426224\n",
      "Training Accuracy (Overall): 87% (21925/25000)\n",
      "Validation Accuracy (Overall): 85% (8594/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.387410 \tValidation Loss: 0.416751\n",
      "Training Accuracy (Overall): 87% (21903/25000)\n",
      "Validation Accuracy (Overall): 86% (8609/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.381143 \tValidation Loss: 0.406837\n",
      "Training Accuracy (Overall): 88% (22006/25000)\n",
      "Validation Accuracy (Overall): 86% (8670/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.380807 \tValidation Loss: 0.427569\n",
      "Training Accuracy (Overall): 88% (22052/25000)\n",
      "Validation Accuracy (Overall): 86% (8613/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.377780 \tValidation Loss: 0.405490\n",
      "Training Accuracy (Overall): 88% (22044/25000)\n",
      "Validation Accuracy (Overall): 86% (8663/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.373237 \tValidation Loss: 0.418378\n",
      "Training Accuracy (Overall): 88% (22140/25000)\n",
      "Validation Accuracy (Overall): 85% (8599/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.377545 \tValidation Loss: 0.425179\n",
      "Training Accuracy (Overall): 88% (22048/25000)\n",
      "Validation Accuracy (Overall): 86% (8601/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.368622 \tValidation Loss: 0.443416\n",
      "Training Accuracy (Overall): 88% (22143/25000)\n",
      "Validation Accuracy (Overall): 85% (8503/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.377187 \tValidation Loss: 0.444005\n",
      "Training Accuracy (Overall): 88% (22065/25000)\n",
      "Validation Accuracy (Overall): 85% (8523/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.371487 \tValidation Loss: 0.404013\n",
      "Training Accuracy (Overall): 88% (22140/25000)\n",
      "Validation Accuracy (Overall): 87% (8703/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.370441 \tValidation Loss: 0.431825\n",
      "Training Accuracy (Overall): 88% (22108/25000)\n",
      "Validation Accuracy (Overall): 86% (8606/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.364524 \tValidation Loss: 0.422911\n",
      "Training Accuracy (Overall): 88% (22218/25000)\n",
      "Validation Accuracy (Overall): 86% (8620/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.367427 \tValidation Loss: 0.426613\n",
      "Training Accuracy (Overall): 88% (22192/25000)\n",
      "Validation Accuracy (Overall): 85% (8561/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.369988 \tValidation Loss: 0.432742\n",
      "Training Accuracy (Overall): 88% (22220/25000)\n",
      "Validation Accuracy (Overall): 85% (8569/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.360769 \tValidation Loss: 0.442356\n",
      "Training Accuracy (Overall): 88% (22227/25000)\n",
      "Validation Accuracy (Overall): 85% (8515/10000)\n",
      "Test Loss: 0.225371\n",
      "\n",
      "Test Accuracy of airplane: 85% (854/1000)\n",
      "Test Accuracy of automobile: 91% (916/1000)\n",
      "Test Accuracy of  bird: 74% (747/1000)\n",
      "Test Accuracy of   cat: 47% (477/1000)\n",
      "Test Accuracy of  deer: 68% (682/1000)\n",
      "Test Accuracy of   dog: 81% (810/1000)\n",
      "Test Accuracy of  frog: 90% (904/1000)\n",
      "Test Accuracy of horse: 87% (877/1000)\n",
      "Test Accuracy of  ship: 91% (915/1000)\n",
      "Test Accuracy of truck: 90% (902/1000)\n",
      "\n",
      "Test Accuracy (Overall): 80% (8084/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 6\n",
      "tensor([17253,  3193,  3611,  ..., 25236, 33978, 41477])\n",
      "Epoch: 0 \tTraining Loss: 1.189213 \tValidation Loss: 1.056924\n",
      "Training Accuracy (Overall): 74% (18719/25000)\n",
      "Validation Accuracy (Overall): 77% (7709/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.847015 \tValidation Loss: 0.799550\n",
      "Training Accuracy (Overall): 76% (19042/25000)\n",
      "Validation Accuracy (Overall): 77% (7755/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.695939 \tValidation Loss: 0.646638\n",
      "Training Accuracy (Overall): 77% (19412/25000)\n",
      "Validation Accuracy (Overall): 79% (7940/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.627556 \tValidation Loss: 0.616006\n",
      "Training Accuracy (Overall): 79% (19779/25000)\n",
      "Validation Accuracy (Overall): 79% (7995/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.598418 \tValidation Loss: 0.571838\n",
      "Training Accuracy (Overall): 80% (20004/25000)\n",
      "Validation Accuracy (Overall): 81% (8112/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.580229 \tValidation Loss: 0.542925\n",
      "Training Accuracy (Overall): 80% (20091/25000)\n",
      "Validation Accuracy (Overall): 81% (8179/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.562633 \tValidation Loss: 0.541657\n",
      "Training Accuracy (Overall): 80% (20237/25000)\n",
      "Validation Accuracy (Overall): 81% (8184/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.543871 \tValidation Loss: 0.526790\n",
      "Training Accuracy (Overall): 81% (20385/25000)\n",
      "Validation Accuracy (Overall): 82% (8238/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.533927 \tValidation Loss: 0.527533\n",
      "Training Accuracy (Overall): 81% (20431/25000)\n",
      "Validation Accuracy (Overall): 82% (8251/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.519663 \tValidation Loss: 0.502449\n",
      "Training Accuracy (Overall): 82% (20570/25000)\n",
      "Validation Accuracy (Overall): 82% (8297/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.510068 \tValidation Loss: 0.507763\n",
      "Training Accuracy (Overall): 82% (20640/25000)\n",
      "Validation Accuracy (Overall): 83% (8341/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.496842 \tValidation Loss: 0.487665\n",
      "Training Accuracy (Overall): 83% (20783/25000)\n",
      "Validation Accuracy (Overall): 83% (8343/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.485674 \tValidation Loss: 0.483216\n",
      "Training Accuracy (Overall): 83% (20908/25000)\n",
      "Validation Accuracy (Overall): 83% (8346/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.478603 \tValidation Loss: 0.484947\n",
      "Training Accuracy (Overall): 83% (20972/25000)\n",
      "Validation Accuracy (Overall): 83% (8370/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.467284 \tValidation Loss: 0.471314\n",
      "Training Accuracy (Overall): 84% (21047/25000)\n",
      "Validation Accuracy (Overall): 84% (8424/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.467528 \tValidation Loss: 0.474582\n",
      "Training Accuracy (Overall): 84% (21078/25000)\n",
      "Validation Accuracy (Overall): 83% (8394/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.457933 \tValidation Loss: 0.459269\n",
      "Training Accuracy (Overall): 84% (21122/25000)\n",
      "Validation Accuracy (Overall): 84% (8457/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.449188 \tValidation Loss: 0.448191\n",
      "Training Accuracy (Overall): 84% (21229/25000)\n",
      "Validation Accuracy (Overall): 84% (8470/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.447448 \tValidation Loss: 0.444063\n",
      "Training Accuracy (Overall): 85% (21320/25000)\n",
      "Validation Accuracy (Overall): 84% (8499/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.443853 \tValidation Loss: 0.456312\n",
      "Training Accuracy (Overall): 85% (21334/25000)\n",
      "Validation Accuracy (Overall): 84% (8460/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.440798 \tValidation Loss: 0.440739\n",
      "Training Accuracy (Overall): 85% (21321/25000)\n",
      "Validation Accuracy (Overall): 84% (8483/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.438790 \tValidation Loss: 0.440552\n",
      "Training Accuracy (Overall): 85% (21400/25000)\n",
      "Validation Accuracy (Overall): 85% (8507/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.429787 \tValidation Loss: 0.447073\n",
      "Training Accuracy (Overall): 85% (21473/25000)\n",
      "Validation Accuracy (Overall): 84% (8466/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.425065 \tValidation Loss: 0.447554\n",
      "Training Accuracy (Overall): 86% (21535/25000)\n",
      "Validation Accuracy (Overall): 84% (8466/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.422026 \tValidation Loss: 0.442193\n",
      "Training Accuracy (Overall): 86% (21500/25000)\n",
      "Validation Accuracy (Overall): 84% (8490/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.419405 \tValidation Loss: 0.427097\n",
      "Training Accuracy (Overall): 86% (21532/25000)\n",
      "Validation Accuracy (Overall): 85% (8567/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.413857 \tValidation Loss: 0.427814\n",
      "Training Accuracy (Overall): 86% (21635/25000)\n",
      "Validation Accuracy (Overall): 85% (8534/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.411372 \tValidation Loss: 0.431302\n",
      "Training Accuracy (Overall): 86% (21681/25000)\n",
      "Validation Accuracy (Overall): 85% (8550/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.410647 \tValidation Loss: 0.436993\n",
      "Training Accuracy (Overall): 86% (21710/25000)\n",
      "Validation Accuracy (Overall): 85% (8511/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.406019 \tValidation Loss: 0.422155\n",
      "Training Accuracy (Overall): 86% (21718/25000)\n",
      "Validation Accuracy (Overall): 85% (8588/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.401309 \tValidation Loss: 0.418629\n",
      "Training Accuracy (Overall): 87% (21781/25000)\n",
      "Validation Accuracy (Overall): 85% (8597/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.399245 \tValidation Loss: 0.410203\n",
      "Training Accuracy (Overall): 87% (21815/25000)\n",
      "Validation Accuracy (Overall): 85% (8586/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.393498 \tValidation Loss: 0.408670\n",
      "Training Accuracy (Overall): 87% (21852/25000)\n",
      "Validation Accuracy (Overall): 86% (8613/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.392332 \tValidation Loss: 0.412255\n",
      "Training Accuracy (Overall): 87% (21871/25000)\n",
      "Validation Accuracy (Overall): 86% (8608/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.392842 \tValidation Loss: 0.439947\n",
      "Training Accuracy (Overall): 87% (21874/25000)\n",
      "Validation Accuracy (Overall): 84% (8466/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.386294 \tValidation Loss: 0.406849\n",
      "Training Accuracy (Overall): 87% (21975/25000)\n",
      "Validation Accuracy (Overall): 86% (8641/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.384366 \tValidation Loss: 0.401852\n",
      "Training Accuracy (Overall): 87% (21927/25000)\n",
      "Validation Accuracy (Overall): 86% (8660/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.385606 \tValidation Loss: 0.393599\n",
      "Training Accuracy (Overall): 87% (21950/25000)\n",
      "Validation Accuracy (Overall): 86% (8684/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.381284 \tValidation Loss: 0.428964\n",
      "Training Accuracy (Overall): 88% (22002/25000)\n",
      "Validation Accuracy (Overall): 85% (8509/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.385566 \tValidation Loss: 0.395583\n",
      "Training Accuracy (Overall): 87% (21955/25000)\n",
      "Validation Accuracy (Overall): 86% (8690/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.379166 \tValidation Loss: 0.398635\n",
      "Training Accuracy (Overall): 88% (22005/25000)\n",
      "Validation Accuracy (Overall): 86% (8656/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.374850 \tValidation Loss: 0.419144\n",
      "Training Accuracy (Overall): 88% (22150/25000)\n",
      "Validation Accuracy (Overall): 86% (8603/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.376368 \tValidation Loss: 0.421395\n",
      "Training Accuracy (Overall): 88% (22098/25000)\n",
      "Validation Accuracy (Overall): 85% (8556/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.372611 \tValidation Loss: 0.412803\n",
      "Training Accuracy (Overall): 88% (22172/25000)\n",
      "Validation Accuracy (Overall): 86% (8609/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.370864 \tValidation Loss: 0.395615\n",
      "Training Accuracy (Overall): 89% (22250/25000)\n",
      "Validation Accuracy (Overall): 86% (8646/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.372883 \tValidation Loss: 0.408453\n",
      "Training Accuracy (Overall): 88% (22148/25000)\n",
      "Validation Accuracy (Overall): 86% (8635/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.368668 \tValidation Loss: 0.413710\n",
      "Training Accuracy (Overall): 88% (22177/25000)\n",
      "Validation Accuracy (Overall): 86% (8601/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.366047 \tValidation Loss: 0.399077\n",
      "Training Accuracy (Overall): 88% (22213/25000)\n",
      "Validation Accuracy (Overall): 86% (8650/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.363468 \tValidation Loss: 0.391836\n",
      "Training Accuracy (Overall): 89% (22254/25000)\n",
      "Validation Accuracy (Overall): 87% (8718/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.362672 \tValidation Loss: 0.471136\n",
      "Training Accuracy (Overall): 89% (22290/25000)\n",
      "Validation Accuracy (Overall): 83% (8367/10000)\n",
      "Test Loss: 0.246368\n",
      "\n",
      "Test Accuracy of airplane: 83% (835/1000)\n",
      "Test Accuracy of automobile: 82% (826/1000)\n",
      "Test Accuracy of  bird: 85% (852/1000)\n",
      "Test Accuracy of   cat: 74% (747/1000)\n",
      "Test Accuracy of  deer: 79% (798/1000)\n",
      "Test Accuracy of   dog: 56% (561/1000)\n",
      "Test Accuracy of  frog: 70% (706/1000)\n",
      "Test Accuracy of horse: 81% (815/1000)\n",
      "Test Accuracy of  ship: 86% (860/1000)\n",
      "Test Accuracy of truck: 93% (939/1000)\n",
      "\n",
      "Test Accuracy (Overall): 79% (7939/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 7\n",
      "tensor([41580,   203, 16160,  ..., 11018, 17840, 33338])\n",
      "Epoch: 0 \tTraining Loss: 1.168275 \tValidation Loss: 1.068378\n",
      "Training Accuracy (Overall): 75% (18804/25000)\n",
      "Validation Accuracy (Overall): 76% (7678/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.838901 \tValidation Loss: 0.771491\n",
      "Training Accuracy (Overall): 76% (19103/25000)\n",
      "Validation Accuracy (Overall): 78% (7833/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.679542 \tValidation Loss: 0.653725\n",
      "Training Accuracy (Overall): 77% (19478/25000)\n",
      "Validation Accuracy (Overall): 79% (7961/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.619328 \tValidation Loss: 0.610870\n",
      "Training Accuracy (Overall): 79% (19795/25000)\n",
      "Validation Accuracy (Overall): 79% (7994/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.586992 \tValidation Loss: 0.575508\n",
      "Training Accuracy (Overall): 80% (20030/25000)\n",
      "Validation Accuracy (Overall): 80% (8088/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.571141 \tValidation Loss: 0.561373\n",
      "Training Accuracy (Overall): 80% (20112/25000)\n",
      "Validation Accuracy (Overall): 81% (8148/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.552109 \tValidation Loss: 0.553034\n",
      "Training Accuracy (Overall): 81% (20282/25000)\n",
      "Validation Accuracy (Overall): 81% (8136/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.531338 \tValidation Loss: 0.542492\n",
      "Training Accuracy (Overall): 81% (20422/25000)\n",
      "Validation Accuracy (Overall): 82% (8201/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.519517 \tValidation Loss: 0.520947\n",
      "Training Accuracy (Overall): 82% (20577/25000)\n",
      "Validation Accuracy (Overall): 82% (8291/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.510371 \tValidation Loss: 0.520321\n",
      "Training Accuracy (Overall): 82% (20634/25000)\n",
      "Validation Accuracy (Overall): 82% (8236/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.496852 \tValidation Loss: 0.506273\n",
      "Training Accuracy (Overall): 83% (20869/25000)\n",
      "Validation Accuracy (Overall): 83% (8306/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.487506 \tValidation Loss: 0.495291\n",
      "Training Accuracy (Overall): 83% (20924/25000)\n",
      "Validation Accuracy (Overall): 83% (8339/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.473847 \tValidation Loss: 0.488098\n",
      "Training Accuracy (Overall): 83% (20926/25000)\n",
      "Validation Accuracy (Overall): 83% (8327/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.471912 \tValidation Loss: 0.493917\n",
      "Training Accuracy (Overall): 83% (20986/25000)\n",
      "Validation Accuracy (Overall): 83% (8333/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.463139 \tValidation Loss: 0.476668\n",
      "Training Accuracy (Overall): 84% (21120/25000)\n",
      "Validation Accuracy (Overall): 83% (8373/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.459129 \tValidation Loss: 0.472523\n",
      "Training Accuracy (Overall): 84% (21080/25000)\n",
      "Validation Accuracy (Overall): 84% (8424/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.451079 \tValidation Loss: 0.464320\n",
      "Training Accuracy (Overall): 84% (21213/25000)\n",
      "Validation Accuracy (Overall): 84% (8418/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.437339 \tValidation Loss: 0.456815\n",
      "Training Accuracy (Overall): 85% (21309/25000)\n",
      "Validation Accuracy (Overall): 84% (8445/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.440276 \tValidation Loss: 0.452735\n",
      "Training Accuracy (Overall): 85% (21339/25000)\n",
      "Validation Accuracy (Overall): 84% (8471/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.434442 \tValidation Loss: 0.447802\n",
      "Training Accuracy (Overall): 85% (21365/25000)\n",
      "Validation Accuracy (Overall): 84% (8491/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.426877 \tValidation Loss: 0.457487\n",
      "Training Accuracy (Overall): 85% (21445/25000)\n",
      "Validation Accuracy (Overall): 84% (8445/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.426208 \tValidation Loss: 0.439691\n",
      "Training Accuracy (Overall): 85% (21448/25000)\n",
      "Validation Accuracy (Overall): 84% (8494/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.421284 \tValidation Loss: 0.446786\n",
      "Training Accuracy (Overall): 86% (21511/25000)\n",
      "Validation Accuracy (Overall): 84% (8463/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.413737 \tValidation Loss: 0.456749\n",
      "Training Accuracy (Overall): 86% (21647/25000)\n",
      "Validation Accuracy (Overall): 84% (8450/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.414989 \tValidation Loss: 0.451451\n",
      "Training Accuracy (Overall): 86% (21599/25000)\n",
      "Validation Accuracy (Overall): 84% (8468/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.408471 \tValidation Loss: 0.443534\n",
      "Training Accuracy (Overall): 86% (21658/25000)\n",
      "Validation Accuracy (Overall): 84% (8484/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.405586 \tValidation Loss: 0.425362\n",
      "Training Accuracy (Overall): 86% (21728/25000)\n",
      "Validation Accuracy (Overall): 85% (8533/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.401649 \tValidation Loss: 0.424099\n",
      "Training Accuracy (Overall): 87% (21788/25000)\n",
      "Validation Accuracy (Overall): 85% (8577/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.398151 \tValidation Loss: 0.435367\n",
      "Training Accuracy (Overall): 87% (21814/25000)\n",
      "Validation Accuracy (Overall): 85% (8547/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.398116 \tValidation Loss: 0.454648\n",
      "Training Accuracy (Overall): 87% (21796/25000)\n",
      "Validation Accuracy (Overall): 84% (8442/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.394863 \tValidation Loss: 0.422246\n",
      "Training Accuracy (Overall): 87% (21820/25000)\n",
      "Validation Accuracy (Overall): 85% (8578/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.392293 \tValidation Loss: 0.415689\n",
      "Training Accuracy (Overall): 87% (21848/25000)\n",
      "Validation Accuracy (Overall): 86% (8607/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.385667 \tValidation Loss: 0.431967\n",
      "Training Accuracy (Overall): 87% (21975/25000)\n",
      "Validation Accuracy (Overall): 85% (8557/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.384711 \tValidation Loss: 0.417689\n",
      "Training Accuracy (Overall): 87% (21931/25000)\n",
      "Validation Accuracy (Overall): 85% (8569/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.381662 \tValidation Loss: 0.410724\n",
      "Training Accuracy (Overall): 87% (21972/25000)\n",
      "Validation Accuracy (Overall): 86% (8621/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.385117 \tValidation Loss: 0.419178\n",
      "Training Accuracy (Overall): 87% (21932/25000)\n",
      "Validation Accuracy (Overall): 86% (8613/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.376991 \tValidation Loss: 0.399832\n",
      "Training Accuracy (Overall): 88% (22066/25000)\n",
      "Validation Accuracy (Overall): 86% (8668/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.379288 \tValidation Loss: 0.416132\n",
      "Training Accuracy (Overall): 88% (22039/25000)\n",
      "Validation Accuracy (Overall): 86% (8618/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.375626 \tValidation Loss: 0.445193\n",
      "Training Accuracy (Overall): 88% (22049/25000)\n",
      "Validation Accuracy (Overall): 84% (8495/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.374759 \tValidation Loss: 0.438192\n",
      "Training Accuracy (Overall): 88% (22009/25000)\n",
      "Validation Accuracy (Overall): 85% (8525/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.373463 \tValidation Loss: 0.408105\n",
      "Training Accuracy (Overall): 88% (22099/25000)\n",
      "Validation Accuracy (Overall): 86% (8628/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.372242 \tValidation Loss: 0.400912\n",
      "Training Accuracy (Overall): 88% (22095/25000)\n",
      "Validation Accuracy (Overall): 86% (8678/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.365773 \tValidation Loss: 0.426987\n",
      "Training Accuracy (Overall): 88% (22160/25000)\n",
      "Validation Accuracy (Overall): 85% (8590/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.367764 \tValidation Loss: 0.415703\n",
      "Training Accuracy (Overall): 88% (22199/25000)\n",
      "Validation Accuracy (Overall): 86% (8648/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.363895 \tValidation Loss: 0.467198\n",
      "Training Accuracy (Overall): 88% (22238/25000)\n",
      "Validation Accuracy (Overall): 84% (8406/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.365616 \tValidation Loss: 0.427366\n",
      "Training Accuracy (Overall): 88% (22186/25000)\n",
      "Validation Accuracy (Overall): 85% (8566/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.361105 \tValidation Loss: 0.411793\n",
      "Training Accuracy (Overall): 89% (22257/25000)\n",
      "Validation Accuracy (Overall): 86% (8612/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.363447 \tValidation Loss: 0.412611\n",
      "Training Accuracy (Overall): 89% (22254/25000)\n",
      "Validation Accuracy (Overall): 86% (8629/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.364284 \tValidation Loss: 0.391431\n",
      "Training Accuracy (Overall): 89% (22268/25000)\n",
      "Validation Accuracy (Overall): 87% (8716/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.356420 \tValidation Loss: 0.416745\n",
      "Training Accuracy (Overall): 89% (22340/25000)\n",
      "Validation Accuracy (Overall): 86% (8653/10000)\n",
      "Test Loss: 0.224159\n",
      "\n",
      "Test Accuracy of airplane: 84% (846/1000)\n",
      "Test Accuracy of automobile: 90% (904/1000)\n",
      "Test Accuracy of  bird: 67% (674/1000)\n",
      "Test Accuracy of   cat: 53% (537/1000)\n",
      "Test Accuracy of  deer: 79% (799/1000)\n",
      "Test Accuracy of   dog: 80% (803/1000)\n",
      "Test Accuracy of  frog: 83% (838/1000)\n",
      "Test Accuracy of horse: 90% (904/1000)\n",
      "Test Accuracy of  ship: 93% (936/1000)\n",
      "Test Accuracy of truck: 89% (893/1000)\n",
      "\n",
      "Test Accuracy (Overall): 81% (8134/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 8\n",
      "tensor([31365, 14956, 28797,  ...,  7449, 24310, 49928])\n",
      "Epoch: 0 \tTraining Loss: 1.196593 \tValidation Loss: 1.071642\n",
      "Training Accuracy (Overall): 75% (18802/25000)\n",
      "Validation Accuracy (Overall): 77% (7713/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.853745 \tValidation Loss: 0.784568\n",
      "Training Accuracy (Overall): 76% (19071/25000)\n",
      "Validation Accuracy (Overall): 78% (7832/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.690978 \tValidation Loss: 0.651704\n",
      "Training Accuracy (Overall): 77% (19430/25000)\n",
      "Validation Accuracy (Overall): 79% (7977/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.636299 \tValidation Loss: 0.590822\n",
      "Training Accuracy (Overall): 78% (19699/25000)\n",
      "Validation Accuracy (Overall): 80% (8037/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.598116 \tValidation Loss: 0.577649\n",
      "Training Accuracy (Overall): 79% (19982/25000)\n",
      "Validation Accuracy (Overall): 80% (8085/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.579009 \tValidation Loss: 0.552206\n",
      "Training Accuracy (Overall): 80% (20059/25000)\n",
      "Validation Accuracy (Overall): 81% (8152/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.559407 \tValidation Loss: 0.550539\n",
      "Training Accuracy (Overall): 81% (20285/25000)\n",
      "Validation Accuracy (Overall): 81% (8175/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.544202 \tValidation Loss: 0.531703\n",
      "Training Accuracy (Overall): 81% (20349/25000)\n",
      "Validation Accuracy (Overall): 82% (8244/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.531696 \tValidation Loss: 0.516176\n",
      "Training Accuracy (Overall): 81% (20476/25000)\n",
      "Validation Accuracy (Overall): 82% (8279/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.522671 \tValidation Loss: 0.514331\n",
      "Training Accuracy (Overall): 82% (20565/25000)\n",
      "Validation Accuracy (Overall): 82% (8280/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.507538 \tValidation Loss: 0.501665\n",
      "Training Accuracy (Overall): 82% (20657/25000)\n",
      "Validation Accuracy (Overall): 83% (8345/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.502043 \tValidation Loss: 0.501013\n",
      "Training Accuracy (Overall): 82% (20739/25000)\n",
      "Validation Accuracy (Overall): 83% (8338/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.491008 \tValidation Loss: 0.485534\n",
      "Training Accuracy (Overall): 83% (20852/25000)\n",
      "Validation Accuracy (Overall): 83% (8378/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.482982 \tValidation Loss: 0.480738\n",
      "Training Accuracy (Overall): 83% (20841/25000)\n",
      "Validation Accuracy (Overall): 83% (8385/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.474798 \tValidation Loss: 0.482203\n",
      "Training Accuracy (Overall): 83% (20987/25000)\n",
      "Validation Accuracy (Overall): 83% (8370/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.465369 \tValidation Loss: 0.479745\n",
      "Training Accuracy (Overall): 84% (21085/25000)\n",
      "Validation Accuracy (Overall): 83% (8381/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.463662 \tValidation Loss: 0.460859\n",
      "Training Accuracy (Overall): 84% (21094/25000)\n",
      "Validation Accuracy (Overall): 84% (8446/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.457079 \tValidation Loss: 0.460953\n",
      "Training Accuracy (Overall): 84% (21191/25000)\n",
      "Validation Accuracy (Overall): 84% (8427/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.445855 \tValidation Loss: 0.475301\n",
      "Training Accuracy (Overall): 85% (21273/25000)\n",
      "Validation Accuracy (Overall): 83% (8393/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.442673 \tValidation Loss: 0.450652\n",
      "Training Accuracy (Overall): 85% (21253/25000)\n",
      "Validation Accuracy (Overall): 84% (8479/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.437144 \tValidation Loss: 0.449311\n",
      "Training Accuracy (Overall): 85% (21361/25000)\n",
      "Validation Accuracy (Overall): 84% (8482/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.435381 \tValidation Loss: 0.438337\n",
      "Training Accuracy (Overall): 85% (21382/25000)\n",
      "Validation Accuracy (Overall): 85% (8510/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.431414 \tValidation Loss: 0.454561\n",
      "Training Accuracy (Overall): 85% (21437/25000)\n",
      "Validation Accuracy (Overall): 84% (8463/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.428477 \tValidation Loss: 0.467348\n",
      "Training Accuracy (Overall): 85% (21465/25000)\n",
      "Validation Accuracy (Overall): 83% (8397/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.420802 \tValidation Loss: 0.432470\n",
      "Training Accuracy (Overall): 86% (21527/25000)\n",
      "Validation Accuracy (Overall): 85% (8542/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.422813 \tValidation Loss: 0.441353\n",
      "Training Accuracy (Overall): 85% (21487/25000)\n",
      "Validation Accuracy (Overall): 85% (8519/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.413189 \tValidation Loss: 0.436938\n",
      "Training Accuracy (Overall): 86% (21604/25000)\n",
      "Validation Accuracy (Overall): 85% (8560/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.413370 \tValidation Loss: 0.432328\n",
      "Training Accuracy (Overall): 86% (21632/25000)\n",
      "Validation Accuracy (Overall): 85% (8526/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.415012 \tValidation Loss: 0.422215\n",
      "Training Accuracy (Overall): 86% (21593/25000)\n",
      "Validation Accuracy (Overall): 85% (8573/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.405233 \tValidation Loss: 0.427346\n",
      "Training Accuracy (Overall): 86% (21736/25000)\n",
      "Validation Accuracy (Overall): 85% (8573/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.405111 \tValidation Loss: 0.436270\n",
      "Training Accuracy (Overall): 86% (21663/25000)\n",
      "Validation Accuracy (Overall): 85% (8524/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.404701 \tValidation Loss: 0.420764\n",
      "Training Accuracy (Overall): 87% (21760/25000)\n",
      "Validation Accuracy (Overall): 85% (8594/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.401961 \tValidation Loss: 0.427500\n",
      "Training Accuracy (Overall): 87% (21771/25000)\n",
      "Validation Accuracy (Overall): 85% (8576/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.397343 \tValidation Loss: 0.416270\n",
      "Training Accuracy (Overall): 87% (21808/25000)\n",
      "Validation Accuracy (Overall): 86% (8620/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.395228 \tValidation Loss: 0.424329\n",
      "Training Accuracy (Overall): 87% (21833/25000)\n",
      "Validation Accuracy (Overall): 85% (8592/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.391260 \tValidation Loss: 0.427235\n",
      "Training Accuracy (Overall): 87% (21926/25000)\n",
      "Validation Accuracy (Overall): 85% (8585/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.391835 \tValidation Loss: 0.414804\n",
      "Training Accuracy (Overall): 87% (21880/25000)\n",
      "Validation Accuracy (Overall): 86% (8649/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.390344 \tValidation Loss: 0.438889\n",
      "Training Accuracy (Overall): 87% (21891/25000)\n",
      "Validation Accuracy (Overall): 85% (8522/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.386995 \tValidation Loss: 0.435038\n",
      "Training Accuracy (Overall): 87% (21934/25000)\n",
      "Validation Accuracy (Overall): 85% (8545/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.381823 \tValidation Loss: 0.436713\n",
      "Training Accuracy (Overall): 88% (22019/25000)\n",
      "Validation Accuracy (Overall): 85% (8562/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.382759 \tValidation Loss: 0.432720\n",
      "Training Accuracy (Overall): 87% (21977/25000)\n",
      "Validation Accuracy (Overall): 85% (8568/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.381502 \tValidation Loss: 0.423182\n",
      "Training Accuracy (Overall): 88% (22008/25000)\n",
      "Validation Accuracy (Overall): 85% (8570/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.377620 \tValidation Loss: 0.465463\n",
      "Training Accuracy (Overall): 88% (22075/25000)\n",
      "Validation Accuracy (Overall): 84% (8445/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.382553 \tValidation Loss: 0.444352\n",
      "Training Accuracy (Overall): 88% (22020/25000)\n",
      "Validation Accuracy (Overall): 85% (8530/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.377288 \tValidation Loss: 0.429073\n",
      "Training Accuracy (Overall): 88% (22103/25000)\n",
      "Validation Accuracy (Overall): 85% (8597/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.377090 \tValidation Loss: 0.390009\n",
      "Training Accuracy (Overall): 88% (22063/25000)\n",
      "Validation Accuracy (Overall): 87% (8713/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.371868 \tValidation Loss: 0.440252\n",
      "Training Accuracy (Overall): 88% (22213/25000)\n",
      "Validation Accuracy (Overall): 85% (8526/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.372549 \tValidation Loss: 0.407990\n",
      "Training Accuracy (Overall): 88% (22142/25000)\n",
      "Validation Accuracy (Overall): 86% (8633/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.367055 \tValidation Loss: 0.443196\n",
      "Training Accuracy (Overall): 88% (22216/25000)\n",
      "Validation Accuracy (Overall): 85% (8552/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.373480 \tValidation Loss: 0.404522\n",
      "Training Accuracy (Overall): 88% (22138/25000)\n",
      "Validation Accuracy (Overall): 86% (8690/10000)\n",
      "Test Loss: 0.217362\n",
      "\n",
      "Test Accuracy of airplane: 78% (781/1000)\n",
      "Test Accuracy of automobile: 92% (923/1000)\n",
      "Test Accuracy of  bird: 78% (781/1000)\n",
      "Test Accuracy of   cat: 70% (700/1000)\n",
      "Test Accuracy of  deer: 74% (744/1000)\n",
      "Test Accuracy of   dog: 73% (738/1000)\n",
      "Test Accuracy of  frog: 86% (868/1000)\n",
      "Test Accuracy of horse: 85% (858/1000)\n",
      "Test Accuracy of  ship: 88% (887/1000)\n",
      "Test Accuracy of truck: 88% (880/1000)\n",
      "\n",
      "Test Accuracy (Overall): 81% (8160/10000)\n",
      "##########################################################\n",
      "#########################################################\n",
      "trial: 9\n",
      "tensor([19018,   240, 21082,  ...,  2402,  4498,  6118])\n",
      "Epoch: 0 \tTraining Loss: 1.196736 \tValidation Loss: 1.065314\n",
      "Training Accuracy (Overall): 74% (18749/25000)\n",
      "Validation Accuracy (Overall): 77% (7746/10000)\n",
      "Epoch: 1 \tTraining Loss: 0.845106 \tValidation Loss: 0.789834\n",
      "Training Accuracy (Overall): 76% (19184/25000)\n",
      "Validation Accuracy (Overall): 78% (7810/10000)\n",
      "Epoch: 2 \tTraining Loss: 0.691458 \tValidation Loss: 0.651596\n",
      "Training Accuracy (Overall): 78% (19524/25000)\n",
      "Validation Accuracy (Overall): 79% (7960/10000)\n",
      "Epoch: 3 \tTraining Loss: 0.636360 \tValidation Loss: 0.606357\n",
      "Training Accuracy (Overall): 78% (19716/25000)\n",
      "Validation Accuracy (Overall): 80% (8032/10000)\n",
      "Epoch: 4 \tTraining Loss: 0.599112 \tValidation Loss: 0.572588\n",
      "Training Accuracy (Overall): 79% (19892/25000)\n",
      "Validation Accuracy (Overall): 80% (8099/10000)\n",
      "Epoch: 5 \tTraining Loss: 0.575444 \tValidation Loss: 0.567282\n",
      "Training Accuracy (Overall): 80% (20102/25000)\n",
      "Validation Accuracy (Overall): 81% (8112/10000)\n",
      "Epoch: 6 \tTraining Loss: 0.551405 \tValidation Loss: 0.555614\n",
      "Training Accuracy (Overall): 81% (20354/25000)\n",
      "Validation Accuracy (Overall): 81% (8166/10000)\n",
      "Epoch: 7 \tTraining Loss: 0.547767 \tValidation Loss: 0.537254\n",
      "Training Accuracy (Overall): 81% (20285/25000)\n",
      "Validation Accuracy (Overall): 82% (8223/10000)\n",
      "Epoch: 8 \tTraining Loss: 0.529132 \tValidation Loss: 0.529410\n",
      "Training Accuracy (Overall): 81% (20476/25000)\n",
      "Validation Accuracy (Overall): 82% (8219/10000)\n",
      "Epoch: 9 \tTraining Loss: 0.520672 \tValidation Loss: 0.521975\n",
      "Training Accuracy (Overall): 82% (20529/25000)\n",
      "Validation Accuracy (Overall): 82% (8253/10000)\n",
      "Epoch: 10 \tTraining Loss: 0.502732 \tValidation Loss: 0.512917\n",
      "Training Accuracy (Overall): 82% (20712/25000)\n",
      "Validation Accuracy (Overall): 82% (8282/10000)\n",
      "Epoch: 11 \tTraining Loss: 0.493697 \tValidation Loss: 0.506206\n",
      "Training Accuracy (Overall): 82% (20744/25000)\n",
      "Validation Accuracy (Overall): 83% (8311/10000)\n",
      "Epoch: 12 \tTraining Loss: 0.483915 \tValidation Loss: 0.494814\n",
      "Training Accuracy (Overall): 83% (20838/25000)\n",
      "Validation Accuracy (Overall): 83% (8350/10000)\n",
      "Epoch: 13 \tTraining Loss: 0.478011 \tValidation Loss: 0.486962\n",
      "Training Accuracy (Overall): 83% (20908/25000)\n",
      "Validation Accuracy (Overall): 83% (8377/10000)\n",
      "Epoch: 14 \tTraining Loss: 0.467146 \tValidation Loss: 0.478138\n",
      "Training Accuracy (Overall): 84% (21046/25000)\n",
      "Validation Accuracy (Overall): 83% (8373/10000)\n",
      "Epoch: 15 \tTraining Loss: 0.464604 \tValidation Loss: 0.469621\n",
      "Training Accuracy (Overall): 84% (21058/25000)\n",
      "Validation Accuracy (Overall): 84% (8416/10000)\n",
      "Epoch: 16 \tTraining Loss: 0.457844 \tValidation Loss: 0.476359\n",
      "Training Accuracy (Overall): 84% (21123/25000)\n",
      "Validation Accuracy (Overall): 84% (8406/10000)\n",
      "Epoch: 17 \tTraining Loss: 0.448170 \tValidation Loss: 0.461988\n",
      "Training Accuracy (Overall): 84% (21219/25000)\n",
      "Validation Accuracy (Overall): 84% (8464/10000)\n",
      "Epoch: 18 \tTraining Loss: 0.444847 \tValidation Loss: 0.469784\n",
      "Training Accuracy (Overall): 85% (21258/25000)\n",
      "Validation Accuracy (Overall): 83% (8379/10000)\n",
      "Epoch: 19 \tTraining Loss: 0.436709 \tValidation Loss: 0.451226\n",
      "Training Accuracy (Overall): 85% (21295/25000)\n",
      "Validation Accuracy (Overall): 84% (8496/10000)\n",
      "Epoch: 20 \tTraining Loss: 0.432981 \tValidation Loss: 0.482026\n",
      "Training Accuracy (Overall): 85% (21381/25000)\n",
      "Validation Accuracy (Overall): 83% (8377/10000)\n",
      "Epoch: 21 \tTraining Loss: 0.429912 \tValidation Loss: 0.449465\n",
      "Training Accuracy (Overall): 85% (21398/25000)\n",
      "Validation Accuracy (Overall): 84% (8487/10000)\n",
      "Epoch: 22 \tTraining Loss: 0.422044 \tValidation Loss: 0.451666\n",
      "Training Accuracy (Overall): 86% (21501/25000)\n",
      "Validation Accuracy (Overall): 84% (8467/10000)\n",
      "Epoch: 23 \tTraining Loss: 0.422292 \tValidation Loss: 0.430494\n",
      "Training Accuracy (Overall): 85% (21482/25000)\n",
      "Validation Accuracy (Overall): 85% (8535/10000)\n",
      "Epoch: 24 \tTraining Loss: 0.417784 \tValidation Loss: 0.442486\n",
      "Training Accuracy (Overall): 86% (21543/25000)\n",
      "Validation Accuracy (Overall): 85% (8530/10000)\n",
      "Epoch: 25 \tTraining Loss: 0.415941 \tValidation Loss: 0.440352\n",
      "Training Accuracy (Overall): 86% (21625/25000)\n",
      "Validation Accuracy (Overall): 85% (8531/10000)\n",
      "Epoch: 26 \tTraining Loss: 0.409231 \tValidation Loss: 0.442486\n",
      "Training Accuracy (Overall): 86% (21708/25000)\n",
      "Validation Accuracy (Overall): 84% (8489/10000)\n",
      "Epoch: 27 \tTraining Loss: 0.409339 \tValidation Loss: 0.435680\n",
      "Training Accuracy (Overall): 86% (21635/25000)\n",
      "Validation Accuracy (Overall): 85% (8508/10000)\n",
      "Epoch: 28 \tTraining Loss: 0.404207 \tValidation Loss: 0.436963\n",
      "Training Accuracy (Overall): 86% (21719/25000)\n",
      "Validation Accuracy (Overall): 85% (8507/10000)\n",
      "Epoch: 29 \tTraining Loss: 0.400474 \tValidation Loss: 0.423790\n",
      "Training Accuracy (Overall): 87% (21759/25000)\n",
      "Validation Accuracy (Overall): 85% (8559/10000)\n",
      "Epoch: 30 \tTraining Loss: 0.398497 \tValidation Loss: 0.413304\n",
      "Training Accuracy (Overall): 87% (21777/25000)\n",
      "Validation Accuracy (Overall): 86% (8621/10000)\n",
      "Epoch: 31 \tTraining Loss: 0.395203 \tValidation Loss: 0.442211\n",
      "Training Accuracy (Overall): 87% (21806/25000)\n",
      "Validation Accuracy (Overall): 85% (8515/10000)\n",
      "Epoch: 32 \tTraining Loss: 0.392803 \tValidation Loss: 0.436621\n",
      "Training Accuracy (Overall): 87% (21807/25000)\n",
      "Validation Accuracy (Overall): 85% (8543/10000)\n",
      "Epoch: 33 \tTraining Loss: 0.391869 \tValidation Loss: 0.435236\n",
      "Training Accuracy (Overall): 87% (21896/25000)\n",
      "Validation Accuracy (Overall): 85% (8538/10000)\n",
      "Epoch: 34 \tTraining Loss: 0.389732 \tValidation Loss: 0.407575\n",
      "Training Accuracy (Overall): 87% (21893/25000)\n",
      "Validation Accuracy (Overall): 86% (8650/10000)\n",
      "Epoch: 35 \tTraining Loss: 0.387890 \tValidation Loss: 0.424083\n",
      "Training Accuracy (Overall): 87% (21843/25000)\n",
      "Validation Accuracy (Overall): 85% (8569/10000)\n",
      "Epoch: 36 \tTraining Loss: 0.384022 \tValidation Loss: 0.421708\n",
      "Training Accuracy (Overall): 87% (21953/25000)\n",
      "Validation Accuracy (Overall): 86% (8607/10000)\n",
      "Epoch: 37 \tTraining Loss: 0.383622 \tValidation Loss: 0.415446\n",
      "Training Accuracy (Overall): 87% (21942/25000)\n",
      "Validation Accuracy (Overall): 85% (8585/10000)\n",
      "Epoch: 38 \tTraining Loss: 0.377353 \tValidation Loss: 0.420256\n",
      "Training Accuracy (Overall): 88% (22072/25000)\n",
      "Validation Accuracy (Overall): 85% (8581/10000)\n",
      "Epoch: 39 \tTraining Loss: 0.378830 \tValidation Loss: 0.417023\n",
      "Training Accuracy (Overall): 88% (22019/25000)\n",
      "Validation Accuracy (Overall): 86% (8601/10000)\n",
      "Epoch: 40 \tTraining Loss: 0.378190 \tValidation Loss: 0.409045\n",
      "Training Accuracy (Overall): 88% (22049/25000)\n",
      "Validation Accuracy (Overall): 86% (8637/10000)\n",
      "Epoch: 41 \tTraining Loss: 0.371444 \tValidation Loss: 0.407666\n",
      "Training Accuracy (Overall): 88% (22140/25000)\n",
      "Validation Accuracy (Overall): 86% (8634/10000)\n",
      "Epoch: 42 \tTraining Loss: 0.372323 \tValidation Loss: 0.415907\n",
      "Training Accuracy (Overall): 88% (22129/25000)\n",
      "Validation Accuracy (Overall): 86% (8604/10000)\n",
      "Epoch: 43 \tTraining Loss: 0.371561 \tValidation Loss: 0.416454\n",
      "Training Accuracy (Overall): 88% (22140/25000)\n",
      "Validation Accuracy (Overall): 85% (8567/10000)\n",
      "Epoch: 44 \tTraining Loss: 0.372321 \tValidation Loss: 0.444042\n",
      "Training Accuracy (Overall): 88% (22155/25000)\n",
      "Validation Accuracy (Overall): 85% (8519/10000)\n",
      "Epoch: 45 \tTraining Loss: 0.365367 \tValidation Loss: 0.455651\n",
      "Training Accuracy (Overall): 88% (22222/25000)\n",
      "Validation Accuracy (Overall): 84% (8450/10000)\n",
      "Epoch: 46 \tTraining Loss: 0.364292 \tValidation Loss: 0.421010\n",
      "Training Accuracy (Overall): 88% (22186/25000)\n",
      "Validation Accuracy (Overall): 86% (8609/10000)\n",
      "Epoch: 47 \tTraining Loss: 0.367586 \tValidation Loss: 0.427692\n",
      "Training Accuracy (Overall): 88% (22218/25000)\n",
      "Validation Accuracy (Overall): 85% (8565/10000)\n",
      "Epoch: 48 \tTraining Loss: 0.363515 \tValidation Loss: 0.421897\n",
      "Training Accuracy (Overall): 88% (22203/25000)\n",
      "Validation Accuracy (Overall): 86% (8606/10000)\n",
      "Epoch: 49 \tTraining Loss: 0.358410 \tValidation Loss: 0.396662\n",
      "Training Accuracy (Overall): 89% (22289/25000)\n",
      "Validation Accuracy (Overall): 86% (8681/10000)\n",
      "Test Loss: 0.216989\n",
      "\n",
      "Test Accuracy of airplane: 80% (807/1000)\n",
      "Test Accuracy of automobile: 92% (925/1000)\n",
      "Test Accuracy of  bird: 68% (689/1000)\n",
      "Test Accuracy of   cat: 78% (784/1000)\n",
      "Test Accuracy of  deer: 77% (774/1000)\n",
      "Test Accuracy of   dog: 74% (740/1000)\n",
      "Test Accuracy of  frog: 86% (862/1000)\n",
      "Test Accuracy of horse: 83% (832/1000)\n",
      "Test Accuracy of  ship: 91% (913/1000)\n",
      "Test Accuracy of truck: 88% (885/1000)\n",
      "\n",
      "Test Accuracy (Overall): 82% (8211/10000)\n",
      "81.21700000000001\n"
     ]
    }
   ],
   "source": [
    "#matrix = np.array([])\n",
    "params_all = []\n",
    "count = 1\n",
    "avg = 0 \n",
    "for i in range(10): \n",
    "    params_trial = []\n",
    "    print(\"##########################################################\")\n",
    "    print(\"#########################################################\")\n",
    "    print(\"trial: \" + str(i))\n",
    "    indices = torch.randperm(len(trainset))[:25000]\n",
    "    print(indices)\n",
    "    trainset1 = torch.utils.data.Subset(trainset, indices)\n",
    "    modelloader = torch.utils.data.DataLoader(trainset1, shuffle=True, num_workers=2, batch_size = 100)\n",
    "    testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size = 20)\n",
    "    #model pretrained by resnet\n",
    "    #model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "    model = resnet20()\n",
    "    model.load_state_dict(torch.load(\"/notebooks/resnet5k\"))\n",
    "    model.to(device) \n",
    "    #print(get_n_params(model))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay = 0.001, momentum = 0.9)\n",
    "    ##########\n",
    "    #prune model by removing 0.2 of least\n",
    "    ##########\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "      if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.7)\n",
    "        #prune.l1_unstructured(module, name='bias', amount=0.2)\n",
    "      elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.7)\n",
    "        prune.l1_unstructured(module, name='bias', amount=0.7) \n",
    "    test_model(model)\n",
    "    \"\"\"\n",
    "  ################\n",
    "  #training model\n",
    "  ####################\n",
    "    train_losslist = []\n",
    "    n_epochs = [*range(50)]\n",
    "    valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "    for epoch in n_epochs:\n",
    "    # keep track of training and validation loss\n",
    "        train_loss = 0.0\n",
    "        train_loss1 = 0.0\n",
    "        valid_loss = 0.0   \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "        model.train()\n",
    "    #model1.train()\n",
    "        class_correct1 = list(0. for i in range(10))\n",
    "        class_total1 = list(0. for i in range(10))\n",
    "        for data, target in modelloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "            data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "        #optimizer1.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "        #output1 = model1(data)\n",
    "        # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "        #loss1 = criterion1(output1,target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5, norm_type=2)\n",
    "        #loss1.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "        #exp_lr_scheduler.step()\n",
    "        #optimizer1.step()\n",
    "        # update training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        #train_loss1 += loss1.item()*data.size(0)\n",
    "            _, pred = torch.max(output, 1)    \n",
    "        # compare predictions to true label\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # calculate test accuracy for each object class\n",
    "            for i in range(100):\n",
    "                label = target.data[i]\n",
    "                class_correct1[label] += correct[i].item()\n",
    "                class_total1[label] += 1\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "        model.eval()\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        for data, target in validloader:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            #if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            # convert output probabilities to predicted class\n",
    "            _, pred = torch.max(output, 1)    \n",
    "            # compare predictions to true label\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "            # calculate test accuracy for each object class\n",
    "            for i in range(100):\n",
    "                label = target.data[i]\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "            # calculate average losses\n",
    "        valid_loss = valid_loss/len(validloader.dataset)\n",
    "        train_loss = train_loss/len(modelloader.dataset)\n",
    "        train_losslist.append(train_loss)\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "        print('Training Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct1) / np.sum(class_total1),\n",
    "        np.sum(class_correct1), np.sum(class_total1)))\n",
    "        print('Validation Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "        if epoch == 0:\n",
    "            params_trial = get_params(model).cpu().detach().numpy()\n",
    "           # print(np.shape(params_trial))\n",
    "            params_trial = np.reshape(params_trial,(1,-1))\n",
    "        else:\n",
    "            temp =get_params(model).cpu().detach().numpy()\n",
    "            temp = np.reshape(temp,(1,-1))\n",
    "            params_trial = np.append(params_trial, temp, axis = 0)\n",
    "     #   print(np.shape(params_trial))\n",
    "    ########################\n",
    "    # testing model #\n",
    "    ######################### \n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "      if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "        #prune.l1_unstructured(module, name='bias', amount=0.2)\n",
    "      elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "        prune.l1_unstructured(module, name='bias', amount=0.2) \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "      if isinstance(module, torch.nn.Conv2d):\n",
    "        #prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "        prune.remove(module, 'weight')\n",
    "      elif isinstance(module, torch.nn.Linear):\n",
    "       #prune.l1_unstructured(module, name='weight', amount=0.4)\n",
    "       # prune.l1_unstructured(module, name='bias', amount=0.4)\n",
    "        prune.remove(module, 'weight')\n",
    "        prune.remove(module, 'bias')\n",
    "    \"\"\"\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    test_loss = 0.\n",
    "    train_on_gpu = torch.cuda.is_available()\n",
    "    model.eval()\n",
    "    # iterate over test data\n",
    "    for data, target in testloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)    \n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(20):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "    # average test loss\n",
    "    test_loss = test_loss/len(modelloader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "              classes[i], 100 * class_correct[i] / class_total[i],\n",
    "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "    avg += (100. * np.sum(class_correct) / np.sum(class_total))\n",
    "    \"\"\"\n",
    "    arr = np.array([])\n",
    "    for name, param in model.named_parameters():\n",
    "        temp = param.cpu().detach().numpy()\n",
    "        bruh = temp.flatten()\n",
    "        arr =np.concatenate((arr,bruh),axis = 0)\n",
    "    arr =np.reshape(arr, (1,-1))\n",
    "\n",
    "    print(arr.shape)\n",
    "    if matrix.size == 0:\n",
    "        matrix = arr\n",
    "    else: \n",
    "        matrix = np.append(matrix, arr, axis = 0)\n",
    "    print(count)\n",
    "    torch.save(model.state_dict(), \"test\"+str(count))\n",
    "    count = count +1\n",
    "    \"\"\"\n",
    "    if params_all == []:\n",
    "        params_all = params_trial\n",
    "        params_all = np.reshape(params_all,(1, params_all.shape[0], params_all.shape[1]))  \n",
    "    else:\n",
    "        params_trial = np.reshape(params_trial,(1, params_trial.shape[0], params_trial.shape[1]))\n",
    "        params_all = np.append(params_all,params_trial, axis = 0)\n",
    "  #  print(np.shape(params_all))\n",
    "   # params_all.append(params_trial)\n",
    "print(avg/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "383c799d-45af-4293-abc0-aef7d5b91414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-10T05:00:28.451354Z",
     "iopub.status.busy": "2023-06-10T05:00:28.451080Z",
     "iopub.status.idle": "2023-06-10T05:00:29.412444Z",
     "shell.execute_reply": "2023-06-10T05:00:29.411544Z",
     "shell.execute_reply.started": "2023-06-10T05:00:28.451308Z"
    }
   },
   "outputs": [],
   "source": [
    "#params_all = [torch.stack(trial) for trial in params_all]\n",
    "#params_all =torch.FloatTensor(params_all)\n",
    "#params_all = np.asarray(params_all)\n",
    "params = torch.from_numpy(params_all)\n",
    "#params = torch.stack(params_all) # Dimensions (in order) correspond to: parameters, time steps, and separarte trials\n",
    "torch.save(params, \"50epoch0.5clip1.pt\")\n",
    "#torch.save(params, f\"{100}EpochsDivergence{5}Trials1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26adbeb5-6d80-48fd-b3f7-6174aca1887e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T02:25:58.004343Z",
     "iopub.status.busy": "2023-06-30T02:25:58.003912Z",
     "iopub.status.idle": "2023-06-30T02:26:02.792853Z",
     "shell.execute_reply": "2023-06-30T02:26:02.792005Z",
     "shell.execute_reply.started": "2023-06-30T02:25:58.004315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5833)\n",
      "tensor(1.0417)\n",
      "tensor(1.4429)\n",
      "tensor(1.8305)\n",
      "tensor(2.2076)\n",
      "tensor(2.5595)\n",
      "tensor(2.9114)\n",
      "tensor(3.2523)\n",
      "tensor(3.5853)\n",
      "tensor(3.8926)\n",
      "tensor(4.1904)\n",
      "tensor(4.4647)\n",
      "tensor(4.7434)\n",
      "tensor(4.9989)\n",
      "tensor(5.2456)\n",
      "tensor(5.4869)\n",
      "tensor(5.7013)\n",
      "tensor(5.9229)\n",
      "tensor(6.1303)\n",
      "tensor(6.3253)\n",
      "tensor(6.5080)\n",
      "tensor(6.6707)\n",
      "tensor(6.8243)\n",
      "tensor(6.9589)\n",
      "tensor(7.0989)\n",
      "tensor(7.2146)\n",
      "tensor(7.3327)\n",
      "tensor(7.4316)\n",
      "tensor(7.5153)\n",
      "tensor(7.5941)\n",
      "tensor(7.6648)\n",
      "tensor(7.7213)\n",
      "tensor(7.7924)\n",
      "tensor(7.8402)\n",
      "tensor(7.8795)\n",
      "tensor(7.9136)\n",
      "tensor(7.9324)\n",
      "tensor(7.9570)\n",
      "tensor(7.9760)\n",
      "tensor(7.9752)\n",
      "tensor(7.9758)\n",
      "tensor(7.9770)\n",
      "tensor(7.9823)\n",
      "tensor(7.9727)\n",
      "tensor(7.9551)\n",
      "tensor(7.9339)\n",
      "tensor(7.9177)\n",
      "tensor(7.9041)\n",
      "tensor(7.8728)\n",
      "tensor(7.8496)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fce80c5b6d0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9PElEQVR4nO3deXwU9eHG8Wd3k90k5ISEhIRACKdcAQLEUFGrKYEqhaoVLQpiW1urVhutSlvBqw0eVapQsbYePRTE/jyrqESDF4eE+w5ngJALyLUh1+78/ghGUzmSkGR2N5/367WvbGZnh2fHJPv43e/MWAzDMAQAAODBrGYHAAAAOBsKCwAA8HgUFgAA4PEoLAAAwONRWAAAgMejsAAAAI9HYQEAAB6PwgIAADyen9kB2oLb7VZ+fr5CQkJksVjMjgMAAJrBMAxVVFQoNjZWVuuZx1B8orDk5+crPj7e7BgAAKAVDh48qJ49e55xHZ8oLCEhIZIaXnBoaKjJaQAAQHOUl5crPj6+8X38THyisHz1MVBoaCiFBQAAL9Oc6RxMugUAAB6PwgIAADwehQUAAHg8CgsAAPB4FBYAAODxKCwAAMDjUVgAAIDHo7AAAACPR2EBAAAej8ICAAA8HoUFAAB4PAoLAADweD5x8UMAgHdzuw2VnqhTcUWNSiprVOdyy2KxyCLJYpEsspz82nChvK+ulWcYkiFDMk5+/81lJ++7DaNxmdvdsE7DsoblLsNQbb1bdS63auvdqql3q871jWWuhq+B/jZ1cfgpyN7wtYvdT0EOW8PXk8sigvwVFujfrIv5oWUoLACAdlV2ok6Hj5/Q4dITyi89oeKKmoZbZU3j/ZLKGtW7DbOjtgk/q0Xdgu3q1sWhbsF2RQY7FBlsV7dgh7p1sSsqxKGYsADFhAZQblqAwgIAaDVnTX1j4Sgor24sJt/8WlFT3+ztRQT5KzLYIX+b9eRoydcjJYa+GilpWP7VG/03R2F08n7D14Y7VotkPTkq89WojfXk/a++Ovysstus8rdZZfdruPnbrA3L/ayyWiyqrnOpqrZezlqXqmpOfq2tV1WNS87aejlrXKqsqVe921BheY0Ky2vO+noD/K2KDg1QdGhDgYkJa7jfPcShrl3sjbeIILvsfp17FgeFBQBwSidqXdpTXKldhRU6dLzpyEjJydGRqlpXs7bVtYtdceGBig1veEOOCnYoKqTprVsXh9e/KdfUu3TMWaujlbUqqaxRSWWtjlbW6Kjz6++LyqtVWF6t41V1qq5z68DRKh04WnXWbYc4/BTxPyUmLLDhI6jwkx9FhQX6K/R/lvnbvHuffoXCAgCdXHVdQzHJLWwoJ7sKK5VbVKG8Y1UymvEpTaC/Td1DHYoKdiguIlBx4YGNX3tGBCo2PFBB9s7xduPws6lHWKB6hAWedd3qOpeKyhtGpgrKq1VYVt14v7iiRsedtTpeVatjzlq5Damipl4VNfXKO3b2cvNNceGBSozqor5Rwep78mtiVLCiQx1e9XGUxTCa8+Po2crLyxUWFqaysjKFhoaaHQcAPFZpVa225pdry+Eybckv19b8Mu0vcep000cigvw1IDpEfSK7qHuIQ5EhjiajI5HBDnVxdI4yYha321B5dZ2OOWub3qpqVXaiTuUn6lR28lZa9fX9iuozfxTXxW5T4skSE981SD3CGkbAYsMD1SMsQCEB/u3+2lry/s1PGQD4qOKKmoZicrisoaTkl+nQ8ROnXDcs0F8DooPVPzpEA7oHa0B0iPpHhygy2O5V/xfui6xWi8KD7AoPsisxqvnPc7kNHXPWav9Rp/YWV2pP8ddf845VyVnr0ubDZdp8uOyUzw8J8FNsWKB6nCwxsWEB+tmFiXL42drolbUMhQUAfIBhGNpb4tSX+47py/3HtfbAsdPOi+jVNUhD40I1JDZMQ+PCdF5MiKJCvOvjAZydzWppHAkbk9C1yWO19W7lHXOeLDFOHS6t0pHSauWXVSu/9ETjCM3O6grtLKyQJNltVv3y4n5mvBRJFBYA8Ep1Lre25pdr7f5jWrPvmNYeOK5jztom61gsUr+oYA2NC9OQ2IaCMjg2VGGB7T/UD89m97OqX/cQ9esecsrHnTX1OnKyvBwpO6H80mpV17lktZpXaiksAOAlaupdWrGzWG9uzNdH24t0oq7pEToOP6tGxIdrTEJXjU6I0KjeEQrtgHkI8D1dHH7q1z1Y/boHmx2lEYUFADyYy21o9b6jemtDvt7dfETl35hIGRborzEJERqd0FVjErpqaFyoafMLgPZGYQEAD2MYhjYfLtObG/L19sZ8FVV8fQKy6FCHJg+P1Q9GxGpobJipQ/RAR6KwAIAHqKl3KWf/ca3ILdYHWwu1r8TZ+FhogJ8uG95DP0iK09g+XWWjpKATorAAgAkMw9D+o1X6ZFexPtlVrJV7jzY5a2yAv1Vp50Vryog4XTggko960OlRWACggzhr6vX57hJ9klusT3aVfOuMpZHBDl04IFIXDYhS2nnRnJAN+AZ+GwCgHdXUu5S9s1hvbcxX1vZCVde5Gx/zt1mU3DtCFw3orgsHROq8mFDmpACnQWEBgDbmchtaueeo3tp4WO9tKWhyivT4roG6eEB3XTQgSuf37aZgRlGAZuE3BQDagGEYWn+wVG9tyNc7m46opPLrI3tiQgN0+fAemjIiTkPjQjmjLNAKFBYAOAcFZdV6LeegluYcanIq/PAgf00a2kNTRsRqbEJXPuoBzhGFBQBaqM7l1kc7irTky4PK3lnUeKXjILtNEwZH6wcjYnVBvyjZ/azmBgV8CIUFAJppb3Gllqw9qP/kHG7ykc/YhK66eky8vj8sRkF2/qwC7YHfLAA4g+o6l/676YiWfHlQa/Yfa1weGezQlclxunp0vPpGec71VgBfRWEBgFPYX+LUv1cf0KtrD6nsRJ0kyWqRvjuwu64eE69LBnWXv42PfICOQmEBgJPqT85N+eeqA/o0t6RxeVx4oK4dG6+rkuMVExZgYkKg86KwAOj0iiqqtWTNQb2yJk/5ZdWSJItFunhAlK5P7a2LBnTn+j2AySgsADqtTYdK9ddP9mrZlgLVnzzUJyLIX9PG9NL0lF6K7xpkckIAX6GwAOh0cg4c01NZu7ViV3HjslG9wnV9am9NGtpDAf5caBDwNBQWAJ2CYRhatfeYnsrK1cq9RyVJNqtFU5JideMFfTQ0LszkhADOhMICwKcZhqFPc0v09Ee5+nL/cUmSn9Wiq5J76uaL+6p3ty4mJwTQHBQWAD7JMAx9tKNIT320WxsPlkqS7Darpo2J1y8u7qu48EBzAwJoEQoLAJ+z8WCp7n97q9bnlUqSAvyt+vHY3vr5RYmKDuWwZMAbUVgA+Iyiimo9tmynluYcktRwbZ/rU3vrpxckKirEYXI6AOeCwgLA69XWu/XiF/v0VNZuVdbUS5KuGBWneyYOYkQF8BEUFgBe7eMdRXronW3aW+KUJCX1DNPcHwzRqF4RJicD0JYoLAC80r4Spx56Z5s+2lEkqeFihHdPHKirRvWUlbPSAj6HwgLAq1RU12nBR7v1/Of7VOcy5Ge16MYL+ui2S/opJMDf7HgA2gmFBYBXcLkNvbr2oP70wU6VVNZKki4eGKX7Lh+svlHBJqcD0N4oLAA83hd7SvTg29u0o6BCkpQY1UW/v+w8XTIo2uRkADoKhQWAx9pf4tQf392uD7YVSpJCA/x0R9oAXZ/aW/42q8npAHSkVv3GL1y4UAkJCQoICFBKSorWrFlz2nWfe+45jR8/XhEREYqIiFBaWtq31r/hhhtksVia3CZOnNiaaAB8QHl1nTLf3a7vPblCH2wrlM1q0czU3lrxm+/qxgv6UFaATqjFIyxLlixRRkaGFi1apJSUFM2fP1/p6enauXOnunfv/q31s7Ozde2112rcuHEKCAjQI488ogkTJmjr1q2Ki4trXG/ixIl64YUXGr93ODjJE9DZuE/OU3ns/Z066myYp3LhgCjdd9l56h8dYnI6AGayGIZhtOQJKSkpGjNmjBYsWCBJcrvdio+P12233aZ77733rM93uVyKiIjQggULNGPGDEkNIyylpaV64403Wv4KJJWXlyssLExlZWUKDQ1t1TYAmKugrFq/eW2jPs0tkST1jeqi318+WN8d+O3/EQLgG1ry/t2iEZba2lrl5ORo9uzZjcusVqvS0tK0cuXKZm2jqqpKdXV16tq1a5Pl2dnZ6t69uyIiInTJJZfo4YcfVrdu3U65jZqaGtXU1DR+X15e3pKXAcDDvLnhsO57Y4vKq+sV4G/VXRMGaua4BD76AdCoRYWlpKRELpdL0dFNZ+ZHR0drx44dzdrGPffco9jYWKWlpTUumzhxoq644gr16dNHe/bs0W9/+1tNmjRJK1eulM1m+9Y2MjMz9cADD7QkOgAPVFpVq9+/sUXvbDoiqeEstU9MG8FhygC+pUOPEpo3b54WL16s7OxsBQR8fX2Pa665pvH+sGHDNHz4cPXt21fZ2dm69NJLv7Wd2bNnKyMjo/H78vJyxcfHt294AG1qxa5i3f3aRhWW18hmtei2S/rplu/2Y1QFwCm1qLBERkbKZrOpsLCwyfLCwkLFxMSc8bmPP/645s2bp+XLl2v48OFnXDcxMVGRkZHavXv3KQuLw+FgUi7gpapq65X57g79c9UBSQ3nVHny6hFKig83NxgAj9ai/5Wx2+1KTk5WVlZW4zK3262srCylpqae9nmPPvqoHnroIS1btkyjR48+679z6NAhHT16VD169GhJPAAebn3ecV321GeNZeWGcQn6723jKSsAzqrFHwllZGRo5syZGj16tMaOHav58+fL6XRq1qxZkqQZM2YoLi5OmZmZkqRHHnlEc+bM0csvv6yEhAQVFBRIkoKDgxUcHKzKyko98MADuvLKKxUTE6M9e/bo7rvvVr9+/ZSent6GLxWAWWrqXfrz8lw9+8leudyGYkID9NiPhmt8/yizowHwEi0uLNOmTVNxcbHmzJmjgoICjRgxQsuWLWuciJuXlyer9euBm2eeeUa1tbW66qqrmmxn7ty5uv/++2Wz2bRp0ya99NJLKi0tVWxsrCZMmKCHHnqIj30AH7DhYKl+s3SjcosqJUlTRsTqwR8MVVgQFyoE0HwtPg+LJ+I8LIDnqa5zaf7yXP31kz1yG1JksEMPTx2qiUPPPN8NQOfRbudhAYDmWJd3XL9ZulF7ip2SGkZV7p88RBFd7CYnA+CtKCwA2kx1nUtPfrhLz326V25Digpx6A9Th2rCEEZVAJwbCguANpFz4Lh+89pG7T05qnLFyDjNmTxY4UGMqgA4dxQWAOek3uXWn7NyteDj3TIMqXuIQ3/84TClDY4++5MBoJkoLABarai8Wr9avF6r9h6TJF0xKk5zLx/CEUAA2hyFBUCrfLGnRL96ZYNKKmsUZLcp84phmjIizuxYAHwUhQVAi7jdhv6SvVtPfLhLbkMaGB2ihdNHqV93LlgIoP1QWAA02zFnre5YskGf7CqWJP0ouacenDJUgfZvX1UdANoShQVAs6zdf0y3vrxeBeXVCvC36sEpQ3X1aK6SDqBjUFgAnJFhGHru0716ZNlOudyGEqO66C/TR2lQDGeVBtBxKCwATquypl53vrpB728tlCRNTopV5hXDFOzgTweAjsVfHQCntLe4Ujf9M0e7iyplt1k1Z/JgTU/pJYvFYnY0AJ0QhQXAt3y8o0i/WrxeFdX1ig51aNF1yRrZK8LsWAA6MQoLgEaGYegv2Xv0+Ac7ZRhScu8IPXPdKHUPCTA7GoBOjsICQJLkrKnXXUs36r0tBZKk6Sm9NHfyENn9rCYnAwAKCwBJB446ddM/crSzsEL+NosenDJU147tZXYsAGhEYQE6uRW7inXby+tUXl2vqBCHFl03Ssm9u5odCwCaoLAAndRX51eZ994OuQ1pZK9wLbouWdGhzFcB4HkoLEAn5HIbevDtrXpp5QFJ0jVj4vXAlCFy+HGKfQCeicICdDI19S5lLNmo/24+Ikm67/LBuvE7CZxfBYBHo7AAnUh5dZ1+/o8crdx7VP42i/509Qj9ICnW7FgAcFYUFqCTKCqv1swXvtT2I+UKdvjp2euT9Z1+kWbHAoBmobAAncDe4krNeH6NDh0/ochgh16cNUZD48LMjgUAzUZhAXzcxoOlmvXilzrmrFXvbkH6540p6tUtyOxYANAiFBbAh2XvLNLN/1qnE3UuDYsL0wuzxigy2GF2LABoMQoL4KP+b90h3f3aJtW7DY3vH6lnrktWsINfeQDeib9egA/6x8r9mvPmVknSlBGxeuyqJK4JBMCrUVgAH/PXT/boj+/ukCTN+k6C7rtssKxWzrECwLtRWAAfYRiGnsrarSeX75Ik3frdfrpzwgBOCAfAJ1BYAB9gGIYefX+nnsneI0m6a8IA3XpJf5NTAUDbobAAXs4wDD3w9ja9+MV+SdLvLztPPx2faG4oAGhjFBbAi7ndhn73xma9suagJOnhqUN13fm9TU4FAG2PwgJ4qXqXW3e/tkn/t/6wrBbp0auSdFVyT7NjAUC7oLAAXqi23q07lqzXu5sLZLNaNH/aCE3mIoYAfBiFBfAy1XUu3fryOi3fXiS7zaoFPx6pCUNizI4FAO2KwgJ4kdp6t37573X6aEeRHH5WPXt9si4e2N3sWADQ7igsgJeoc7l168sNZSXA36rnZ47RuH6RZscCgA7BuboBL1DvcuuOxRv0wbZC2f2sem7GaMoKgE6FwgJ4OJfb0J1LN+q/m4/Ibmv4GGh8/yizYwFAh6KwAB7M7TZ092ub9OaGfPlZLVo4fZS+y5wVAJ0QhQXwUG63od++vln/WXdINqtFT187Ut8bHG12LAAwBYUF8ECGYWjOW1u0+MuDslqk+dNGaNKwHmbHAgDTUFgAD2MYhh58Z5v+tSpPFov0p6uTOCkcgE6PwgJ4EMMwlPneDr3w+X5J0iNXDtcPR3K6fQCgsAAeZP7yXP31k72SpD/+cJiuHh1vciIA8AwUFsBDvPTFfv05K1eS9MAPhujHKb1MTgQAnoPCAniAtzbm6/63t0qSMr43QDPHJZgbCAA8DIUFMNknu4p156sbZBjSzNTeuu2SfmZHAgCPQ2EBTLThYKl+8a8c1bkMTU6K1dzJQ2SxWMyOBQAeh8ICmGR3UaVmvbBGVbUuje8fqT/9KElWK2UFAE6FwgKY4EjZCc34+2odr6pTUny4Fl2XLLsfv44AcDr8hQQ62HFnra7/+xrll1UrMaqLXrhhjLo4/MyOBQAejcICdKCq2nrd+NKX2l1UqZjQAP3zJynq2sVudiwA8HitKiwLFy5UQkKCAgIClJKSojVr1px23eeee07jx49XRESEIiIilJaW9q31DcPQnDlz1KNHDwUGBiotLU25ubmtiQZ4rDqXWzf/a53W55UqLNBf//zJWMWFB5odCwC8QosLy5IlS5SRkaG5c+dq3bp1SkpKUnp6uoqKik65fnZ2tq699lp9/PHHWrlypeLj4zVhwgQdPny4cZ1HH31UTz31lBYtWqTVq1erS5cuSk9PV3V1detfGeBB3G5Ddy3dqBW7ihXob9PzN4xR/+gQs2MBgNewGIZhtOQJKSkpGjNmjBYsWCBJcrvdio+P12233aZ77733rM93uVyKiIjQggULNGPGDBmGodjYWN1555266667JEllZWWKjo7Wiy++qGuuueas2ywvL1dYWJjKysoUGhrakpcDtDvDMPS7N7bo5dV58rNa9NzM0fruwO5mxwIA07Xk/btFIyy1tbXKyclRWlra1xuwWpWWlqaVK1c2axtVVVWqq6tT165dJUn79u1TQUFBk22GhYUpJSXltNusqalReXl5kxvgiQzD0B/f3a6XV+fJapHmXzOCsgIArdCiwlJSUiKXy6Xo6Ogmy6Ojo1VQUNCsbdxzzz2KjY1tLChfPa8l28zMzFRYWFjjLT6eC8TBMz390W499+k+SdK8K4br8uGxJicCAO/UoUcJzZs3T4sXL9brr7+ugICAVm9n9uzZKisra7wdPHiwDVMCbePvn+3TEx/ukiTdd/lgXT2GYg0ArdWikz9ERkbKZrOpsLCwyfLCwkLFxMSc8bmPP/645s2bp+XLl2v48OGNy796XmFhoXr06NFkmyNGjDjlthwOhxwOR0uiAx1qyZd5euidbZIaLmb4kwv6mJwIALxbi0ZY7Ha7kpOTlZWV1bjM7XYrKytLqampp33eo48+qoceekjLli3T6NGjmzzWp08fxcTENNlmeXm5Vq9efcZtAp7q7Y35uvf/NkuSbrowkYsZAkAbaPHpNTMyMjRz5kyNHj1aY8eO1fz58+V0OjVr1ixJ0owZMxQXF6fMzExJ0iOPPKI5c+bo5ZdfVkJCQuO8lODgYAUHB8tiseiOO+7Qww8/rP79+6tPnz667777FBsbq6lTp7bdKwU6wEc7CvXrJQ1XXr52bC/NnjSIixkCQBtocWGZNm2aiouLNWfOHBUUFGjEiBFatmxZ46TZvLw8Wa1fD9w888wzqq2t1VVXXdVkO3PnztX9998vSbr77rvldDp10003qbS0VBdccIGWLVt2TvNcgI72xZ4S/eJf61TvNjRlRKwenjqUsgIAbaTF52HxRJyHBWZbn3dc1/1ttZy1LqWdF61nrhslfxtXvgCAM2m387AA+Lb9JU7NevFLOWtd+k6/blrw45GUFQBoY/xVBc5BWVWdbnzpS5VW1SmpZ5j+ev1oBfjbzI4FAD6HwgK0Up3LrVteXqe9xU71CAvQczNGq4ujxdPCAADNQGEBWunBt7fps90lCrLb9LeZo9U9lEniANBeKCxAK7z0xX79c9UBWSzS/GkjNCQ2zOxIAODTKCxAC63YVawH3t4qSbpn4iBNGHLmszwDAM4dhQVogdzCCt3673VyG9KPknvq5xcmmh0JADoFCgvQTMectfrJS2tVUVOvsQld9YcfDuPEcADQQSgsQDPU1Lv0i3/mKO9YlXp1DdKi65Nl9+PXBwA6Cn9xgbMwDEO/e32L1uw/phCHn/4+c7S6drGbHQsAOhUKC3AWz36yV6/lHJLVIi2YPkr9o0PMjgQAnQ6FBTiD5dsK9ciyHZKkuZOH6KIBUSYnAoDOicICnMauwgrdvni9DEO67vxemjkuwexIANBpUViAUzjurNVPX1orZ61L5yd21dzJQ8yOBACdGoUF+B91Lrd++e91yjtWpfiugfrL9GSuvgwAJuOvMPA/Hnpnm1buPaoudpv+NmMMRwQBgAegsADf8O/VB/SPlSevEXTNSA2M4YggAPAEFBbgpNV7j2rumw3XCLprwkB9b3C0yYkAAF+hsACSDh6r0s3/Xqd6t6HJSbH65cV9zY4EAPgGCgs6PWdNvX72j7U65qzVsLgwPXrlcK4RBAAehsKCTs3tNpTx6gbtKKhQZLBDf52RrEC7zexYAID/QWFBpzY/K1fvby2U3WbVs9cnq0dYoNmRAACnQGFBp/Xe5iN6KitXkvTHK4YpuXeEyYkAAKdDYUGntKuwQncu3ShJ+ukFfXRVck+TEwEAzoTCgk6n7ESdfv7PHFXVujSubzfdO2mQ2ZEAAGdBYUGn4nYbyliyQftKnIoLD9TT146UH6fdBwCPx19qdCpPfZSrrB1FsvtZtei6ZHULdpgdCQDQDBQWdBpZ2ws1f3nDJNs/TB2qYT3DTE4EAGguCgs6hX0lTt2xZIMk6frze+tHo+PNDQQAaBEKC3yes6ZeN/1jrSqq6zW6d4Tuu3yw2ZEAAC1EYYFPMwxDv3lto3KLKtU9xKG/TB8lux8/9gDgbfjLDZ/27Cd79e7mAvnbLHrmulHqHhpgdiQAQCtQWOCzPs0t1qPLdkiS5k4eouTeXU1OBABoLQoLfNLBY1W67ZX1chvS1aN7anpKL7MjAQDOAYUFPqfO5datr6xXaVWdknqG6cEpQ2WxWMyOBQA4BxQW+JwnPtyljQdLFRrgp4XTRynA32Z2JADAOaKwwKd8lluiRSv2SJIeuXK4ekYEmZwIANAWKCzwGSWVNfr1qxtkGNKPU3pp0rAeZkcCALQRCgt8gttt6K6lG1VcUaP+3YN132WcHA4AfAmFBT7hhS/2K3tnsRx+Vj3945EKtDNvBQB8CYUFXm/L4TLNe2+7JOn3lw/WoJhQkxMBANoahQVezVlTr1+9sl51LkMTBkfrOs63AgA+icICrzb3ra3aW+JUj7AAPXrVcM63AgA+isICr/XmhsN6LeeQrBbpyWkjFB5kNzsSAKCdUFjglfKOVul3r2+RJN16SX+dn9jN5EQAgPZEYYHXqXO5ddvi9aqsqdfo3hH61SX9zI4EAGhnFBZ4nT998PWp9+dfM0J+Nn6MAcDX8ZceXmXFrmJOvQ8AnRCFBV6jqLxaGUs2SJKmc+p9AOhUKCzwCi63oTuWbNBRZ60GxYTovss59T4AdCYUFniFv3y8W1/sOapAf5sW/HiUAvw59T4AdCYUFni8NfuO6cnluyRJD00dqn7dg01OBADoaBQWeLTjzlrdvni93IZ0xcg4XZXc0+xIAAATtKqwLFy4UAkJCQoICFBKSorWrFlz2nW3bt2qK6+8UgkJCbJYLJo/f/631rn//vtlsVia3AYNGtSaaPAhhmHorqUbdaSsWomRXfTQ1KFmRwIAmKTFhWXJkiXKyMjQ3LlztW7dOiUlJSk9PV1FRUWnXL+qqkqJiYmaN2+eYmJiTrvdIUOG6MiRI423zz77rKXR4GOe/3y/snYUye5n1dM/HqkuDj+zIwEATNLiwvLEE0/oZz/7mWbNmqXBgwdr0aJFCgoK0vPPP3/K9ceMGaPHHntM11xzjRwOx2m36+fnp5iYmMZbZGRkS6PBh2w6VKp5722XJP3+svM0JDbM5EQAADO1qLDU1tYqJydHaWlpX2/AalVaWppWrlx5TkFyc3MVGxurxMRETZ8+XXl5eaddt6amRuXl5U1u8B0V1XW67ZX1qnMZSh8SrevP7212JACAyVpUWEpKSuRyuRQdHd1keXR0tAoKClodIiUlRS+++KKWLVumZ555Rvv27dP48eNVUVFxyvUzMzMVFhbWeIuPj2/1vw3PYhiGfvv6Fh04WqW48EA9emWSLBaL2bEAACbziKOEJk2apB/96EcaPny40tPT9e6776q0tFSvvvrqKdefPXu2ysrKGm8HDx7s4MRoL0u+PKi3N+bLZrXoqWtHKizI3+xIAAAP0KJZjJGRkbLZbCosLGyyvLCw8IwTalsqPDxcAwYM0O7du0/5uMPhOON8GHinfSVO3f/2VknSXRMGKrl3hMmJAACeokUjLHa7XcnJycrKympc5na7lZWVpdTU1DYLVVlZqT179qhHD64V01m43IbufHWDquvcGte3m35+YaLZkQAAHqTFx4lmZGRo5syZGj16tMaOHav58+fL6XRq1qxZkqQZM2YoLi5OmZmZkhom6m7btq3x/uHDh7VhwwYFBwerX79+kqS77rpLkydPVu/evZWfn6+5c+fKZrPp2muvbavXCQ/310/2al1eqUIcfnrsR0myWpm3AgD4WosLy7Rp01RcXKw5c+aooKBAI0aM0LJlyxon4ubl5clq/XrgJj8/XyNHjmz8/vHHH9fjjz+uiy66SNnZ2ZKkQ4cO6dprr9XRo0cVFRWlCy64QKtWrVJUVNQ5vjx4gx0F5Xryw4ZT78+ZPFhx4YEmJwIAeBqLYRiG2SHOVXl5ucLCwlRWVqbQ0FCz46AFauvd+uFfPtfW/HKlndddz80YzVFBANBJtOT92yOOEkLnteCjXG3NL1d4kL/+eMUwygoA4JQoLDDNxoOlWpi9R5L08NSh6h4SYHIiAICnorDAFNV1Lt25dKNcbkOTk2J1+fBYsyMBADwYhQWm+NMHO7W7qFJRIQ49+IMhZscBAHg4Cgs63Jp9x/S3z/ZJkuZdMUwRXewmJwIAeDoKCzqUs6Zedy3dKMOQrh7dU5eeF332JwEAOj0KCzrUH9/drrxjDRc2vO/ywWbHAQB4CQoLOsyKXcX69+o8SdJjVw1XSAAXNgQANA+FBR2i7ESd7nltkyTphnEJGtcv0uREAABvQmFBh/jDf7epoLxaCd2CdM/EQWbHAQB4GQoL2t2KXcV6de0hWSzSYz9KUqDdZnYkAICXobCgXVVU12n2fxo+CpqZmqAxCV1NTgQA8EYUFrSrzPd2KL+sWvFdA3X3xIFmxwEAeCkKC9rNF7tL9PLJo4IeuXK4gux+JicCAHgrCgvahbOmXvf8X8NHQdNTemlcX44KAgC0HoUF7eKx93fq4LETigsP1Ozvn2d2HACAl6OwoM2t2XdML36xX5KUecUwBTv4KAgAcG4oLGhTJ2pduvu1jZKkaaPjdeGAKJMTAQB8AYUFbepPH+zU/qNVig516LeX8VEQAKBtUFjQZnIOHNffP98nqeGjoLBArhUEAGgbFBa0ieq6ho+CDEO6YmScLhkUbXYkAIAPobCgTfw5K1d7ip2KCnFozuTBZscBAPgYCgvO2eZDZfrrJ3slSQ9PHarwILvJiQAAvobCgnPichv63Rub5XIbumx4D6UPiTE7EgDAB1FYcE5eXpOnTYfKFOLw01w+CgIAtBMKC1qtpLJGjy3bIUm6c8IAdQ8JMDkRAMBXUVjQapnv7lB5db2GxIbquvN7mx0HAODDKCxoldV7j+o/6w7JYmmYaOtn40cJANB+eJdBi9W53LrvzS2SpGvG9NLIXhEmJwIA+DoKC1rsxc/3a1dhpbp2sevu9IFmxwEAdAIUFrTIkbITenL5LknSvZMGKaIL51wBALQ/Cgta5KF3tqmq1qXk3hG6alRPs+MAADoJCguabcWuYr27uUA2q0UPTx0qq9VidiQAQCdBYUGzVNe5NPfkRNsbxiXovB6hJicCAHQmFBY0y7Mr9mr/0SpFhzp0R1p/s+MAADoZCgvO6sBRpxZm75Yk3Xf5YIUE+JucCADQ2VBYcEaGYWjOm1tVW+/W+P6RumxYD7MjAQA6IQoLzuj9rQVasatYdptVD/xgiCwWJtoCADoehQWndaLWpYfe2S5JuunCRCVGBZucCADQWVFYcFp/yd6tw6UnFBceqFu+28/sOACATozCglPaX+LUsyv2SmqYaBtot5mcCADQmVFYcEoPvbNNta6GibbpQ6LNjgMA6OQoLPiWrO2FytpRJH+bRfcz0RYA4AEoLGiius6lB9/ZJkm68YI+6stEWwCAB6CwoInnPtmrAyfPaPurSzijLQDAM1BY0OjQ8arGM9r+7rLB6uLwMzkRAAANKCxo9If/bld1nVspfbpq8nDOaAsA8BwUFkiSPs0t1ntbCmSzWvTAFCbaAgA8C4UFqq136/63tkqSZqT21qCYUJMTAQDQFIUFevGLfdpT7FRksF13pA0wOw4AAN9CYenkCsur9efluZKkeyedp7BAf5MTAQDwbRSWTu6P726Xs9alUb3CdcXIOLPjAABwShSWTmz13qN6c0O+LBbpwSlDZbUy0RYA4JlaVVgWLlyohIQEBQQEKCUlRWvWrDntulu3btWVV16phIQEWSwWzZ8//5y3iXNX73Jr7smJtj8e20tD48JMTgQAwOm1uLAsWbJEGRkZmjt3rtatW6ekpCSlp6erqKjolOtXVVUpMTFR8+bNU0xMTJtsE+du8ZcHtaOgQmGB/rprwkCz4wAAcEYtLixPPPGEfvazn2nWrFkaPHiwFi1apKCgID3//POnXH/MmDF67LHHdM0118jhcLTJNnFuyk7U6YkPd0mSfp3WXxFd7CYnAgDgzFpUWGpra5WTk6O0tLSvN2C1Ki0tTStXrmxVgPbYJs7sqaxcHXPWqn/3YE0/v7fZcQAAOKsWXSympKRELpdL0dHRTZZHR0drx44drQrQmm3W1NSopqam8fvy8vJW/dud0e6iSr30xX5J0n2XD5a/jXnXAADP55XvVpmZmQoLC2u8xcfHmx3Ja/zhv9tU7zZ06aDuunBAlNlxAABolhYVlsjISNlsNhUWFjZZXlhYeNoJte2xzdmzZ6usrKzxdvDgwVb9253NxzuL9PHOYvnbLPrdZeeZHQcAgGZrUWGx2+1KTk5WVlZW4zK3262srCylpqa2KkBrtulwOBQaGtrkhjOrc7n18DvbJEk3jEtQYlSwyYkAAGi+Fs1hkaSMjAzNnDlTo0eP1tixYzV//nw5nU7NmjVLkjRjxgzFxcUpMzNTUsOk2m3btjXeP3z4sDZs2KDg4GD169evWdvEufvnygPaU+xUty523XZpf7PjAADQIi0uLNOmTVNxcbHmzJmjgoICjRgxQsuWLWucNJuXlyer9euBm/z8fI0cObLx+8cff1yPP/64LrroImVnZzdrmzg3x5y1mr+84TDmu9IHKjSA6wUBALyLxTAMw+wQ56q8vFxhYWEqKyvj46FT+P0bm/WvVXk6r0eo3rntAtk4BT8AwAO05P3bK48SQvNtP1Kul1fnSZLmTh5MWQEAeCUKiw8zDEMPvr1NbkP6/rAYnZ/YzexIAAC0CoXFh72/tVAr9x6V3c+q2ZM4jBkA4L0oLD6qpt6lP767XZJ00/hExXcNMjkRAACtR2HxUc9/tl95x6rUPcShmy/ua3YcAADOCYXFBxVVVGvBR7mSpHsnDVIXR4uPXgcAwKNQWHzQY8t2ylnrUlJ8uKaOiDM7DgAA54zC4mM2HSrV0pxDkhoOY7ZyGDMAwAdQWHzIV4cxS9IPR8ZpVK8IkxMBANA2KCw+5O1NR7T2wHEF+tt0z8RBZscBAKDNUFh8xIlalzJPHsb8y4v7KiYswOREAAC0HQqLj3j2kz06UlatuPBA/ezCRLPjAADQpigsPiC/9IQWrdgjSZr9/UEK8LeZnAgAgLZFYfEB897boeo6t8YmdNVlw3qYHQcAgDZHYfFya/cf01sb82WxSHMmD5bFwmHMAADfQ2HxYm63oQdOHsY8bXS8hsaFmZwIAID2QWHxYv9Zd0ibD5cp2OGnOycMNDsOAADthsLipSpr6vXo+zslSb+6tJ+iQhwmJwIAoP1QWLzUwo93q7iiRgndgnTDuD5mxwEAoF1RWLxQ3tEq/f3TfZKk3102WHY//jMCAHwb73Re6A/vblOty63x/SOVdl53s+MAANDuKCxeZvXeo3p/a6FsVovuu5zDmAEAnQOFxYsYhqHM93ZIkq4ZE68B0SEmJwIAoGNQWLzI+1sLteFgqQL9bbo9rb/ZcQAA6DAUFi9R73Lr0fcbRld+Or6PuodwNWYAQOdBYfESr+Uc0t5ipyKC/HUTV2MGAHQyFBYvcKLWpSeX75Ik3XpJf4UE+JucCACAjkVh8QIvfLFPheU1igsP1HXn9zI7DgAAHY7C4uFKq2r1TPYeSdJd6QPk8LOZnAgAgI5HYfFwf8neo4rqeg2KCdGUpDiz4wAAYAoKiwfLLz2hF7/YL0m6Z9IgWa2cJA4A0DlRWDzYkx/uUm29W+cndtXFA6LMjgMAgGkoLB5qV2GF/rPukCTpnomDOAU/AKBTo7B4qEeX7ZTbkCYNjdHIXhFmxwEAwFQUFg/05f5jWr694QKHd6UPNDsOAACmo7B4GMMw9MjJCxxePTpefaOCTU4EAID5KCweZvn2Iq09cFwB/lbdwQUOAQCQRGHxKC63oUeXNYyu3PidPooO5QKHAABIFBaP8n/rDim3qFLhQf76+UV9zY4DAIDHoLB4iJp6l+Yvz5Uk/fLivgoL5AKHAAB8hcLiIRavOajDpScUHerQjNQEs+MAAOBRKCweoKq2Xk9/tFuS9KtL+yvAnwscAgDwTRQWD/DiF/tVUlmjXl2DdPXoeLPjAADgcSgsJis7UadF2XskSb/+Xn/52/hPAgDA/+Ld0WR/+3SvyqvrNSA6WD9IijM7DgAAHonCYqKSyhr9/bN9kqQ7JwyUzcoFDgEAOBUKi4n+8vEeVdW6lNQzTBMGR5sdBwAAj0VhMcnh0hP616oDkqS70gfKYmF0BQCA06GwmOTprFzVutw6P7GrLugXaXYcAAA8GoXFBPtKnFqac0iS9BtGVwAAOCsKiwme/HCXXG5Dlw7qruTeXc2OAwCAx6OwdLBt+eV6a2O+JCljwgCT0wAA4B0oLB3siQ93SpIuH95DQ2LDTE4DAIB3oLB0oJwDx7V8e5FsVosyvsfoCgAAzdWqwrJw4UIlJCQoICBAKSkpWrNmzRnXX7p0qQYNGqSAgAANGzZM7777bpPHb7jhBlkslia3iRMntiaaR3v8/YbRlatG9VRiVLDJaQAA8B4tLixLlixRRkaG5s6dq3Xr1ikpKUnp6ekqKio65fpffPGFrr32Wv3kJz/R+vXrNXXqVE2dOlVbtmxpst7EiRN15MiRxtsrr7zSulfkoT7fXaKVe4/KbrPqV2n9zY4DAIBXsRiGYbTkCSkpKRozZowWLFggSXK73YqPj9dtt92me++991vrT5s2TU6nU++8807jsvPPP18jRozQokWLJDWMsJSWluqNN95o1YsoLy9XWFiYysrKFBoa2qpttCfDMHTFM19ofV6pbhiXoPt/MMTsSAAAmK4l798tGmGpra1VTk6O0tLSvt6A1aq0tDStXLnylM9ZuXJlk/UlKT09/VvrZ2dnq3v37ho4cKBuvvlmHT169LQ5ampqVF5e3uTmyVbsKtb6vFIF+Fv1y+/2NTsOAABep0WFpaSkRC6XS9HRTa97Ex0drYKCglM+p6Cg4KzrT5w4Uf/4xz+UlZWlRx55RCtWrNCkSZPkcrlOuc3MzEyFhYU13uLj41vyMjqUYRh68sNdkqTrz++t7iEBJicCAMD7+JkdQJKuueaaxvvDhg3T8OHD1bdvX2VnZ+vSSy/91vqzZ89WRkZG4/fl5eUeW1o+3lmkjYfKFOhv088vYnQFAIDWaNEIS2RkpGw2mwoLC5ssLywsVExMzCmfExMT06L1JSkxMVGRkZHavXv3KR93OBwKDQ1tcvNEDaMruZKkGam9FRnsMDkRAADeqUWFxW63Kzk5WVlZWY3L3G63srKylJqaesrnpKamNllfkj788MPTri9Jhw4d0tGjR9WjR4+WxPM4y7cXafPhMgXZbbrpwkSz4wAA4LVafFhzRkaGnnvuOb300kvavn27br75ZjmdTs2aNUuSNGPGDM2ePbtx/dtvv13Lli3Tn/70J+3YsUP333+/1q5dq1tvvVWSVFlZqd/85jdatWqV9u/fr6ysLE2ZMkX9+vVTenp6G73MjmcYhuYvb5i7MnNcgroxugIAQKu1eA7LtGnTVFxcrDlz5qigoEAjRozQsmXLGifW5uXlyWr9ugeNGzdOL7/8sn7/+9/rt7/9rfr376833nhDQ4cOlSTZbDZt2rRJL730kkpLSxUbG6sJEybooYceksPhvW/yH2wr1Nb8cnWx23TTeEZXAAA4Fy0+D4sn8rTzsLjdhr7/1KfaUVChW7/bT3elDzQ7EgAAHqfdzsOC5nl/a4F2FFQoxOGnn47vY3YcAAC8HoWljbndhuYvbzgyaNZ3EhQeZDc5EQAA3o/C0sbe3XJEOwsrFBLgp59cwNwVAADaAoWlDbnchv58cnTlJxf0UViQv8mJAADwDRSWNvTfzUeUW1Sp0AA/3XgBc1cAAGgrFJY20jC60nDelZ+NT1RoAKMrAAC0FQpLG3l7Y772FDsVHuSvG76TYHYcAAB8CoWlDdS73Hoqq2Huys/GJyqE0RUAANoUhaUNvLUxX3tLnIoI8tfMcQlmxwEAwOdQWM6R221owccNV5W+6cK+Cna0+GoHAADgLCgs5yh7V5H2FjsVEuCn61N7mx0HAACfRGE5R3/7dJ8k6dqxvRhdAQCgnVBYzsG2/HJ9seeobFYLc1cAAGhHFJZz8PznDaMrE4fGKC480OQ0AAD4LgpLKxVVVOutDfmSGk7DDwAA2g+FpZX+tSpPtS63RvYK16heEWbHAQDAp1FYWqG6zqV/rzogidEVAAA6AoWlFd7ccFhHnbWKCw/UxCExZscBAMDnUVhayDAM/f2zhsm2N4xLkJ+NXQgAQHvj3baFPs0t0a7CSnWx2zRtbLzZcQAA6BQoLC301ejKj0bHK5SLHAIA0CEoLC2wu6hCK3YVy2KRZn0nwew4AAB0GhSWFvj7Z/slSd87L1q9u3UxNwwAAJ0IhaWZjjlr9X/rDkniUGYAADoahaWZ/r3qgGrq3RoaF6qxfbqaHQcAgE6FwtIMNfUu/ePkieJ+ekGiLBaLyYkAAOhcKCzN8M7GIyquqFF0qEPfH9bD7DgAAHQ6FJaz+OaJ4makJsjuxy4DAKCj8e57Fqv2HtO2I+UK8Ldqekovs+MAANApUVjO4qvRlStH9VR4kN3kNAAAdE4UljPYV+JU1o5CSdKNHMoMAIBp/MwO4Ml6hAVo3hXDtKOgQn2jgs2OAwBAp0VhOYMAf5umjWHeCgAAZuMjIQAA4PEoLAAAwONRWAAAgMejsAAAAI9HYQEAAB6PwgIAADwehQUAAHg8CgsAAPB4FBYAAODxKCwAAMDjUVgAAIDHo7AAAACPR2EBAAAezyeu1mwYhiSpvLzc5CQAAKC5vnrf/up9/Ex8orBUVFRIkuLj401OAgAAWqqiokJhYWFnXMdiNKfWeDi32638/HyFhITIYrG06bbLy8sVHx+vgwcPKjQ0tE23jW9jf3cs9nfHYn93LPZ3x2rN/jYMQxUVFYqNjZXVeuZZKj4xwmK1WtWzZ892/TdCQ0P5ge9A7O+Oxf7uWOzvjsX+7lgt3d9nG1n5CpNuAQCAx6OwAAAAj0dhOQuHw6G5c+fK4XCYHaVTYH93LPZ3x2J/dyz2d8dq7/3tE5NuAQCAb2OEBQAAeDwKCwAA8HgUFgAA4PEoLAAAwONRWM5i4cKFSkhIUEBAgFJSUrRmzRqzI/mETz75RJMnT1ZsbKwsFoveeOONJo8bhqE5c+aoR48eCgwMVFpamnJzc80J6+UyMzM1ZswYhYSEqHv37po6dap27tzZZJ3q6mrdcsst6tatm4KDg3XllVeqsLDQpMTe7ZlnntHw4cMbT56Vmpqq9957r/Fx9nX7mjdvniwWi+64447GZezztnP//ffLYrE0uQ0aNKjx8fbc1xSWM1iyZIkyMjI0d+5crVu3TklJSUpPT1dRUZHZ0bye0+lUUlKSFi5ceMrHH330UT311FNatGiRVq9erS5duig9PV3V1dUdnNT7rVixQrfccotWrVqlDz/8UHV1dZowYYKcTmfjOr/+9a/19ttva+nSpVqxYoXy8/N1xRVXmJjae/Xs2VPz5s1TTk6O1q5dq0suuURTpkzR1q1bJbGv29OXX36pZ599VsOHD2+ynH3etoYMGaIjR4403j777LPGx9p1Xxs4rbFjxxq33HJL4/cul8uIjY01MjMzTUzleyQZr7/+euP3brfbiImJMR577LHGZaWlpYbD4TBeeeUVExL6lqKiIkOSsWLFCsMwGvatv7+/sXTp0sZ1tm/fbkgyVq5caVZMnxIREWH87W9/Y1+3o4qKCqN///7Ghx9+aFx00UXG7bffbhgGP99tbe7cuUZSUtIpH2vvfc0Iy2nU1tYqJydHaWlpjcusVqvS0tK0cuVKE5P5vn379qmgoKDJvg8LC1NKSgr7vg2UlZVJkrp27SpJysnJUV1dXZP9PWjQIPXq1Yv9fY5cLpcWL14sp9Op1NRU9nU7uuWWW3TZZZc12bcSP9/tITc3V7GxsUpMTNT06dOVl5cnqf33tU9c/LA9lJSUyOVyKTo6usny6Oho7dixw6RUnUNBQYEknXLff/UYWsftduuOO+7Qd77zHQ0dOlRSw/622+0KDw9vsi77u/U2b96s1NRUVVdXKzg4WK+//roGDx6sDRs2sK/bweLFi7Vu3Tp9+eWX33qMn++2lZKSohdffFEDBw7UkSNH9MADD2j8+PHasmVLu+9rCgvQidxyyy3asmVLk8+c0fYGDhyoDRs2qKysTK+99ppmzpypFStWmB3LJx08eFC33367PvzwQwUEBJgdx+dNmjSp8f7w4cOVkpKi3r1769VXX1VgYGC7/tt8JHQakZGRstls35rdXFhYqJiYGJNSdQ5f7V/2fdu69dZb9c477+jjjz9Wz549G5fHxMSotrZWpaWlTdZnf7ee3W5Xv379lJycrMzMTCUlJenPf/4z+7od5OTkqKioSKNGjZKfn5/8/Py0YsUKPfXUU/Lz81N0dDT7vB2Fh4drwIAB2r17d7v/fFNYTsNutys5OVlZWVmNy9xut7KyspSammpiMt/Xp08fxcTENNn35eXlWr16Nfu+FQzD0K233qrXX39dH330kfr06dPk8eTkZPn7+zfZ3zt37lReXh77u4243W7V1NSwr9vBpZdeqs2bN2vDhg2Nt9GjR2v69OmN99nn7aeyslJ79uxRjx492v/n+5yn7fqwxYsXGw6Hw3jxxReNbdu2GTfddJMRHh5uFBQUmB3N61VUVBjr16831q9fb0gynnjiCWP9+vXGgQMHDMMwjHnz5hnh4eHGm2++aWzatMmYMmWK0adPH+PEiRMmJ/c+N998sxEWFmZkZ2cbR44cabxVVVU1rvOLX/zC6NWrl/HRRx8Za9euNVJTU43U1FQTU3uve++911ixYoWxb98+Y9OmTca9995rWCwW44MPPjAMg33dEb55lJBhsM/b0p133mlkZ2cb+/btMz7//HMjLS3NiIyMNIqKigzDaN99TWE5i6efftro1auXYbfbjbFjxxqrVq0yO5JP+Pjjjw1J37rNnDnTMIyGQ5vvu+8+Izo62nA4HMall15q7Ny509zQXupU+1mS8cILLzSuc+LECeOXv/ylERERYQQFBRk//OEPjSNHjpgX2ovdeOONRu/evQ273W5ERUUZl156aWNZMQz2dUf438LCPm8706ZNM3r06GHY7XYjLi7OmDZtmrF79+7Gx9tzX1sMwzDOfZwGAACg/TCHBQAAeDwKCwAA8HgUFgAA4PEoLAAAwONRWAAAgMejsAAAAI9HYQEAAB6PwgIAADwehQUAAHg8CgsAAPB4FBYAAODxKCwAAMDj/T+/vlDCEGmc7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_l2_norm(data):\n",
    "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
    "    deviation_data = data-mu\n",
    "    total_norm = 0.0\n",
    "    for row in deviation_data:\n",
    "        norm = torch.norm(row, p=2)\n",
    "        norm *= norm\n",
    "        total_norm += norm\n",
    "   # print(total_norm)\n",
    "   # print(data.size()[0])\n",
    "    return total_norm / data.size()[0]\n",
    "params = torch.load(\"50epoch0.75clip1.pt\")\n",
    "#print(params.size())\n",
    "difference_norms = []\n",
    "model_norms = []\n",
    "for step in range(params.size()[1]):\n",
    "    params_step = params[:, step, :]\n",
    "    print(get_l2_norm(params_step))\n",
    "    difference_norms.append(get_l2_norm(params_step))\n",
    "    for model in range(params.size()[0]):\n",
    "        model_norm = params[model, step, :]\n",
    "        model_norms.append(torch.norm(model_norm, p=2))\n",
    "avg_norm = sum(model_norms)/len(model_norms)\n",
    "#print(avg_norm)\n",
    "difference_norms_relative = [(norm/avg_norm).item() for norm in difference_norms]\n",
    "plt.plot(difference_norms_relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13a5c6c-41a9-43e7-ac9e-5e2208602536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T05:31:24.267159Z",
     "iopub.status.busy": "2023-10-10T05:31:24.266627Z",
     "iopub.status.idle": "2023-10-10T05:31:24.505260Z",
     "shell.execute_reply": "2023-10-10T05:31:24.504517Z",
     "shell.execute_reply.started": "2023-10-10T05:31:24.267137Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('10matrix5to25.npy', params_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe16508-5d54-4361-8c0f-a70f7be83abe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T02:43:41.448902Z",
     "iopub.status.busy": "2023-10-12T02:43:41.448012Z",
     "iopub.status.idle": "2023-10-12T02:43:41.466426Z",
     "shell.execute_reply": "2023-10-12T02:43:41.465499Z",
     "shell.execute_reply.started": "2023-10-12T02:43:41.448861Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_eig_of_differences(data):\n",
    "    \"\"\"\n",
    "    data should be a 2-dimensional tensor whose rows are the trained parameters\n",
    "    of an ML model, with one training per row. If you have a model with N\n",
    "    parameters that you run M times, data should be of size (M, N). Returns a\n",
    "    1-dimensional, M-element tensor whose entries are the eigenvalues (in no\n",
    "    particular order) of the square matrix calculated by subtracting the mean\n",
    "    of the input rows from each individual row, then right-multiplying that\n",
    "    matrix by its transpose. This guarantees a M by M square matrix, which\n",
    "    means that calculating the eigenvalues is surprisingly fast, even with\n",
    "    reasonably large models.\n",
    "    \"\"\"\n",
    "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
    "    deviation_data = data-mu\n",
    "    square_data = torch.matmul(deviation_data, deviation_data.transpose(0, 1))\n",
    "    eig = torch.linalg.eigvals(square_data)\n",
    "    return eig.real\n",
    "\n",
    "def get_gaussian_covariance(data):\n",
    "    cov = torch.cov(data.t()) \n",
    "    U, S, V = torch.linalg.svd(cov)\n",
    "    T = torch.nansum(torch.sqrt(S))\n",
    "    for i in range(5130):\n",
    "        S[i] = math.sqrt(S[i])*T\n",
    "    S = torch.diag(S)\n",
    "    temp = torch.matmul(U, S)\n",
    "    return torch.matmul(S,V)\n",
    "\n",
    "def get_gaussian(data):\n",
    "    cov = torch.cov(data.t()) \n",
    "    U, S, V = torch.linalg.svd(cov)\n",
    "    T = torch.nansum(torch.sqrt(S))\n",
    "    for i in range(5130):\n",
    "        S[i] = math.sqrt(S[i])*T\n",
    "    return S\n",
    "    \n",
    "\n",
    "def get_sample_covariance_eigs(data):\n",
    "    cov = torch.cov(data.t()) # torch.cov expects columns, not rows, to be individual trials\n",
    "    print(cov.size())\n",
    "    #eig = torch.linalg.eigvals(cov)\n",
    "    U, S, V = torch.linalg.svd(cov)\n",
    "    return S\n",
    "    #return eig.real\n",
    "\n",
    "def l2norm(data): \n",
    "    matrix = data.numpy()\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        for j in range(i+1, 10):\n",
    "            data1 = matrix[i]\n",
    "            data2 = matrix[j]\n",
    "           # print(np.shape(data1))\n",
    "            difference = data1 -data2\n",
    "           # print(type(difference))\n",
    "            total += np.linalg.norm(difference)\n",
    "            \n",
    "            count = count+1\n",
    "    print(total/count)\n",
    "    print(np.linalg.norm(matrix[0]))\n",
    "    print((total/count)/np.linalg.norm(matrix[0]))\n",
    "\n",
    "def l2normgraph(data, num): \n",
    "    matrix = data\n",
    "    plot1 = []\n",
    "    for a in range(50):\n",
    "        params_step = matrix[:, a, :]\n",
    "        count = 0\n",
    "        total = 0\n",
    "        for i in range(num):\n",
    "            for j in range(i+1, num):\n",
    "                data1 = params_step[i]\n",
    "                data2 = params_step[j]\n",
    "               # print(np.shape(data1))\n",
    "                difference = data1 -data2\n",
    "               # print(type(difference))\n",
    "                total += np.linalg.norm(difference)\n",
    "                count = count+1\n",
    "        plot1.append((total/count)/np.linalg.norm(params_step[0]))\n",
    "    plt.plot(plot1)\n",
    "def get_eig_of_differences_fast(data, num_eigvals=-1):\n",
    "    \"\"\"\n",
    "    ONLY USE THIS FUNCTION AFTER RUNNING AN EXCESSIVE (>10,000) NUMBER OF TRIALS\n",
    "    Acts similarly to get_eig_of_differences, except that it averages the N by N\n",
    "    square matrix whose eigenvalues are calculated with its transpose, then uses\n",
    "    torch.lobpcg to estimate only the num_eigvals largest eigenvalues, instead\n",
    "    of calculating all of them. num_eigvals defaults to data.size()[0]\n",
    "    \"\"\"\n",
    "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
    "    deviation_data = data-mu\n",
    "    square_data = torch.matmul(deviation_data, deviation_data.transpose(0, 1))\n",
    "    square_data = (square_data+square_data.transpose(0, 1))/2\n",
    "    eigvals_to_gen = data.size()[0] if num_eigvals == -1 else num_eigvals\n",
    "    eig = torch.lobpcg(square_data, k=eigvals_to_gen)\n",
    "    return eig[0]\n",
    "def eig_estimation(data,q):\n",
    "    #cov = torch.cov(data.t())\n",
    "    U, S, V = torch.pca_lowrank(data, q, center=True, niter=2)\n",
    "    return S\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33ff2d2c-28e0-4865-89ef-431543cf0261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-12T02:46:15.389798Z",
     "iopub.status.busy": "2023-10-12T02:46:15.389498Z",
     "iopub.status.idle": "2023-10-12T02:46:17.935195Z",
     "shell.execute_reply": "2023-10-12T02:46:17.934085Z",
     "shell.execute_reply.started": "2023-10-12T02:46:15.389776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7880,  0.6509, -0.0049,  ..., -0.0245, -0.0372,  0.0204],\n",
      "         [ 0.7937,  0.6634, -0.0048,  ..., -0.0239, -0.0363,  0.0199],\n",
      "         [ 0.7808,  0.6511, -0.0047,  ..., -0.0233, -0.0354,  0.0194],\n",
      "         ...,\n",
      "         [ 0.2320,  0.2371, -0.0015,  ..., -0.0076, -0.0115,  0.0063],\n",
      "         [ 0.2235,  0.2317, -0.0015,  ..., -0.0074, -0.0112,  0.0061],\n",
      "         [ 0.2197,  0.2260, -0.0014,  ..., -0.0072, -0.0109,  0.0060]],\n",
      "\n",
      "        [[ 0.7969,  0.6661, -0.0049,  ..., -0.0245, -0.0372,  0.0204],\n",
      "         [ 0.7804,  0.6615, -0.0048,  ..., -0.0239, -0.0363,  0.0199],\n",
      "         [ 0.7877,  0.6636, -0.0047,  ..., -0.0233, -0.0354,  0.0194],\n",
      "         ...,\n",
      "         [ 0.2961,  0.2682, -0.0015,  ..., -0.0076, -0.0115,  0.0063],\n",
      "         [ 0.2916,  0.2616, -0.0015,  ..., -0.0074, -0.0112,  0.0061],\n",
      "         [ 0.2862,  0.2526, -0.0014,  ..., -0.0072, -0.0109,  0.0060]],\n",
      "\n",
      "        [[ 0.7809,  0.6457, -0.0049,  ..., -0.0245, -0.0372,  0.0204],\n",
      "         [ 0.7779,  0.6543, -0.0048,  ..., -0.0239, -0.0363,  0.0199],\n",
      "         [ 0.7632,  0.6411, -0.0047,  ..., -0.0233, -0.0354,  0.0194],\n",
      "         ...,\n",
      "         [ 0.2269,  0.2631, -0.0015,  ..., -0.0076, -0.0115,  0.0063],\n",
      "         [ 0.2288,  0.2671, -0.0015,  ..., -0.0074, -0.0112,  0.0061],\n",
      "         [ 0.2300,  0.2596, -0.0014,  ..., -0.0072, -0.0109,  0.0060]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.7759,  0.6456, -0.0049,  ..., -0.0245, -0.0372,  0.0204],\n",
      "         [ 0.7655,  0.6453, -0.0048,  ..., -0.0239, -0.0363,  0.0199],\n",
      "         [ 0.7766,  0.6601, -0.0047,  ..., -0.0233, -0.0354,  0.0194],\n",
      "         ...,\n",
      "         [ 0.2567,  0.2311, -0.0015,  ..., -0.0076, -0.0115,  0.0063],\n",
      "         [ 0.2506,  0.2237, -0.0015,  ..., -0.0074, -0.0112,  0.0061],\n",
      "         [ 0.2475,  0.2243, -0.0014,  ..., -0.0072, -0.0109,  0.0060]],\n",
      "\n",
      "        [[ 0.7729,  0.6425, -0.0049,  ..., -0.0245, -0.0372,  0.0204],\n",
      "         [ 0.7651,  0.6474, -0.0048,  ..., -0.0239, -0.0363,  0.0199],\n",
      "         [ 0.7389,  0.6298, -0.0047,  ..., -0.0233, -0.0354,  0.0194],\n",
      "         ...,\n",
      "         [ 0.2618,  0.2451, -0.0015,  ..., -0.0076, -0.0115,  0.0063],\n",
      "         [ 0.2517,  0.2362, -0.0015,  ..., -0.0074, -0.0112,  0.0061],\n",
      "         [ 0.2484,  0.2379, -0.0014,  ..., -0.0072, -0.0109,  0.0060]],\n",
      "\n",
      "        [[ 0.7745,  0.6440, -0.0049,  ..., -0.0245, -0.0372,  0.0204],\n",
      "         [ 0.7744,  0.6540, -0.0048,  ..., -0.0239, -0.0363,  0.0199],\n",
      "         [ 0.7724,  0.6614, -0.0047,  ..., -0.0233, -0.0354,  0.0194],\n",
      "         ...,\n",
      "         [ 0.2488,  0.2814, -0.0015,  ..., -0.0076, -0.0115,  0.0063],\n",
      "         [ 0.2279,  0.2556, -0.0015,  ..., -0.0074, -0.0112,  0.0061],\n",
      "         [ 0.2304,  0.2607, -0.0014,  ..., -0.0072, -0.0109,  0.0060]]])\n",
      "input matrix successfully loaded. It is of size torch.Size([10, 50, 269722])\n",
      "32.47960722181532\n",
      "211.86305\n",
      "0.153304726136835\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#matrix = np.load('/notebooks/10matrix5to25pruned70%.npy', allow_pickle = True)\n",
    "#matrix = torch.load('100EpochsDivergence5Trials6.pt')\n",
    "#data = matrix\n",
    "params = torch.load(\"50epoch1.0clip1.pt\")\n",
    "matrix = params.numpy()\n",
    "\n",
    "data = torch.from_numpy(matrix)\n",
    "print(data)\n",
    "print(f\"input matrix successfully loaded. It is of size {data.size()}\")\n",
    "\"\"\"\n",
    "eig = eig_estimation(data,10)\n",
    "print(eig)\n",
    "print(eig.size())\n",
    "print(torch.nansum(torch.sqrt(eig)))\n",
    "\"\"\"\n",
    "norm1 = l2norm(data)\n",
    "l2normgraph(matrix,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc3be3-6ebf-4778-a3a3-50f9e4888381",
   "metadata": {},
   "source": [
    "70% when changing from 5k to 25k, pruning 0.3 before, about a 17% from 67 to 84%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924d3c7-2052-4818-8859-1d6cea3fe5d5",
   "metadata": {},
   "source": [
    "57% l2 norm when changing from 5k to 25k, pruning 0.2 before and 0.2 after, about a 14% from 67 to 81%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db5cb65-56c1-488f-bc81-d1f12706f38b",
   "metadata": {},
   "source": [
    "55% l2 norm from 5k to 25k, pruning 0.5 before, from 67% to 82%\n",
    "sgd better than adam for me, seems to slow down after 50 epochs and less divergence, pruning yields decent results and pairwise still goes from 70% to 80%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1b6ac-9fe2-4f47-ba9d-a20cacb9154a",
   "metadata": {},
   "source": [
    "l2 norm code produces way too crazy results for me though"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
