{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym2-5JKp6PMa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import scipy.io\n",
        "import copy\n",
        "from itertools import islice\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "\n",
        "random.seed(3242023)\n",
        "torch.manual_seed(3242023) # Seeded with a constant, so that behavior is deterministic.\n",
        "torch.cuda.manual_seed_all(3242023)\n",
        "\n",
        "one_hot = True\n",
        "run_multitest = False\n",
        "run_training_divergence = True\n",
        "\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "   device = \"cpu\"\n",
        "\n",
        "\n",
        "print(f\"Pytorch running on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRmL3XIJ3U_W"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def cov(mat):\n",
        "    \"\"\"\n",
        "    mat should be a matrix whose rows are separate samples.\n",
        "    \"\"\"\n",
        "    xbar = torch.zeros(mat.size()[1], device=device)\n",
        "    for row in mat:\n",
        "        xbar += row\n",
        "    xbar /= mat.size()[0]\n",
        "    matdelta = [row-xbar for row in mat]\n",
        "    print(f\"Squares of L2 norms of X-Xbar: {[torch.nansum(torch.square(row)) for row in matdelta]}\")\n",
        "    ans = []\n",
        "    for row in matdelta:\n",
        "        unsqueezed_row = row.unsqueeze(0)\n",
        "        ans.append(torch.matmul(unsqueezed_row.t(), unsqueezed_row))\n",
        "    return sum(ans)/(len(ans)-1)\n",
        "\n",
        "def get_eig_of_differences(data):\n",
        "    \"\"\"\n",
        "    data should be a 2-dimensional tensor whose rows are the trained parameters\n",
        "    of an ML model, with one training per row. If you have a model with N\n",
        "    parameters that you run M times, data should be of size (M, N). Returns a\n",
        "    1-dimensional, M-element tensor whose entries are the eigenvalues (in no\n",
        "    particular order) of the square matrix calculated by subtracting the mean\n",
        "    of the input rows from each individual row, then right-multiplying that\n",
        "    matrix by its transpose. This guarantees a M by M square matrix, which\n",
        "    means that calculating the eigenvalues is surprisingly fast, even with\n",
        "    reasonably large models.\n",
        "    \"\"\"\n",
        "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
        "    deviation_data = data-mu\n",
        "    square_data = torch.matmul(deviation_data, deviation_data.transpose(0, 1))\n",
        "    eig = torch.linalg.eigvals(square_data)\n",
        "    return eig.real\n",
        "\n",
        "def get_l2_norm(data):\n",
        "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
        "    deviation_data = data-mu\n",
        "    total_norm = 0.0\n",
        "    for row in deviation_data:\n",
        "        norm = torch.norm(row, p=2)\n",
        "        norm *= norm\n",
        "        total_norm += norm\n",
        "    return total_norm / data.size()[0]\n",
        "\n",
        "def get_sample_covariance_eigs(data):\n",
        "    cov = torch.cov(data.t()) # torch.cov expects columns, not rows, to be individual trials\n",
        "    eig = torch.linalg.eigvals(cov)\n",
        "    return eig.real\n",
        "\n",
        "def get_eig_of_differences_fast(data, num_eigvals=-1):\n",
        "    \"\"\"\n",
        "    ONLY USE THIS FUNCTION AFTER RUNNING AN EXCESSIVE (>10,000) NUMBER OF TRIALS\n",
        "    Acts similarly to get_eig_of_differences, except that it averages the N by N\n",
        "    square matrix whose eigenvalues are calculated with its transpose, then uses\n",
        "    torch.lobpcg to estimate only the num_eigvals largest eigenvalues, instead\n",
        "    of calculating all of them. num_eigvals defaults to data.size()[0]\n",
        "    \"\"\"\n",
        "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
        "    deviation_data = data-mu\n",
        "    square_data = torch.matmul(deviation_data, deviation_data.transpose(0, 1))\n",
        "    square_data = (square_data+square_data.transpose(0, 1))/2\n",
        "    eigvals_to_gen = data.size()[0] if num_eigvals == -1 else num_eigvals\n",
        "    eig = torch.lobpcg(square_data, k=eigvals_to_gen)\n",
        "    return eig[0]\n",
        "\n",
        "def generate_gaussian_noise(cov, noise_mult=1, copies = 1):\n",
        "    U, S, V = torch.linalg.svd(cov)\n",
        "    Sprime = torch.sqrt(S)\n",
        "    print(\"Noise: sum of sqrts of svd of cov:\")\n",
        "    print(torch.sum(Sprime))\n",
        "    Sprime *= torch.nansum(Sprime)\n",
        "    Sprime = torch.sqrt(Sprime)\n",
        "    noise_list = []\n",
        "    for i in range(copies): # TODO this may be suboptimal, but I'm not sure the obvious speedup doesn't change the output\n",
        "        noise = torch.randn(cov.size()[0], device=device)*noise_mult\n",
        "        noise = torch.matmul(noise, torch.diag(Sprime))\n",
        "        noise = torch.matmul(U, noise)\n",
        "        noise_list.append(noise)\n",
        "    return torch.stack(noise_list)\n",
        "\n",
        "def gen_noise2(data,q,noise_mult=0.1):\n",
        "    \"\"\"\n",
        "    Taken wholesale from Aidan's code.\n",
        "    \"\"\"\n",
        "    U, S, V = torch.pca_lowrank(data, q, center=True, niter=2)\n",
        "    Sprime = torch.sqrt(S)\n",
        "    Sprime *= torch.nansum(Sprime)\n",
        "    Sprime = torch.sqrt(Sprime)\n",
        "   # cov = torch.cov(data.t())\n",
        "    print(\"here\")\n",
        "    noise = torch.randn(data.size()[0], device=device)*noise_mult\n",
        "    print(noise.size())\n",
        "    temp = torch.diag(Sprime).type(torch.FloatTensor)\n",
        "    temp = temp.to(device)\n",
        "    noise = torch.matmul(noise, temp)\n",
        "    print(noise.size())\n",
        "    noise = noise.type(torch.DoubleTensor)\n",
        "    noise = torch.matmul(V.to(device), noise.to(device).float())\n",
        "    print(noise.size())\n",
        "    return noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVrXAHve6XDw"
      },
      "outputs": [],
      "source": [
        "def convert_to_one_hot(data): # Taken from https://stackoverflow.com/questions/36960320/convert-a-2d-matrix-to-a-3d-one-hot-matrix-numpy\n",
        "    \"\"\"\n",
        "    WARNING: mutates input, incrementing each value in it by 1\n",
        "    \"\"\"\n",
        "    data += 1\n",
        "    return np.squeeze((np.arange(data.max()) == data[...,None]-1).astype(int))\n",
        "\n",
        "train_mat = scipy.io.loadmat('data_cifar100_train.mat', verify_compressed_data_integrity=False) # data_cifar100_train.mat is the right file\n",
        "print(train_mat.keys())\n",
        "train_mat = train_mat[\"data\"]\n",
        "print(type(train_mat))\n",
        "print(train_mat.shape)\n",
        "ans_mat = scipy.io.loadmat('label_train.mat')\n",
        "ans_mat = ans_mat[\"data\"]\n",
        "print(ans_mat.shape)\n",
        "print(ans_mat[:12])\n",
        "ans_mat = ans_mat.transpose()\n",
        "coeffs = np.linalg.lstsq(train_mat, ans_mat)[0]\n",
        "print(coeffs.shape)\n",
        "ans = np.matmul(train_mat, coeffs)\n",
        "print(sum(ans))\n",
        "print(sum(ans-ans_mat))\n",
        "if one_hot:\n",
        "    ans_mat = convert_to_one_hot(ans_mat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz56BiCTFi7c"
      },
      "outputs": [],
      "source": [
        "round_to_n = lambda x, n: 0 if math.isnan(x) or x == 0 else round(x, -int(math.floor(math.log10(x))) + (n - 1)) # Taken from https://stackoverflow.com/questions/3410976/how-to-round-a-number-to-significant-figures-in-python\n",
        "\n",
        "def create_heatmap(data, x_axis_labels, y_axis_labels, title=None, sig_figs = 2):\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(data)\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries\n",
        "    ax.set_xticks(np.arange(len(x_axis_labels)), labels=x_axis_labels)\n",
        "    ax.set_yticks(np.arange(len(y_axis_labels)), labels=y_axis_labels)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(y_axis_labels)):\n",
        "        for j in range(len(x_axis_labels)):\n",
        "            data_to_display = round_to_n(data[i, j], sig_figs)\n",
        "            if data_to_display % 1 == 0:\n",
        "                data_to_display = int(data_to_display)\n",
        "            text = ax.text(j, i, data_to_display,\n",
        "                            ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    if title != None:\n",
        "        ax.set_title(title)\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq3EXBv-8xD5"
      },
      "outputs": [],
      "source": [
        "def get_params(layer):\n",
        "    return torch.cat((layer.weight.data, layer.bias.data.unsqueeze(1)), 1)\n",
        "\n",
        "def clip_group_sample_grad(grad_list, max_norm=1.0): # MICROBATCHING IS HERE\n",
        "    reshaped_grad_list = []\n",
        "    grad_lengths = []\n",
        "    for part_grad in grad_list:\n",
        "        if len(part_grad.shape) == 1:\n",
        "            reshaped_grad_list.append(part_grad.unsqueeze(1))\n",
        "            grad_lengths.append(1)\n",
        "        elif len(part_grad.shape) > 2:\n",
        "            raise RuntimeError(\"Gradient is 3-dimensional. That means that this quick gradient-stacking code finally needs to be redone.\")\n",
        "        else:\n",
        "            grad_lengths.append(part_grad.shape[1])\n",
        "            reshaped_grad_list.append(part_grad)\n",
        "    grad_tensor = torch.cat(reshaped_grad_list, 1) # This will fail if the network is more complicated than a linear layer; it will need to be rewritten at some point\n",
        "\n",
        "    l2_norm = torch.norm(grad_tensor, p=2)\n",
        "    # print(f\"Current norm: {l2_norm}\")\n",
        "    if l2_norm > max_norm:\n",
        "        # print(\"Clipping!\")\n",
        "        grad_tensor *= max_norm/l2_norm\n",
        "    # else:\n",
        "        # print(\"Not clipping\")\n",
        "\n",
        "    out_list = []\n",
        "    grad_idx = 0\n",
        "    for length in grad_lengths:\n",
        "        out_list.append(grad_tensor[:, grad_idx:grad_idx+length].squeeze())\n",
        "        grad_idx += length\n",
        "    return out_list\n",
        "\n",
        "def measure_model_stability(models):\n",
        "    coeffs_list = []\n",
        "\n",
        "    for model in models:\n",
        "        coeffs_list.append(np.array(get_params(model)))\n",
        "\n",
        "    coeffs_list = [i.flatten() for i in coeffs_list]\n",
        "    coeffs_norms = [math.sqrt(np.dot(i, i)) for i in coeffs_list]\n",
        "\n",
        "    num_trials = len(models)\n",
        "\n",
        "    avg_coeff = sum(coeffs_list)/num_trials\n",
        "    coeff_diffs_list = [i-avg_coeff for i in coeffs_list]\n",
        "    coeffs_diffs_norms = [math.sqrt(np.dot(i, i)) for i in coeff_diffs_list]\n",
        "\n",
        "    avg_l2 = sum(coeffs_norms)/num_trials\n",
        "    avg_l2_deviation = sum(coeffs_diffs_norms)/num_trials\n",
        "\n",
        "    return avg_l2, avg_l2_deviation\n",
        "\n",
        "def train_models_divergent_one_step(models_in, optimizers, data_sets, label_sets, measure_stability=True, criterion=nn.CrossEntropyLoss(), test_data=None, test_labels=None, microbatch_size=100, clipping_func = lambda x: x):\n",
        "    assert len(data_sets) == len(label_sets) and len(data_sets) == len(models_in)\n",
        "    for i in range(len(data_sets)):\n",
        "        all_group_sample_gradients = []\n",
        "        for j in range(0, len(data_sets[i]), microbatch_size):\n",
        "            microbatch = data_sets[i][j:j+microbatch_size]\n",
        "            microbatch_ans = label_sets[i][j:j+microbatch_size]\n",
        "\n",
        "            out = models_in[i](microbatch)\n",
        "            loss = criterion(out, microbatch_ans)\n",
        "            loss.backward()\n",
        "\n",
        "            group_sample_gradients = clipping_func([p.grad.detach().clone() for p in models_in[i].parameters()])\n",
        "\n",
        "            all_group_sample_gradients.append(group_sample_gradients)\n",
        "            optimizers[i].zero_grad()\n",
        "\n",
        "        num_microbatches = len(all_group_sample_gradients) # MICROBATCHING IS HERE\n",
        "        processed_grads = [sum([idx[k] for idx in all_group_sample_gradients])/num_microbatches for k in range(len(all_group_sample_gradients[0]))]\n",
        "\n",
        "        for idx, p in enumerate(models_in[i].parameters()):\n",
        "            p.grad = processed_grads[idx]\n",
        "\n",
        "        optimizers[i].step()\n",
        "        if test_data is not None:\n",
        "            test_out = models_in[i](test_data)\n",
        "            accuracy = get_accuracy(test_out.detach(), test_labels)\n",
        "            print(f\"Model {i} has an accuracy of {accuracy}\")\n",
        "    if measure_stability:\n",
        "        return measure_model_stability(models_in)\n",
        "\n",
        "def train_models_divergent(train_set, train_ans, num_datapoints=10000, num_trials=8, steps=16, microbatch_size=100, test_data=None, test_labels=None, graph_results=True, clipping_threshold=1.0):\n",
        "    \"\"\"\n",
        "    returns a list of tuples. Each tuple represents one epoch and has two\n",
        "    elements. The first element is the average l2 norm of the models after that\n",
        "    epoch. The second element is the average deviation in that same epoch.\n",
        "    \"\"\"\n",
        "    data_sets, label_sets = random_sample(train_set, train_ans, num_datapoints, num_trials)\n",
        "    # data_sets, label_sets = random_sample_some_removed(train_set, train_ans, num_datapoints, 1, num_trials)\n",
        "    data_sets = torch.Tensor(data_sets)\n",
        "    label_sets = torch.Tensor(label_sets)\n",
        "    if test_data is not None:\n",
        "        test_data = torch.Tensor(test_data)\n",
        "        test_labels = torch.Tensor(test_labels)\n",
        "\n",
        "    model_base = nn.Linear(1024, 10)\n",
        "    models = [copy.deepcopy(model_base) for i in range(num_trials)]\n",
        "    optimizers = [torch.optim.SGD(m.parameters(), lr=0.1) for m in models]\n",
        "\n",
        "    stability_data = []\n",
        "\n",
        "    temp_clip_func = lambda x: clip_group_sample_grad(x, clipping_threshold)\n",
        "\n",
        "    for step in range(steps):\n",
        "        if test_data is not None:\n",
        "            print(f\"Now beginning epoch {step}\")\n",
        "        stability_data.append(train_models_divergent_one_step(models, optimizers, data_sets, label_sets, True, test_data=test_data, test_labels=test_labels, microbatch_size=microbatch_size, clipping_func=temp_clip_func))\n",
        "\n",
        "    if graph_results:\n",
        "        l2_fractional_deviation = [data[1]/data[0] for data in stability_data]\n",
        "        plt.plot([0] + l2_fractional_deviation)\n",
        "\n",
        "    return stability_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq9BmKV0LpwS"
      },
      "outputs": [],
      "source": [
        "def random_sample(data, labels, qty=1000, num_samples=1):\n",
        "    ans_data = []\n",
        "    ans_labels = []\n",
        "    dataset_size = data.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        listed = np.random.choice(range(dataset_size), qty, replace=False)\n",
        "        ans_data.append(np.take(data, listed, axis=0))\n",
        "        ans_labels.append(np.take(labels, listed, axis=0))\n",
        "    return np.stack(ans_data), np.stack(ans_labels)\n",
        "\n",
        "def random_sample_some_removed(data, labels, qty=1000, samples_removed=1, num_samples=1):\n",
        "    ans_data = []\n",
        "    ans_labels = []\n",
        "    dataset_size = data.shape[0]\n",
        "    sample_pool = np.random.choice(range(dataset_size), qty, replace=False)\n",
        "    for i in range(num_samples):\n",
        "        listed = np.random.choice(sample_pool, qty-samples_removed, replace=False)\n",
        "        ans_data.append(np.take(data, listed, axis=0))\n",
        "        ans_labels.append(np.take(labels, listed, axis=0))\n",
        "    return np.stack(ans_data), np.stack(ans_labels)\n",
        "\n",
        "def grade_prediction_single_channel(prediction, truth):\n",
        "    return int(prediction) == int(truth)\n",
        "\n",
        "def grade_prediction_one_hot(prediction, truth):\n",
        "    return truth[np.argmax(prediction)] == 1\n",
        "\n",
        "def grade_prediction_context_sensitive(prediction, truth):\n",
        "    if one_hot:\n",
        "        return grade_prediction_one_hot(prediction, truth)\n",
        "    else:\n",
        "        return grade_prediction_single_channel(prediction, truth)\n",
        "\n",
        "def get_accuracy(output, ground_truth, grader = grade_prediction_context_sensitive):\n",
        "    samples = output.shape[0]\n",
        "    successes = 0\n",
        "    for sample in range(samples):\n",
        "        if grader(output[sample], ground_truth[sample]):\n",
        "            successes += 1\n",
        "    return successes/samples\n",
        "\n",
        "def linear_regression(train_data, train_ans):\n",
        "    if len(train_ans.shape) <= 2: # If data is not 1-hot\n",
        "        return np.linalg.lstsq(train_data, train_ans)[0]\n",
        "    ans = []\n",
        "    for batch in range(train_ans.shape[0]):\n",
        "        ans.append(np.linalg.lstsq(train_data[batch], train_ans[batch])[0])\n",
        "    return np.stack(ans)\n",
        "\n",
        "def test_samples(data_samples, label_samples):\n",
        "    test_samples_separate_train(data_samples, label_samples, data_samples, label_samples)\n",
        "\n",
        "def test_samples_separate_train(train_in, train_ans, test_in, test_ans, fast_noise=False):\n",
        "    coeffs_list = []\n",
        "    train_error_pre_noise = []\n",
        "    test_error_pre_noise = []\n",
        "    for i in range(train_in.shape[0]):\n",
        "        coeffs = np.linalg.lstsq(train_in[i], train_ans[i])[0]\n",
        "        train_results = np.matmul(train_in[i], coeffs)\n",
        "        print(\"Train:\")\n",
        "        print(\"Train accuracy: \", get_accuracy(train_results, train_ans[i]))\n",
        "        print(math.sqrt((train_results**2).mean()))\n",
        "        error = math.sqrt(((train_results - train_ans[i])**2).mean())\n",
        "        train_error_pre_noise.append(error)\n",
        "        print(error)\n",
        "        print(\"Test:\")\n",
        "        ans = np.matmul(test_in, coeffs)\n",
        "        print(\"Test accuracy: \", get_accuracy(ans, test_ans))\n",
        "        print(math.sqrt((ans**2).mean()))\n",
        "        error = math.sqrt(((ans - test_ans)**2).mean())\n",
        "        test_error_pre_noise.append(error)\n",
        "        print(error)\n",
        "        coeffs_list.append(coeffs)\n",
        "\n",
        "    coeffs = np.stack(coeffs_list).squeeze()\n",
        "    if fast_noise:\n",
        "        noise = gen_noise2(torch.Tensor(coeffs).to(device), q=train_in.shape[0], noise_mult=1)\n",
        "        noise = np.array(noise.cpu())\n",
        "        noise = np.expand_dims(noise, 0).repeat(train_in.shape[0], 0) # Uses many copies of the same noise\n",
        "    else:\n",
        "        # noise = generate_gaussian_noise(cov(torch.Tensor(coeffs).to(device)), noise_mult=1, copies=train_in.shape[0]) # uses many different sets of noise\n",
        "        noise = generate_gaussian_noise(cov(torch.Tensor(coeffs).to(device)), noise_mult=1)\n",
        "        noise = np.array(noise.cpu())\n",
        "        noise = noise.repeat(5, 0) # Uses many copies of the same noise\n",
        "    coeffs = np.expand_dims(np.add(coeffs, noise), 2)\n",
        "    train_error_post_noise = []\n",
        "    test_error_post_noise = []\n",
        "\n",
        "    for i in range(train_in.shape[0]):\n",
        "        train_ans = np.matmul(train_in[i], coeffs)\n",
        "        print(\"Train:\")\n",
        "        print(\"Train accuracy: \", get_accuracy(train_results, train_ans[i]))\n",
        "        print(math.sqrt((train_ans**2).mean()))\n",
        "        error = math.sqrt(((train_ans - train_ans[i])**2).mean())\n",
        "        train_error_post_noise.append(error)\n",
        "        print(error)\n",
        "        print(\"Test:\")\n",
        "        ans = np.matmul(test_in, coeffs[i])\n",
        "        print(\"Test accuracy: \", get_accuracy(ans, test_ans))\n",
        "        print(math.sqrt((ans**2).mean()))\n",
        "        error = math.sqrt(((ans - test_ans)**2).mean())\n",
        "        test_error_post_noise.append(error)\n",
        "        print(error)\n",
        "\n",
        "    return sum(test_error_pre_noise) / len(test_error_pre_noise), sum(test_error_post_noise) / len(test_error_post_noise)\n",
        "\n",
        "def test_stability(train_in, train_ans, num_removed = None, printout = True, train_printout = True, test_data = None, test_ans = None):\n",
        "    \"\"\"\n",
        "    Returns a tuple whose first element is the average model l2 norm and whose\n",
        "    second element is the standard deviation of l2 norms.\n",
        "    \"\"\"\n",
        "    coeffs_list = []\n",
        "    for i in range(train_in.shape[0]):\n",
        "        coeffs = np.linalg.lstsq(train_in[i], train_ans[i])[0]\n",
        "        if train_printout:\n",
        "            train_results = np.matmul(train_in[i], coeffs)\n",
        "            print(\"Train accuracy: \", get_accuracy(train_results, train_ans[i]))\n",
        "        if test_data is not None and test_ans is not None:\n",
        "            test_results = np.matmul(test_data, coeffs)\n",
        "            print(\"Test accuracy: \", get_accuracy(test_results, test_ans))\n",
        "        coeffs_list.append(coeffs)\n",
        "\n",
        "    coeffs = np.stack(coeffs_list).squeeze()\n",
        "    coeffs_list = [i.flatten() for i in coeffs_list]\n",
        "    coeffs_norms = [math.sqrt(np.dot(i, i)) for i in coeffs_list]\n",
        "\n",
        "    num_trials = len(coeffs_norms)\n",
        "\n",
        "    avg_coeff = sum(coeffs_list)/num_trials\n",
        "    coeff_diffs_list = [i-avg_coeff for i in coeffs_list]\n",
        "    coeffs_diffs_norms = [math.sqrt(np.dot(i, i)) for i in coeff_diffs_list]\n",
        "\n",
        "    avg_l2 = sum(coeffs_norms)/num_trials\n",
        "    avg_l2_deviation = sum(coeffs_diffs_norms)/num_trials\n",
        "\n",
        "    if printout:\n",
        "        print(\"\")\n",
        "        if num_removed == None:\n",
        "            print(f\"{num_trials} were conducted. Each trial contained {train_in.shape[1]} data points.\")\n",
        "            print(f\"Average l2 norm was {avg_l2:.2f}. We expect the average deviation to be equal to the average multiplied by the fraction of samples removed (e.g. for removing 1 sample from a set of 1000, divide by 1000)\")\n",
        "        else:\n",
        "            print(f\"{num_trials} trials were conducted. A bank of {train_in.shape[1]+num_removed} data points was used with each trial omitting {num_removed} data points\")\n",
        "            print(f\"Average l2 norm was {avg_l2:.2f}. We expect the average deviation to be equal to 1/{(train_in.shape[1]+num_removed)/num_removed}\")\n",
        "        print(f\"Average l2 norm deviation was {avg_l2_deviation:.2f}. This is equal to {100*avg_l2_deviation/avg_l2:.4f}% of the average l2 norm, or 1/{avg_l2/avg_l2_deviation:.1f}\")\n",
        "\n",
        "    return avg_l2, avg_l2_deviation\n",
        "\n",
        "def multitest_stability(train_set, ans_set, dataset_sizes=(math.pow(2, i) for i in range(6, 16)), nums_removed=(math.pow(2, i) for i in range(15)), samples_per_trial=25, plot=True, status_printouts=True):\n",
        "    print(\"Beginning stability testing\")\n",
        "    l2_results = np.zeros((len(dataset_sizes), len(nums_removed)))\n",
        "    l2_deviation_results = np.zeros((len(dataset_sizes), len(nums_removed)))\n",
        "    for i, dataset_size in enumerate(dataset_sizes):\n",
        "        for j, num_removed in enumerate(nums_removed):\n",
        "            if num_removed >= dataset_size:\n",
        "                continue\n",
        "            if status_printouts:\n",
        "                print(f\"Now calculating with {dataset_size} total samples and {num_removed} samples removed per trial.\")\n",
        "            in_samples, out_samples = random_sample_some_removed(train_mat, ans_mat, dataset_size, num_removed, samples_per_trial)\n",
        "            l2, deviation = test_stability(in_samples, out_samples, num_removed, printout=False, train_printout=False)\n",
        "            l2_results[i, j] = l2\n",
        "            l2_deviation_results[i, j] = deviation\n",
        "    if plot:\n",
        "        x_labels = [f\"{i} removed\" for i in nums_removed]\n",
        "        y_labels = [f\"{i} datapoints\" for i in dataset_sizes]\n",
        "\n",
        "        create_heatmap(l2_results, x_labels, y_labels, \"L2 norm\")\n",
        "        create_heatmap(l2_deviation_results, x_labels, y_labels, \"L2 deviation\")\n",
        "\n",
        "        deviation_fraction = np.divide(l2_deviation_results, l2_results)\n",
        "        create_heatmap(deviation_fraction, x_labels, y_labels, \"L2 deviation as a fraction of L2 norm\")\n",
        "\n",
        "        theoretical_deviation = np.array([[0 if j >= i else j/i for j in nums_removed] for i in dataset_sizes])\n",
        "        create_heatmap(theoretical_deviation, x_labels, y_labels, \"Theoretical expected deviation\")\n",
        "\n",
        "        deviation_efficacy = np.divide(deviation_fraction, theoretical_deviation)\n",
        "        create_heatmap(deviation_efficacy, x_labels, y_labels, \"Ratio of observed to expected deviation\")\n",
        "\n",
        "    return l2_results, l2_deviation_results\n",
        "\n",
        "test_mat = scipy.io.loadmat('data_cifar100_test.mat', verify_compressed_data_integrity=False)[\"data\"]\n",
        "ans_mat_test = scipy.io.loadmat('label_test.mat')[\"data\"].transpose()\n",
        "if one_hot:\n",
        "    ans_mat_test = convert_to_one_hot(ans_mat_test)\n",
        "\n",
        "# in_samples, out_samples = random_sample(train_mat, ans_mat, 20000, 2)\n",
        "# test_stability(in_samples, out_samples, None, True, True, test_mat, ans_mat_test)\n",
        "\n",
        "if run_multitest:\n",
        "    dataset_sizes = (2048, 4096, 8092, 16184, 32368, 50000)\n",
        "    nums_removed = (1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8092, 16184)\n",
        "    samples_per_trial = 16\n",
        "    results = multitest_stability(train_mat, ans_mat, dataset_sizes, nums_removed, samples_per_trial, False)\n",
        "\n",
        "# samples=100\n",
        "# num_removed=1\n",
        "# total_dataset_size=10000\n",
        "# in_samples, out_samples = random_sample_some_removed(train_mat, ans_mat, total_dataset_size, num_removed, samples)\n",
        "# test_stability(in_samples, out_samples, num_removed, train_printout=False)\n",
        "\n",
        "# pre, post = test_samples_separate_train(in_samples, out_samples, test_mat, ans_mat_test, fast_noise=True)\n",
        "# print(pre)\n",
        "# print(post)\n",
        "\n",
        "# DONE (strange results) try with momentum (0.9).\n",
        "# DONE try with microbatch size of 1 (i.e. use Opacus). See how it compares.\n",
        "# DONE try for a large # of epochs, see when it plateaus\n",
        "# DONE(ish) (interesting results...) try with lower clipping threshold, see if it plateaus sooner\n",
        "# TODO (Aidan?) retrofit the microbatching stuff to work with a Resnet20 and the raw data\n",
        "\n",
        "# TODO train normally (no group-sample clipping) for 100 epochs. Release the model by adding noise (in future attempts, not necessarily at first). Continue training for 100 epochs, repeat indefinitely.\n",
        "if run_training_divergence:\n",
        "    stability_data = []\n",
        "    for run_data in ((10, 1.0), (25, 1.0), (100, 1.0), (400, 1.0)): # ((10, 3.0), (25, 2.25), (100, 1.5), (400, 1.0)):\n",
        "        stability_data.append(train_models_divergent(train_mat, ans_mat, num_datapoints=20000, num_trials=8, steps=128, test_data=test_mat, test_labels=ans_mat_test, microbatch_size=run_data[0], clipping_threshold=run_data[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZAdieD3LziN"
      },
      "outputs": [],
      "source": [
        "if run_multitest:\n",
        "    x_labels = [f\"{i} removed\" for i in nums_removed]\n",
        "    y_labels = [f\"{i} datapoints\" for i in dataset_sizes]\n",
        "    create_heatmap(results[0], x_labels, y_labels, \"L2 norm\")\n",
        "    create_heatmap(results[1], x_labels, y_labels, \"L2 deviation\")\n",
        "    deviation_fraction = np.divide(results[1], results[0])\n",
        "    create_heatmap(deviation_fraction, x_labels, y_labels, \"L2 deviation as a fraction of L2 norm\")\n",
        "    theoretical_deviation = np.array([[0 if j >= i else j/i for j in nums_removed] for i in dataset_sizes])\n",
        "    create_heatmap(theoretical_deviation, x_labels, y_labels, \"Theoretical expected deviation\")\n",
        "    deviation_efficacy = np.divide(deviation_fraction, theoretical_deviation)\n",
        "    create_heatmap(deviation_efficacy, x_labels, y_labels, \"Ratio of observed to expected deviation\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaQEUo97omrH"
      },
      "outputs": [],
      "source": [
        "for series in stability_data:\n",
        "    plt.plot([0] + [i[1] for i in series])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRBcYR9bPr8Y"
      },
      "outputs": [],
      "source": [
        "for series in stability_data:\n",
        "    for i in series:\n",
        "        print(i[1], end=\", \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}