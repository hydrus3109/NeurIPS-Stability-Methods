{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTF9SsQdlojNOCGIdjaf9Q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ],
      "metadata": {
        "id": "yeH1MRhpDspz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "\n",
        "random.seed(3242023)\n",
        "torch.manual_seed(3242023) # Seeded with a constant, so that behavior is deterministic.\n",
        "torch.cuda.manual_seed_all(3242023)\n",
        "\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "   device = \"cpu\"\n",
        "\n",
        "PRETRAIN_FROM_SCRATCH = True # If True, pretrains a Resnet20. If False, uploads a pretrained Resnet20\n",
        "samples = 5000\n",
        "sample_mode = \"independent\" # Should be one of \"independent\" or an integer number of samples to exclude from each training\n",
        "pretraining_epochs = 1000\n",
        "pretraining_samples = 50000\n",
        "pretraining_batch_size = 2000 # Set equal to pretraining_samples if you don't want to use batches.\n",
        "epochs = 250\n",
        "num_trials = 25\n",
        "\n",
        "print(f\"Pytorch running on {device}\")"
      ],
      "metadata": {
        "id": "ym2-5JKp6PMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _weights_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3]).to(device)\n",
        "\n",
        "\"\"\"\n",
        "All code above here with the exception of setting the random seed was taken from\n",
        "https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py\n",
        "\"\"\"\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "def get_params(model):\n",
        "    params = []\n",
        "    for param in model.parameters():\n",
        "        params.append(param.view(-1))\n",
        "    params = torch.cat(params)\n",
        "    return params"
      ],
      "metadata": {
        "id": "wrV8iqiK6NlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def test_model(model, preprocessing = lambda x: x):\n",
        "    test_loss = 0.0\n",
        "    testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size = 20)\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "    model.eval()\n",
        "    # iterate over test data\n",
        "    for data, target in testloader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # preprocess data with an arbitrary function\n",
        "        data = preprocessing(data)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update test loss\n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(20):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "    # average test loss\n",
        "    test_loss = test_loss/len(testloader.dataset)\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    for i in range(10):\n",
        "        if class_total[i] > 0:\n",
        "\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "              classes[i], 100 * class_correct[i] / class_total[i],\n",
        "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "    return 100. * np.sum(class_correct) / np.sum(class_total)"
      ],
      "metadata": {
        "id": "DRmL3XIJ3U_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD_DEV = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD_DEV),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "net = resnet20()\n",
        "net.to(device)\n",
        "\n",
        "if not PRETRAIN_FROM_SCRATCH:\n",
        "    uploaded = files.upload()\n",
        "    torch_object = torch.load(BytesIO(list(uploaded.values())[0]), map_location=device)\n",
        "    net.load_state_dict(torch_object)\n",
        "else:\n",
        "    pretrain_subset = torch.utils.data.Subset(trainset, random.sample(range(50000), pretraining_samples))\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(pretrain_subset, batch_size=pretraining_batch_size,\n",
        "                                        shuffle=False)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "\n",
        "params = []"
      ],
      "metadata": {
        "id": "qVrXAHve6XDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if PRETRAIN_FROM_SCRATCH:\n",
        "    print(\"Beginning pretraining...\")\n",
        "    net.train()\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "    for epoch in range(pretraining_epochs):\n",
        "        for data in iter(trainloader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} of pretraining successfully completed\")\n",
        "    torch.save(net.state_dict(), \"PretrainedResnet20.pt\")\n",
        "    files.download(\"PretrainedResnet20.pt\")"
      ],
      "metadata": {
        "id": "zLADmpke6e8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnqGxnYF6CcG"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss after pretraining:\")\n",
        "test_model(net)\n",
        "net.linear = Identity()\n",
        "net.eval()\n",
        "preprocessing = net"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseModel(nn.Module): # TODO replace with the last layer of pretrained\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(64, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 32)\n",
        "        self.layer4 = nn.Linear(32, 16)\n",
        "        self.layer5 = nn.Linear(16, 16)\n",
        "        self.layer6 = nn.Linear(16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.layer1(x))\n",
        "        out = F.relu(self.layer2(out))\n",
        "        out = F.relu(self.layer3(out))\n",
        "        out = F.relu(self.layer4(out))\n",
        "        out = F.relu(self.layer5(out))\n",
        "        out = self.layer6(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yaSYCv8E7PGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_base = DenseModel()\n",
        "net_base.to(device)\n",
        "params = []\n",
        "print(\"Beginning training...\")\n",
        "for trial in range(num_trials):\n",
        "    net = DenseModel()\n",
        "    net.to(device)\n",
        "    net.load_state_dict(net_base.state_dict()) # Ensure we start from the same place with both models. This appears to be the crucial step.\n",
        "\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "    if sample_mode == \"independent\":\n",
        "        train_subset = torch.utils.data.Subset(trainset, random.sample(range(50000), samples))\n",
        "    else:\n",
        "        train_subset = torch.utils.data.Subset(trainset, random.sample(range(samples), samples-sample_mode))\n",
        "    trainloader = torch.utils.data.DataLoader(train_subset, batch_size=samples,\n",
        "                                          shuffle=False)\n",
        "    net.train()\n",
        "\n",
        "    print(f\"Beginning trial #{trial}\")\n",
        "    for epoch in range(epochs):\n",
        "        for data in iter(trainloader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            inputs = preprocessing(inputs)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch} of trial {trial} successfully completed with {loss} train loss\")\n",
        "    params.append(get_params(net))\n",
        "    print(f\"Test loss at the conclusion of trial {trial}:\")\n",
        "    test_model(net, preprocessing)\n"
      ],
      "metadata": {
        "id": "QfZFWaX69F3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.stack(params)\n",
        "torch.save(params, f\"DenseWithPretrainedtrials{num_trials}X.pt\")\n",
        "files.download(f\"DenseWithPretrainedtrials{num_trials}X.pt\")"
      ],
      "metadata": {
        "id": "rZD1VOjg9nbo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}