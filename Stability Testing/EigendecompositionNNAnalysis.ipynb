{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "TESTING_ENABLED = True\n",
        "ANALYSIS_ENABLED = False"
      ],
      "metadata": {
        "id": "my9TH_eIXzDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhbHWigUXijJ"
      },
      "outputs": [],
      "source": [
        "def cov(mat):\n",
        "    \"\"\"\n",
        "    mat should be a matrix whose rows are separate samples.\n",
        "    \"\"\"\n",
        "    xbar = torch.zeros(mat.size()[1], device=device)\n",
        "    for row in mat:\n",
        "        xbar += row\n",
        "    xbar /= mat.size()[0]\n",
        "    matdelta = [row-xbar for row in mat]\n",
        "    print(f\"Squares of L2 norms of X-Xbar: {[torch.nansum(torch.square(row)) for row in matdelta]}\")\n",
        "    ans = []\n",
        "    for row in matdelta:\n",
        "        unsqueezed_row = row.unsqueeze(0)\n",
        "        ans.append(torch.matmul(unsqueezed_row.t(), unsqueezed_row))\n",
        "    return sum(ans)/(len(ans)-1)\n",
        "\n",
        "def get_eig_of_differences(data):\n",
        "    \"\"\"\n",
        "    data should be a 2-dimensional tensor whose rows are the trained parameters\n",
        "    of an ML model, with one training per row. If you have a model with N\n",
        "    parameters that you run M times, data should be of size (M, N). Returns a\n",
        "    1-dimensional, M-element tensor whose entries are the eigenvalues (in no\n",
        "    particular order) of the square matrix calculated by subtracting the mean\n",
        "    of the input rows from each individual row, then right-multiplying that\n",
        "    matrix by its transpose. This guarantees a M by M square matrix, which\n",
        "    means that calculating the eigenvalues is surprisingly fast, even with\n",
        "    reasonably large models.\n",
        "    \"\"\"\n",
        "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
        "    deviation_data = data-mu\n",
        "    square_data = torch.matmul(deviation_data, deviation_data.transpose(0, 1))\n",
        "    eig = torch.linalg.eigvals(square_data)\n",
        "    return eig.real\n",
        "\n",
        "def get_l2_norm(data):\n",
        "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
        "    deviation_data = data-mu\n",
        "    total_norm = 0.0\n",
        "    for row in deviation_data:\n",
        "        norm = torch.norm(row, p=2)\n",
        "        norm *= norm\n",
        "        total_norm += norm\n",
        "    return total_norm / data.size()[0]\n",
        "\n",
        "def get_sample_covariance_eigs(data):\n",
        "    cov = torch.cov(data.t()) # torch.cov expects columns, not rows, to be individual trials\n",
        "    eig = torch.linalg.eigvals(cov)\n",
        "    return eig.real\n",
        "\n",
        "def get_eig_of_differences_fast(data, num_eigvals=-1):\n",
        "    \"\"\"\n",
        "    ONLY USE THIS FUNCTION AFTER RUNNING AN EXCESSIVE (>10,000) NUMBER OF TRIALS\n",
        "    Acts similarly to get_eig_of_differences, except that it averages the N by N\n",
        "    square matrix whose eigenvalues are calculated with its transpose, then uses\n",
        "    torch.lobpcg to estimate only the num_eigvals largest eigenvalues, instead\n",
        "    of calculating all of them. num_eigvals defaults to data.size()[0]\n",
        "    \"\"\"\n",
        "    mu = torch.mean(data, 0, keepdim=True).expand(data.size()[0], -1)\n",
        "    deviation_data = data-mu\n",
        "    square_data = torch.matmul(deviation_data, deviation_data.transpose(0, 1))\n",
        "    square_data = (square_data+square_data.transpose(0, 1))/2\n",
        "    eigvals_to_gen = data.size()[0] if num_eigvals == -1 else num_eigvals\n",
        "    eig = torch.lobpcg(square_data, k=eigvals_to_gen)\n",
        "    return eig[0]\n",
        "\n",
        "def generate_gaussian_noise(cov, noise_mult=1):\n",
        "    U, S, V = torch.linalg.svd(cov)\n",
        "    plt.hist([eig.item() for eig in S], bins=50, log=True)\n",
        "    print(f\"Sum of eigenvalues: {torch.nansum(S)}\")\n",
        "    Sprime = torch.sqrt(S)\n",
        "    print(f\"sqrt sum of eigenvalues: {torch.nansum(Sprime)}\")\n",
        "    Sprime *= torch.nansum(Sprime)\n",
        "    Sprime = torch.sqrt(Sprime)\n",
        "    print(f\"Average eigenvalue of noise (after sqrt etc.): {sum(Sprime)/Sprime.size()[0]}. Max after-processing eigenvalue of noise: {torch.max(Sprime)}\")\n",
        "    plt.hist([eig.item() for eig in Sprime], bins=50, log=True)\n",
        "    print(sum(U[0]), sum(V[0]))\n",
        "    print(f\"L2 norm of noise (before being multiplied by {noise_mult}): {math.sqrt(sum(torch.square(torch.matmul(U, torch.matmul(torch.ones(cov.size()[0], device=device), torch.diag(Sprime))))))}\")\n",
        "    noise = torch.randn(cov.size()[0], device=device)*noise_mult\n",
        "    noise = torch.matmul(noise, torch.diag(Sprime))\n",
        "    noise = torch.matmul(U, noise)\n",
        "    return noise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload a matrix to analyze. Each row should be the results of one trial.\")\n",
        "uploaded = files.upload()\n",
        "if len(uploaded.keys()) == 1:\n",
        "    data = torch.load(BytesIO(list(uploaded.values())[0]), map_location=device)\n",
        "else:\n",
        "    data = []\n",
        "    for trial in uploaded.values():\n",
        "        data.append(torch.load(BytesIO(trial, map_location=device)))\n",
        "    if len(data[0].size()) == 1:\n",
        "        data = torch.stack(data)\n",
        "    else:\n",
        "        data = torch.cat(data, 0)\n",
        "assert isinstance(data, torch.Tensor)\n",
        "assert len(data.size()) == 2\n",
        "data = data.to(device)\n",
        "print(f\"input matrix successfully loaded. It is of size {data.size()}\")\n",
        "if ANALYSIS_ENABLED:\n",
        "    eig = get_sample_covariance_eigs(data)\n",
        "    eig.to('cpu')\n",
        "    print(f\"Eigenvalues: {eig}\")\n",
        "    norm = get_l2_norm(data)\n",
        "    print(f\"L2 norm: {norm}\")\n",
        "    torch.save(eig, \"SampleCovarianceEigenvalues.pt\")\n",
        "    files.download(\"SampleCovarianceEigenvalues.pt\")\n",
        "    print(f\"Sum of sqrts of eigenvalues: {torch.nansum(torch.sqrt(eig))}\")"
      ],
      "metadata": {
        "id": "_GJzgqaO85Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _weights_init(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3]).to(device)\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD_DEV = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD_DEV),\n",
        "])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train = False, download=True, transform=transform)\n",
        "\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def test_model(model, preprocessing = lambda x: x):\n",
        "    test_loss = 0.0\n",
        "    testloader = torch.utils.data.DataLoader(testset, shuffle=True, num_workers=2, batch_size = 20)\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    train_on_gpu = torch.cuda.is_available()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    model.eval()\n",
        "    # iterate over test data\n",
        "    for data, target in testloader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # preprocess data with an arbitrary function\n",
        "        data = preprocessing(data)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update test loss\n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(20):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "    # average test loss\n",
        "    test_loss = test_loss/len(testloader.dataset)\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    for i in range(10):\n",
        "        if class_total[i] > 0:\n",
        "\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "              classes[i], 100 * class_correct[i] / class_total[i],\n",
        "              np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "    return 100. * np.sum(class_correct) / np.sum(class_total)\n",
        "\n",
        "class DenseModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DenseModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(64, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 32)\n",
        "        self.layer4 = nn.Linear(32, 16)\n",
        "        self.layer5 = nn.Linear(16, 16)\n",
        "        self.layer6 = nn.Linear(16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.layer1(x))\n",
        "        out = F.relu(self.layer2(out))\n",
        "        out = F.relu(self.layer3(out))\n",
        "        out = F.relu(self.layer4(out))\n",
        "        out = F.relu(self.layer5(out))\n",
        "        out = self.layer6(out)\n",
        "        return out\n",
        "\n",
        "def load_params(model, params):\n",
        "    \"\"\"\n",
        "    This will error (or possibly silenetly fail!) if params were not generated\n",
        "    using the get_params function from a model of the same type as the model\n",
        "    passed in.\n",
        "    \"\"\"\n",
        "    params_idx = 0\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            params_needed = param.flatten().size()[0]\n",
        "            params_to_take = params[params_idx:params_idx+params_needed]\n",
        "            params_idx += params_needed\n",
        "            param.copy_(params_to_take.reshape(param.size()))\n",
        "    return model"
      ],
      "metadata": {
        "id": "zetgqvmPfhL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_IDX = 24 # The number of the model we are using. Should not be greater than the number of trials that have been run\n",
        "NOISE_MULTIPLIER = 1 # TODO determine what levels of noise correspond to what levels of DP (FAR easier said than done)\n",
        "\n",
        "if TESTING_ENABLED:\n",
        "    preprocessing = resnet20()\n",
        "    preprocessing.to(device)\n",
        "    print(\"Please upload the pretrained ResNet20 used when training the analyzed models.\")\n",
        "    uploaded = files.upload()\n",
        "    preprocessing.load_state_dict(torch.load(BytesIO(list(uploaded.values())[0]), map_location=device))\n",
        "    preprocessing.linear = Identity()\n",
        "    preprocessing.eval()\n",
        "\n",
        "    model = DenseModel()\n",
        "    model.to(device)\n",
        "    params = data[MODEL_IDX].clone().detach()\n",
        "    print(f\"L2 norm of model parameters: {math.sqrt(sum(torch.square(params)))}\")\n",
        "    load_params(model, params)\n",
        "\n",
        "    print(\"Testing model without noise:\")\n",
        "    test_model(model, preprocessing)\n",
        "\n",
        "    noise = generate_gaussian_noise(cov(data), NOISE_MULTIPLIER)\n",
        "    with torch.no_grad():\n",
        "        params += noise\n",
        "        load_params(model, params)\n",
        "\n",
        "    print(f\"Testing model with noise scaled by a factor of {NOISE_MULTIPLIER}\")\n",
        "    test_model(model, preprocessing)\n"
      ],
      "metadata": {
        "id": "Wh5zR7M-yfsT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}